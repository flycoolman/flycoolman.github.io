<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DevOps | flycoolman</title>
    <link>/devops/</link>
      <atom:link href="/devops/index.xml" rel="self" type="application/rss+xml" />
    <description>DevOps</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>©2020 flycoolman.com All Rights Reserved</copyright><lastBuildDate>Thu, 10 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu42a5118afc49636e6fda7ce5d4b48056_3366_512x512_fill_lanczos_center_2.png</url>
      <title>DevOps</title>
      <link>/devops/</link>
    </image>
    
    <item>
      <title>Docker</title>
      <link>/devops/docker/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/devops/docker/</guid>
      <description>&lt;h2 id=&#34;docker&#34;&gt;Docker&lt;/h2&gt;
&lt;h3 id=&#34;what-is-docker&#34;&gt;What is Docker?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt; is basically a container engine which uses the Linux Kernel features like &lt;strong&gt;namespaces&lt;/strong&gt; and &lt;strong&gt;control groups&lt;/strong&gt; to create containers on top of an operating system and automates application deployment on the container. Docker uses &lt;strong&gt;Copy-on-write union file system&lt;/strong&gt; for its &lt;strong&gt;backend storage&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;what-is-the-difference-between-a-docker-container-and-a-docker-image&#34;&gt;What is the difference between a docker container and a docker image?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Docker Image&lt;/strong&gt; is a set of files which has no state, whereas &lt;strong&gt;Docker Container&lt;/strong&gt; is the instantiation of Docker Image. In other words, Docker Container is the &lt;strong&gt;run time instance&lt;/strong&gt; of images.&lt;/p&gt;
&lt;h3 id=&#34;what-is-a-container&#34;&gt;What is a container?&lt;/h3&gt;
&lt;p&gt;In 4 bullet points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Containers share the host kernel&lt;/li&gt;
&lt;li&gt;Containers use the kernel ability to group processes for resource control&lt;/li&gt;
&lt;li&gt;Containers ensure isolation through namespaces&lt;/li&gt;
&lt;li&gt;Containers feel like lightweight VMs (lower footprint, faster), but are not Virtual Machines!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;namespaces-and-cgroups----------------docker&#34;&gt;Namespaces and Cgroups  - - - - - - &amp;gt;  Docker&lt;/h3&gt;
&lt;p&gt;Docker makes use of kernel &lt;strong&gt;namespaces&lt;/strong&gt; to provide the &lt;strong&gt;isolated workspace&lt;/strong&gt; called the container.&lt;br&gt;
Docker also makes use of kernel &lt;strong&gt;control groups&lt;/strong&gt; for resource allocation and isolation.&lt;br&gt;
&lt;img src=&#34;./namespace.png&#34; alt=&#34;namespace&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;container-format&#34;&gt;Container Format&lt;/h3&gt;
&lt;p&gt;Docker Engine combines the &lt;strong&gt;namespaces&lt;/strong&gt;, &lt;strong&gt;control groups&lt;/strong&gt; and &lt;strong&gt;UnionFS&lt;/strong&gt; into a wrapper called a container format. The default container format is &lt;strong&gt;libcontainer&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;types-of-containers&#34;&gt;Types of Containers&lt;/h3&gt;
&lt;p&gt;Given the above constructs, containers may be divided into 3 types as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;System Containers&lt;/strong&gt; share rootfs, PID, network, IPC and UTS with host system but live inside a cgroup.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Application Containers&lt;/strong&gt; live inside a cgroup and use namespaces (PID, network, IPC, chroot) for isolation from host system&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pods&lt;/strong&gt; use namespaces for isolation from host system but create sub groups which share PID, network, IPC and UTS except the rootfs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;images--layers&#34;&gt;Images &amp;amp; Layers&lt;/h3&gt;
&lt;p&gt;Each Docker image references a list of &lt;strong&gt;read-only layers&lt;/strong&gt; that represent filesystem differences. Layers are stacked on top of each other to form a base for a container’s rootfs.&lt;br&gt;
One big innovation of the Docker engine was the concept of leveraging Copy-On-Write file systems to significantly speed up the preparation of the rootfs.&lt;/p&gt;
&lt;h3 id=&#34;copy-on-write&#34;&gt;Copy-On-Write&lt;/h3&gt;
&lt;p&gt;When &lt;strong&gt;Docker&lt;/strong&gt; creates a container, it &lt;strong&gt;adds a new, thin, writable layer&lt;/strong&gt; on top of the underlying stack of image layers. This layer is often called the “container layer”.&lt;br&gt;
All changes made to the running container - such as writing new files, modifying existing files, and deleting files - are written to this thin writable container layer.&lt;/p&gt;
&lt;h3 id=&#34;union-file-systems&#34;&gt;Union File Systems&lt;/h3&gt;
&lt;p&gt;Union File Systems provide the following features for storage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Layering&lt;/li&gt;
&lt;li&gt;Copy-On-Write&lt;/li&gt;
&lt;li&gt;Caching&lt;/li&gt;
&lt;li&gt;Diffing&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dangling-images&#34;&gt;Dangling images&lt;/h3&gt;
&lt;p&gt;Docker images consist of multiple layers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dangling images&lt;/strong&gt; are layers that have no relationship to any tagged images.&lt;/li&gt;
&lt;li&gt;They no longer serve a purpose and consume disk space.&lt;/li&gt;
&lt;li&gt;They can be located by adding the filter flag, &lt;strong&gt;-f&lt;/strong&gt; with a value of &lt;strong&gt;dangling=true&lt;/strong&gt; to the docker images command.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Another description:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An unused image means that it has not been assigned or used in a container. For example, when running docker ps -a - it will list all of your exited and currently running containers. Any images shown being used inside any of containers are a &amp;ldquo;used image&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;On the other hand, a dangling image just means that you&amp;rsquo;ve created the new build of the image, but it wasn&amp;rsquo;t given a new name. So the old images you have becomes the &amp;ldquo;dangling image&amp;rdquo;. Those old images are the ones that are untagged and displays &amp;ldquo;&lt;none&gt;&amp;rdquo; on its name when you run docker images.&lt;/li&gt;
&lt;li&gt;When running &lt;strong&gt;docker system prune -a&lt;/strong&gt;, it will remove both unused and dangling images. Therefore any images being used in a container, whether they have been exited or currently running, will NOT be affected.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-difference-between-copy-and-add-in-a-dockerfile&#34;&gt;The Difference between COPY and ADD in a Dockerfile&lt;/h3&gt;
&lt;p&gt;Sometimes you see &lt;strong&gt;COPY&lt;/strong&gt; or &lt;strong&gt;ADD&lt;/strong&gt; being used in a Dockerfile, but 99% of the time you should be using &lt;strong&gt;COPY&lt;/strong&gt;, here&amp;rsquo;s why.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;COPY&lt;/strong&gt; and &lt;strong&gt;ADD&lt;/strong&gt; are both Dockerfile instructions that serve similar purposes. They let you copy files from a specific location into a Docker image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;COPY&lt;/strong&gt; takes in a &lt;strong&gt;src&lt;/strong&gt; and &lt;strong&gt;destination&lt;/strong&gt;. It &lt;strong&gt;only&lt;/strong&gt; lets you copy in a local file or directory from your host (the machine building the Docker image) into the Docker image itself.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ADD&lt;/strong&gt; lets you do that too, but it also supports &lt;strong&gt;2 other sources&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, you can use a &lt;strong&gt;URL&lt;/strong&gt; instead of a local file / directory.&lt;/li&gt;
&lt;li&gt;Secondly, you can extract a tar file from the source directly into the destination.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In most cases if you’re using a URL, you’re downloading a zip file and are then using the RUN command to extract it. However, you might as well just use RUN with curl instead of ADD here so you chain everything into 1 RUN command to &lt;strong&gt;make a smaller Docker image&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A valid use case for &lt;strong&gt;ADD&lt;/strong&gt; is when you want to extract a local tar file into a specific directory in your Docker image. This is exactly what the Alpine image does with ADD rootfs.tar.gz /.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you’re copying in local files to your Docker image, always use COPY because it’s more explicit.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cmd-vs-entrypoint&#34;&gt;CMD vs. ENTRYPOINT&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Run or execute something when docker starts&lt;/li&gt;
&lt;li&gt;The main purpose of a CMD is to provide defaults for an executing container&lt;/li&gt;
&lt;li&gt;An ENTRYPOINT helps you to configure a container that you can run as an executable&lt;/li&gt;
&lt;li&gt;CMD can be overridden, The ENTRYPOINT instruction works very similarly to CMD in that it is used to specify the command executed when the container is started. However, where it differs is that ENTRYPOINT doesn’t allow you to override the command.&lt;/li&gt;
&lt;li&gt;CMD will be overridden by the ‘docker run …….’ command line, ENTRYPOINT just gets the parameter from ‘docker run …….’ command line&lt;/li&gt;
&lt;li&gt;One important thing to call out about the ENTRYPOINT instruction is that syntax is critical. Technically, ENTRYPOINT supports both the ENTRYPOINT [&amp;ldquo;command&amp;rdquo;] syntax and the ENTRYPOINT command syntax. However, while both of these are supported, they have two different meanings and change how ENTRYPOINT works.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cmd-and-entrypoint-syntax&#34;&gt;CMD and ENTRYPOINT syntax&lt;/h3&gt;
&lt;p&gt;Both CMD and ENTRYPOINT are straight forward but they have a hidden, err, &amp;ldquo;feature&amp;rdquo; that can cause issues if you are not aware. Two different syntaxes are supported for these instructions.&lt;/p&gt;
&lt;p&gt;CMD /bin/echo&lt;br&gt;
or&lt;br&gt;
CMD [&amp;quot;/bin/echo&amp;rdquo;]&lt;br&gt;
This may not look like it would be an issues but the devil in the details will trip you up. If you use the second syntax where the CMD ( or ENTRYPOINT ) is an array, it acts exactly like you would expect. If you use the first syntax without the array, docker pre-pends /bin/sh -c to your command. This has always been in docker as far as I can remember.&lt;/p&gt;
&lt;p&gt;Pre-pending /bin/sh -c can cause some unexpected issues and functionality that is not easily understood if you did not know that docker modified your CMD. Therefore, you should always use the array syntax for both instructions because both will be executed exactly how you intended.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Always use the array syntax when using CMD and ENTRYPOINT.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;docker-commands&#34;&gt;Docker commands&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;docker build&lt;/li&gt;
&lt;li&gt;docker pull&lt;/li&gt;
&lt;li&gt;docker push&lt;/li&gt;
&lt;li&gt;docker images …&lt;/li&gt;
&lt;li&gt;docker commit&lt;/li&gt;
&lt;li&gt;docker exec -it&lt;/li&gt;
&lt;li&gt;docker run -it&lt;/li&gt;
&lt;li&gt;docker system
&lt;ul&gt;
&lt;li&gt;df&lt;/li&gt;
&lt;li&gt;events&lt;/li&gt;
&lt;li&gt;info&lt;/li&gt;
&lt;li&gt;prune&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;docker ps&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;./docker-commands.png&#34; alt=&#34;docker-commands&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;docker-instructions-in-dockerfile&#34;&gt;Docker instructions in dockerfile&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ENV&lt;/li&gt;
&lt;li&gt;RUN&lt;/li&gt;
&lt;li&gt;CMD&lt;/li&gt;
&lt;li&gt;ENTRYPOINT&lt;/li&gt;
&lt;li&gt;COPY&lt;/li&gt;
&lt;li&gt;ADD&lt;/li&gt;
&lt;li&gt;USER&lt;/li&gt;
&lt;li&gt;WORKDIR&lt;/li&gt;
&lt;li&gt;ARG&lt;/li&gt;
&lt;li&gt;EXPOSE&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dockerfile-best-practices&#34;&gt;Dockerfile Best Practices&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.docker.com/develop/develop-images/dockerfile_best-practices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Best practices for writing Dockerfiles&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intro Guide to Dockerfile Best Practices&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://docs.docker.com/develop/dev-best-practices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Docker development best practices&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://engineering.bitnami.com/articles/best-practices-writing-a-dockerfile.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Best practices writing a Dockerfile&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://nickjanetakis.com/blog/docker-tip-2-the-difference-between-copy-and-add-in-a-dockerile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Difference between COPY and ADD in a Dockerfile&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>Minikube</title>
      <link>/devops/minikube/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/devops/minikube/</guid>
      <description>&lt;h2 id=&#34;minikube&#34;&gt;Minikube&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Minikube&lt;/strong&gt; is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day.&lt;/p&gt;
&lt;h3 id=&#34;set-up-minikube&#34;&gt;Set up Minikube&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://minikube.sigs.k8s.io/docs/start/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;minikube start&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;minikube-vm-login&#34;&gt;Minikube VM Login&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;On VM console:&lt;br&gt;
username: root
no password&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;from host terminal:&lt;br&gt;
minikube ssh&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;username+IP on host:&lt;br&gt;
username: docker&lt;br&gt;
password: tcuser&lt;br&gt;
i.e.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh docker@192.168.99.103
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./minikube-login.png&#34; alt=&#34;minikube-login&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh docker@$(minikube ip)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./minikube-login-1.png&#34; alt=&#34;minikube-login-1&#34;&gt;&lt;/p&gt;
&lt;p&gt;Exit the login:
exit&lt;/p&gt;
&lt;h3 id=&#34;setting-a-vm-driver-by-default&#34;&gt;Setting a VM driver by default&lt;/h3&gt;
&lt;p&gt;minikube config set vm-driver virtualbox&lt;/p&gt;
&lt;h3 id=&#34;use-local-images-by-re-using-the-docker-daemon&#34;&gt;Use local images by re-using the Docker daemon&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How To Install and Use Docker on Ubuntu 18.04&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://kubernetes.io/docs/setup/learning-environment/minikube/#use-local-images-by-re-using-the-docker-daemon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Use local images by re-using the Docker daemon&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;eval $(minikube docker-env)&lt;/p&gt;
&lt;h3 id=&#34;commands-for-using-minikube&#34;&gt;Commands for using minikube&lt;/h3&gt;
&lt;p&gt;minikube start&lt;br&gt;
minikube delete&lt;br&gt;
minikube status&lt;/p&gt;
&lt;p&gt;minikube start -p test&lt;br&gt;
minikube delete -p test&lt;br&gt;
&lt;br&gt;
minikube ssh&lt;br&gt;
minikube ssh -p test&lt;br&gt;
minikube ip&lt;br&gt;
minikube dashboard&lt;br&gt;
minikube addons list&lt;br&gt;
&lt;br&gt;
kubectl get pods -A&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://blog.pilosus.org/posts/2019/05/18/minikube-cheat-sheet/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minikube Cheat Sheet: most helpful commands and features I wish I knew from the start&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;minikube-command-auto-completion&#34;&gt;Minikube command auto completion&lt;/h3&gt;
&lt;p&gt;sudo apt install bash-completion&lt;br&gt;

&lt;a href=&#34;https://www.cyberciti.biz/faq/add-bash-auto-completion-in-ubuntu-linux/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to add bash auto completion in Ubuntu Linux&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://stackoverflow.com/questions/57891054/how-can-i-enable-tab-completion-to-minikube&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How can I enable Tab completion to minikube?&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes Notes</title>
      <link>/devops/k8s-notes/</link>
      <pubDate>Tue, 31 Dec 2019 23:59:59 +0000</pubDate>
      <guid>/devops/k8s-notes/</guid>
      <description>&lt;h2 id=&#34;k8s-notes&#34;&gt;K8S NOTES&lt;/h2&gt;
&lt;h3 id=&#34;kubernetes-components&#34;&gt;Kubernetes Components&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cluster
&lt;ul&gt;
&lt;li&gt;A cluster is a set of machines, called nodes, that run containerized applications managed by Kubernetes.&lt;/li&gt;
&lt;li&gt;A cluster has at least one worker node and at least one master node.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Node
&lt;ul&gt;
&lt;li&gt;Master node(s)&lt;br&gt;
The master node(s) manages the worker nodes and the pods in the cluster. Multiple master nodes are used to provide a cluster with failover and high availability.&lt;/li&gt;
&lt;li&gt;Worker node(s)&lt;br&gt;
The worker node(s) host the pods that are the components of the application.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;master-node-components&#34;&gt;Master Node Components&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;kube-apiserver&lt;/li&gt;
&lt;li&gt;etcd&lt;/li&gt;
&lt;li&gt;kube-scheduler&lt;/li&gt;
&lt;li&gt;kube-controller-manager
&lt;ul&gt;
&lt;li&gt;Node Controller&lt;/li&gt;
&lt;li&gt;Replication Controller&lt;/li&gt;
&lt;li&gt;Endpoints Controller&lt;/li&gt;
&lt;li&gt;Service Account &amp;amp; Token Controllers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cloud-controller-manager&lt;br&gt;
The following controllers have cloud provider dependencies:
&lt;ul&gt;
&lt;li&gt;Node Controller&lt;/li&gt;
&lt;li&gt;Route Controller&lt;/li&gt;
&lt;li&gt;Service Controller&lt;/li&gt;
&lt;li&gt;Volume Controller&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;worker-node-components&#34;&gt;Worker Node Components&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Kubelet&lt;/li&gt;
&lt;li&gt;kube-proxy&lt;/li&gt;
&lt;li&gt;Container Runtime&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;addons&#34;&gt;Addons&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DNS&lt;/li&gt;
&lt;li&gt;Web UI (Dashboard)&lt;/li&gt;
&lt;li&gt;Container Resource Monitoring&lt;/li&gt;
&lt;li&gt;Cluster-level Logging&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;k8s-abbreviation&#34;&gt;K8S Abbreviation&lt;/h3&gt;
&lt;h4 id=&#34;cni&#34;&gt;CNI&lt;/h4&gt;
&lt;p&gt;Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster.&lt;/p&gt;
&lt;h4 id=&#34;cri&#34;&gt;CRI&lt;/h4&gt;
&lt;p&gt;Container Runtime Interface (CRI)&lt;br&gt;
Each container runtime has it own strengths, and many users have asked for Kubernetes to support more runtimes. CRI consists of a protocol buffers and gRPC API, and libraries, with additional specifications and tools under active development.&lt;br&gt;
Kubelet communicates with the container runtime (or a CRI shim for the runtime) over Unix sockets using the gRPC framework, where kubelet acts as a client and the CRI shim as the server.&lt;br&gt;
&lt;img src=&#34;./cri.png&#34; alt=&#34;cri.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;li&gt;CRI-O&lt;/li&gt;
&lt;li&gt;Containerd&lt;/li&gt;
&lt;li&gt;Other CRI runtimes: frakti&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;csi&#34;&gt;CSI&lt;/h4&gt;
&lt;p&gt;The goal of CSI is to establish a standardized mechanism for Container Orchestration Systems (COs) to expose arbitrary storage systems to their containerized workloads.
Assuming a CSI storage plugin is already deployed on a Kubernetes cluster, users can use CSI volumes through the familiar Kubernetes storage API objects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PersistentVolumeClaims&lt;/li&gt;
&lt;li&gt;PersistentVolumes&lt;/li&gt;
&lt;li&gt;StorageClasses&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;crd&#34;&gt;CRD&lt;/h4&gt;
&lt;p&gt;CustomResourceDefinition&lt;/p&gt;
&lt;h3 id=&#34;k8s-networking&#34;&gt;K8S Networking&lt;/h3&gt;
&lt;h4 id=&#34;pod-communication&#34;&gt;POD Communication&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Inner POD&lt;br&gt;
Multi-containers in one POD&lt;br&gt;
Containers within a pod share an IP address and port space, and can find each other via localhost. They can also communicate with each other using standard inter-process communications.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shared volume&lt;/li&gt;
&lt;li&gt;IPC, i.e. queue&lt;/li&gt;
&lt;li&gt;Networking, localhost with different port&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inter PODs&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;services&#34;&gt;Services&lt;/h3&gt;
&lt;h4 id=&#34;clusterip&#34;&gt;ClusterIP&lt;/h4&gt;
&lt;p&gt;ClusterIP accesses the services through proxy.&lt;br&gt;
ClusterIP can access services only inside the cluster.&lt;br&gt;
&lt;img src=&#34;./clusterip.png&#34; alt=&#34;clusterip&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;nodeport&#34;&gt;Nodeport&lt;/h4&gt;
&lt;p&gt;NodePort opens a specific port on each node of the cluster and traffic on that node is forwarded directly to the service.&lt;br&gt;
&lt;img src=&#34;./nodeport.png&#34; alt=&#34;nodeport&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;loadbalancer&#34;&gt;Loadbalancer&lt;/h4&gt;
&lt;p&gt;All the traffic on the port is forwarded to the service, there&amp;rsquo;s no filtering, no routing.&lt;br&gt;
&lt;img src=&#34;./loadbalancer.png&#34; alt=&#34;loadbalancer&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;ingress-controller&#34;&gt;Ingress Controller&lt;/h3&gt;
&lt;p&gt;Ingress Controller but there are third party solutions like &lt;strong&gt;Traefik&lt;/strong&gt; and &lt;strong&gt;Nginx&lt;/strong&gt; available. Ingress controller also provide L7 load balancing unlike cluster services.&lt;/p&gt;
&lt;h3 id=&#34;network-policies&#34;&gt;Network policies&lt;/h3&gt;
&lt;p&gt;Isolation policies are configured on a per-namespace basis.&lt;br&gt;

&lt;a href=&#34;https://cloudnativelabs.github.io/post/2017-04-18-kubernetes-networking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Networking&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;k8s-objects&#34;&gt;K8S Objects&lt;/h3&gt;
&lt;p&gt;To find all the objects in some specific API version:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kubectl api-resources | cut -c92-&lt;/li&gt;
&lt;li&gt;kubectl api-resources | cut -c92-150&lt;/li&gt;
&lt;li&gt;kubectl api-resources | cut -c92-150 | wc -l&lt;/li&gt;
&lt;li&gt;kubectl api-resources&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;pod&#34;&gt;Pod&lt;/h4&gt;
&lt;p&gt;A thin wrapper around one or more containers&lt;/p&gt;
&lt;h4 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h4&gt;
&lt;p&gt;Implements a single instance of a pod on a worker node&lt;/p&gt;
&lt;h4 id=&#34;deployment&#34;&gt;Deployment&lt;/h4&gt;
&lt;p&gt;Details how to roll out (or roll back) across versions of your application&lt;/p&gt;
&lt;h4 id=&#34;replicaset&#34;&gt;ReplicaSet&lt;/h4&gt;
&lt;p&gt;Ensures a defined number of pods are always running&lt;/p&gt;
&lt;h4 id=&#34;job&#34;&gt;Job&lt;/h4&gt;
&lt;p&gt;Ensures a pod properly runs to completion&lt;/p&gt;
&lt;h4 id=&#34;service&#34;&gt;Service&lt;/h4&gt;
&lt;p&gt;Maps a fixed IP address to a logical group of pods&lt;/p&gt;
&lt;h4 id=&#34;label&#34;&gt;Label&lt;/h4&gt;
&lt;p&gt;Key/Value pairs used for association and filtering&lt;br&gt;
Labels in Kubernetes are intended to be used to specify identifying attributes for objects. They are used by selector queries or with label selectors. Since they are used internally by Kubernetes, the structure of keys and values is constrained, to optimize queries.&lt;/p&gt;
&lt;h4 id=&#34;annotations&#34;&gt;Annotations&lt;/h4&gt;
&lt;p&gt;annotations are a way to attach non-identifying metadata to objects. This metadata is not used internally by Kubernetes, so they cannot be used to identify within k8s. Instead, they are used by external tools and libraries. Examples of annotations include build/release timestamps, client library information for debugging, or fields managed by a network policy like Calico in this case.&lt;/p&gt;
&lt;h4 id=&#34;label-vs-annotation&#34;&gt;Label vs. Annotation&lt;/h4&gt;
&lt;p&gt;You can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects.&lt;/p&gt;
&lt;h4 id=&#34;taints-and-tolerations&#34;&gt;Taints and Tolerations&lt;/h4&gt;
&lt;p&gt;Node affinity, described here, is a property of pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite – they allow a node to repel a set of pods.&lt;/p&gt;
&lt;p&gt;Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints. Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.&lt;/p&gt;
&lt;h4 id=&#34;node-isolationrestriction&#34;&gt;Node isolation/restriction&lt;/h4&gt;
&lt;p&gt;Adding labels to Node objects allows targeting pods to specific nodes or groups of nodes. This can be used to ensure specific pods only run on nodes with certain isolation, security, or regulatory properties.&lt;/p&gt;
&lt;h4 id=&#34;nodeselector&#34;&gt;NODESELECTOR&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Attach a label to the node&lt;/li&gt;
&lt;li&gt;Add a nodeSelector field to your pod configuration&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;affinity-and-anti-affinity&#34;&gt;Affinity and anti-affinity&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;• Node affinity
• Inter-pod affinity and anti-affinity
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nodeSelector provides a very simple way to constrain pods to nodes with particular labels. The affinity/anti-affinity feature, currently in beta, greatly extends the types of constraints you can express. The key enhancements are:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;• The language is more expressive (not just “AND of exact match”)
• You can indicate that the rule is “soft”/“preference” rather than a hard requirement, so if the scheduler can’t satisfy it, the pod will still be scheduled
• You can constrain against labels on other pods running on the node (or other topological domain), rather than against labels on the node itself, which allows rules about which pods can and cannot be co-located
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The affinity feature consists of two types of affinity, “node affinity” and “inter-pod affinity/anti-affinity”. Node affinity is like the existing nodeSelector (but with the first two benefits listed above), while inter-pod affinity/anti-affinity constrains against pod labels rather than node labels, as described in the third item listed above, in addition to having the first and second properties listed above.&lt;/p&gt;
&lt;p&gt;Node affinity is conceptually similar to nodeSelector – it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node.&lt;/p&gt;
&lt;h3 id=&#34;deploy&#34;&gt;Deploy&lt;/h3&gt;
&lt;p&gt;When you wish to deploy an application in Kubernetes, you usually define three components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;strong&gt;Deployment&lt;/strong&gt; — which is a recipe for creating copies of your application called Pods&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;Service&lt;/strong&gt; — an internal load balancer that routes the traffic to Pods&lt;/li&gt;
&lt;li&gt;an &lt;strong&gt;Ingress&lt;/strong&gt; — a description of how the traffic should flow from outside the cluster to your Service&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ingress&#34;&gt;Ingress&lt;/h3&gt;
&lt;h4 id=&#34;what-is-an-ingress&#34;&gt;What is an ingress?&lt;/h4&gt;
&lt;p&gt;In Kubernetes, an Ingress is an object that allows access to your Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services.&lt;br&gt;
&lt;br&gt;
Ingress, added in Kubernetes v1.1, exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.&lt;br&gt;
&lt;br&gt;
Internet&amp;mdash;[ Ingress ]&amp;ndash;|&amp;ndash;|&amp;ndash;[ Services ]
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;An Ingress can be configured to give services externally-reachable URLs, load balance traffic, terminate SSL, and offer name based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a loadbalancer, though it may also configure your edge router or additional frontends to help handle the traffic.&lt;/p&gt;
&lt;p&gt;An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer.&lt;/p&gt;
&lt;h4 id=&#34;what-is-an-ingress-controller&#34;&gt;What is an ingress controller?&lt;/h4&gt;
&lt;p&gt;Kubernetes supports a high level abstraction called Ingress, which allows simple host or URL based HTTP routing. An ingress is a core concept (in beta) of Kubernetes, but is always implemented by a third party proxy. These implementations are known as ingress controllers.&lt;br&gt;
&lt;br&gt;
In order for the Ingress resource to work, the cluster must have an ingress controller running.&lt;br&gt;
Unlike other types of controllers which run as part of the kube-controller-manager binary, Ingress controllers are not started automatically with a cluster. Let’s see some options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ALB Ingress Controller&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;ingress-vs-ingress-controller&#34;&gt;Ingress vs. Ingress controller&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ingress should be the rules for the traffic, which indicate the destination of a request will go through in the cluster.&lt;/li&gt;
&lt;li&gt;Ingress Controller is the implementation for the Ingress. GCE and Nginx are both supported by k8s. They will take care of L4 or L7 proxy.&lt;/li&gt;
&lt;li&gt;Just like other objects in K8s, ingress is also a type of object, which is mainly referred as set of redirection rules.&lt;/li&gt;
&lt;li&gt;Where as ingress controller is like other deployment objects(could be deamon set as well) which listen and configure those ingress rules.&lt;/li&gt;
&lt;li&gt;If I talk in terms of Nginx, Ingress controller is Nginx software itself where as ingress(ingress rules) are nginx configurations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;the-ingress-resource&#34;&gt;The Ingress Resource&lt;/h4&gt;
&lt;p&gt;A minimal ingress resource example:&lt;br&gt;
&lt;img src=&#34;./ingress-resource-examle.png&#34; alt=&#34;ingress-resource-examle&#34;&gt;&lt;/p&gt;
&lt;p&gt;As with all other Kubernetes resources, an Ingress needs apiVersion, kind, and metadata fields. Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which is the rewrite-target annotation. Different Ingress controller support different annotations. Review the documentation for your choice of Ingress controller to learn which annotations are supported.&lt;br&gt;
The Ingress spec has all the information needed to configure a loadbalancer or proxy server. Most importantly, it contains a list of rules matched against all incoming requests. Ingress resource only supports rules for directing HTTP traffic.&lt;/p&gt;
&lt;h4 id=&#34;ingress-rules&#34;&gt;Ingress Rules&lt;/h4&gt;
&lt;p&gt;Each http rule contains the following information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An optional host. In this example, no host is specified, so the rule applies to all inbound HTTP traffic through the IP address specified. If a host is provided (for example, foo.bar.com), the rules apply to that host.&lt;/li&gt;
&lt;li&gt;A list of paths (for example, /testpath), each of which has an associated backend defined with a serviceName and servicePort. Both the host and path must match the content of an incoming request before the loadbalancer will direct traffic to the referenced service.&lt;/li&gt;
&lt;li&gt;A backend is a combination of service and port names as described in the services doc. HTTP (and HTTPS) requests to the Ingress matching the host and path of the rule will be sent to the listed backend.&lt;/li&gt;
&lt;li&gt;A default backend is often configured in an Ingress controller that will service any requests that do not match a path in the spec.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;default-backend&#34;&gt;Default Backend&lt;/h4&gt;
&lt;p&gt;An Ingress with no rules sends all traffic to a single default backend. The default backend is typically a configuration option of the Ingress controller and is not specified in your Ingress resources.&lt;/p&gt;
&lt;p&gt;If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend.&lt;/p&gt;
&lt;h3 id=&#34;configmap&#34;&gt;ConfigMap&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://matthewpalmer.net/kubernetes-app-developer/articles/ultimate-configmap-guide-kubernetes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ultimate Guide to ConfigMaps in Kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;kubernetes-operator&#34;&gt;Kubernetes Operator&lt;/h3&gt;
&lt;p&gt;An Operator is an application-specific controller that extends the Kubernetes API to create, configure, and manage instances of complex stateful applications on behalf of a Kubernetes user. It builds upon the basic Kubernetes resource and controller concepts but includes domain or application-specific knowledge to automate common tasks.&lt;br&gt;
Examples are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;elasticsearch-operator&lt;/li&gt;
&lt;li&gt;prometheus-operator&lt;/li&gt;
&lt;li&gt;cassandra-operator&lt;/li&gt;
&lt;li&gt;Flux  The GitOps Kubernetes Operator&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Always use the array syntax when using CMD and ENTRYPOINT.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://nickjanetakis.com/blog/docker-tip-2-the-difference-between-copy-and-add-in-a-dockerile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Difference between COPY and ADD in a Dockerfile&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h4&gt;
</description>
    </item>
    
  </channel>
</rss>
