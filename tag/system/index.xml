<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>System | flycoolman</title>
    <link>/tag/system/</link>
      <atom:link href="/tag/system/index.xml" rel="self" type="application/rss+xml" />
    <description>System</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>©2023 flycoolman.com All Rights Reserved</copyright><lastBuildDate>Fri, 24 Feb 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu42a5118afc49636e6fda7ce5d4b48056_3366_512x512_fill_lanczos_center_2.png</url>
      <title>System</title>
      <link>/tag/system/</link>
    </image>
    
    <item>
      <title>Terminator Cheatsheet</title>
      <link>/linux/terminator-cheatsheet/</link>
      <pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate>
      <guid>/linux/terminator-cheatsheet/</guid>
      <description>&lt;h2 id=&#34;installation&#34;&gt;Installation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install terminator
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;shortcuts&#34;&gt;Shortcuts&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Shortcuts&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Split Horizontally&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Shift+O&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Split Vertically&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Shift+E&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Move to Next Terminal&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Shift+N or Ctrl+Tab&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Move Parent Dragbar&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Arrow_key&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hide/Show Scrollbar&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Shift+S&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Search for a Keyword&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Shift+F&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Move to other Terminal&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Alt+Arrow_key&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Copy text to clipboard&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Shift+C&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Paste text from Clipboard&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Shift+V&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Close the Current Terminal&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Shift+W&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Quit the Terminator&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Shift+Q&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toggle full screen the tab&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Shift+X&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Open New Tab&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Shift+T&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Increase Font size&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+(+)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Decrease Font Size&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+(-)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reset Font Size to Original&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+(0)(zero)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toggle Full-Screen Mode&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;F11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reset Terminal and Clear Window&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Ctrl+Shift+G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Group all Terminal into one&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Super+G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Ungroup all Terminal&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;Super+shift+G&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toggle Full-Screen Mode&lt;/td&gt;
&lt;td&gt;&amp;mdash;&amp;mdash;&lt;/td&gt;
&lt;td&gt;F11&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.tecmint.com/terminator-a-linux-terminal-emulator-to-manage-multiple-terminal-windows/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Terminator – A Terminal Emulator to Manage Multiple Terminal Windows on Linux&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://terminator-gtk3.readthedocs.io/en/latest/gettingstarted.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Terminator - readthedocs&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes Best Practices</title>
      <link>/devops/kubernetes-best-practices/</link>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/devops/kubernetes-best-practices/</guid>
      <description>&lt;h2 id=&#34;kubernetes-best-practices&#34;&gt;Kubernetes Best Practices&lt;/h2&gt;
&lt;h3 id=&#34;best-practices-for-image-management&#34;&gt;Best Practices for Image Management&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Build your images you base them on only well-known and trusted image providers.&lt;/li&gt;
&lt;li&gt;Some combination of the semantic version and the SHA hash of the commit where the image was built is a good practice for naming images (e.g., v1.0.1-bfeda01f).&lt;/li&gt;
&lt;li&gt;It is a bad idea for production usage because latest is clearly being mutated every time a new image is built.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;deploying-services-best-practices&#34;&gt;Deploying Services Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Most services should be deployed as &lt;strong&gt;Deployment resources&lt;/strong&gt;. Deployments create identical replicas for redundancy and scale.&lt;/li&gt;
&lt;li&gt;Deployments can be exposed using a Service, which is effectively a load balancer. A Service can be exposed either within a cluster (the default) or externally. If you want to expose an HTTP application, you can use an Ingress controller to add things like request routing and SSL.&lt;/li&gt;
&lt;li&gt;Eventually you will want to &lt;strong&gt;parameterize&lt;/strong&gt; your application to make its configuration more reusable in different environments. Packaging tools like 
&lt;a href=&#34;https://helm.sh/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Helm&lt;/a&gt; are the best choice for this kind of parameterization.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;setting-up-a-development-environment-best-practices&#34;&gt;Setting Up a Development Environment Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Think about developer experience in three phases: onboarding, developing, and testing. Make sure that the development environment you build supports all three of these phases.&lt;/li&gt;
&lt;li&gt;When building a development cluster, you can choose between one large cluster and a cluster per developer. There are pros and cons to each, but generally a single large cluster is a better approach.&lt;/li&gt;
&lt;li&gt;When you add users to a cluster, add them with their own identity and access to their own namespace. Use resource limits to restrict how much of the cluster they can use.&lt;/li&gt;
&lt;li&gt;When managing namespaces, think about how you can reap old, unused resources. Developers will have bad hygiene about deleting unused things. Use automation to clean it up for them.&lt;/li&gt;
&lt;li&gt;Think about cluster-level services like logs and monitoring that you can set up for all users. Sometimes, cluster-level dependencies like databases are also useful to set up on behalf of all users using templates like Helm charts.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;best-practices-for-monitoring-logging-and-alerting&#34;&gt;Best Practices for Monitoring, Logging, and Alerting&lt;/h3&gt;
&lt;p&gt;Following are the best practices that you should adopt regarding monitoring, logging, and alerting.&lt;/p&gt;
&lt;h4 id=&#34;monitoring&#34;&gt;Monitoring&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Monitor nodes and all Kubernetes components for utilization, saturation, and error rates, and monitor applications for rate, errors, and duration.&lt;/li&gt;
&lt;li&gt;Use black-box monitoring to monitor for symptoms and not predictive health of a system.&lt;/li&gt;
&lt;li&gt;Use white-box monitoring to inspect the system and its internals with instrumentation.&lt;/li&gt;
&lt;li&gt;Implement time-series-based metrics to gain high-precision metrics that also allow you to gain insight within the behavior of your application.&lt;/li&gt;
&lt;li&gt;Utilize monitoring systems like Prometheus that provide key labeling for high dimensionality; this will give a better signal to symptoms of an impacting issue.&lt;/li&gt;
&lt;li&gt;Use average metrics to visualize subtotals and metrics based on factual data. Utilize sum metrics to visualize the distribution across a specific metric.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;logging&#34;&gt;Logging&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;You should use logging in combination with metrics monitoring to get the full picture of how your environment is operating.&lt;/li&gt;
&lt;li&gt;Be cautious of storing logs for more than 30 to 45 days and, if needed, use cheaper resources for long-term archiving.&lt;/li&gt;
&lt;li&gt;Limit usage of log forwarders in a sidecar pattern, as they will utilize a lot more resources. Opt for using a DaemonSet for the log forwarder and sending logs to STDOUT.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;alerting&#34;&gt;Alerting&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Be cautious of alert fatigue because it can lead to bad behaviors in people and processes.&lt;/li&gt;
&lt;li&gt;Always look at incrementally improving upon alerting and accept that it will not always be perfect.&lt;/li&gt;
&lt;li&gt;Alert for symptoms that affect your SLO and customers and not for transient issues that don’t need immediate human attention.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;It’s a good practice to monitor your cluster from a “utility cluster” to avoid a production issue also affecting your monitoring system.&lt;/li&gt;
&lt;li&gt;Black-box monitoring gives you symptoms.&lt;/li&gt;
&lt;li&gt;White-box monitoring gives you &amp;ldquo;Why&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;USE&lt;/strong&gt; and &lt;strong&gt;RED&lt;/strong&gt; methods are complementary to each other given that the USE method focuses on the infrastructure components and the RED method focuses on monitoring the end-user experience for the application.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;cAdvisor&lt;/strong&gt; and &lt;strong&gt;metrics server&lt;/strong&gt; are used to provide detailed metrics on resource usage, &lt;strong&gt;kube-state-metrics&lt;/strong&gt; is focused on identifying conditions on Kubernetes objects deployed to your cluster.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;best-practices-for-configmaps-and-secrets&#34;&gt;Best Practices for ConfigMaps and Secrets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;To support dynamic changes to your application without having to redeploy new versions of the pods, mount your ConfigMaps/Secrets as a volume and configure your application with a file watcher to detect the changed file data and reconfigure itself as needed.&lt;/li&gt;
&lt;li&gt;ConfigMap/Secrets must exist in the namespace for the pods that will consume them prior to the pod being deployed. The optional flag can be used to prevent the pods from not starting if the ConfigMap/Secret is not present.&lt;/li&gt;
&lt;li&gt;Use an admission controller to ensure specific configuration data or to prevent deployments that do not have specific configuration values set. An example would be if you require all production Java workloads to have certain JVM properties set in production environments. There is an alpha API called PodPresets that will allow ConfigMaps and secrets to be applied to all pods based on an annotation, without needing to write a custom admission controller.&lt;/li&gt;
&lt;li&gt;If you’re using Helm to release applications into your environment, you can use a life cycle hook to ensure the ConfigMap/Secret template is deployed before the Deployment is applied.&lt;/li&gt;
&lt;li&gt;Some applications require their configuration to be applied as a single file such as a JSON or YAML file. ConfigMap/Secrets allows an entire block of raw data by using the | symbol.&lt;/li&gt;
&lt;li&gt;If the application uses system environment variables to determine its configuration, you can use the injection of the ConfigMap data to create an environment variable mapping into the pod. There are two main ways to do this: mounting every key/value pair in the ConfigMap as a series of environment variables into the pod using envFrom and then using configMapRef or secretRef, or assigning individual keys with their respective values using the configMapKeyRef or secretKeyRef.&lt;/li&gt;
&lt;li&gt;If you’re using the configMapKeyRef or secretKeyRef method, be aware that if the actual key does not exist, this will prevent the pod from starting.&lt;/li&gt;
&lt;li&gt;If you’re loading all of the key/value pairs from the ConfigMap/Secret into the pod using envFrom, any keys that are considered invalid environment values will be skipped; however, the pod will be allowed to start. The event for the pod will have an event with reason InvalidVariableNames and the appropriate message about which key was skipped. The following code is an example of a Deployment with a ConfigMap and Secret reference as an environment variable&lt;/li&gt;
&lt;li&gt;If there is a need to pass command-line arguments to your containers, environment variable data can be sourced using $(ENV_KEY) interpolation syntax.&lt;/li&gt;
&lt;li&gt;When consuming ConfigMap/Secret data as environment variables, it is very important to understand that updates to the data in the ConfigMap/Secret will not update in the pod and will require a pod restart either through deleting the pods and letting the ReplicaSet controller create a new pod, or triggering a Deployment update, which will follow the proper application update strategy as declared in the Deployment specification.&lt;/li&gt;
&lt;li&gt;It is easier to assume that all changes to a ConfigMap/Secret require an update to the entire deployment; this ensures that even if you’re using environment variables or volumes, the code will take the new configuration data. To make this easier, you can use a CI/CD pipeline to update the name property of the ConfigMap/Secret and also update the reference in the deployment, which will then trigger an update through normal Kubernetes update strategies of your deployment. We will explore this in the following example code. If you’re using Helm to release your application code into Kubernetes, you can take advantage of an annotation in the Deployment template to check the sha256 checksum of the ConfigMap/Secret. This triggers Helm to update the Deployment using the helm upgrade command when the data within a ConfigMap/Secret is changed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;best-practices-specific-to-secrets&#34;&gt;Best Practices Specific to Secrets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Use 3rd party solutions to allow the use of external storage systems for secret data, such as HashiCorp Vault, Aqua Security, Twistlock, AWS Secrets Manager, Google Cloud KMS, or Azure Key Vault&lt;/li&gt;
&lt;li&gt;Assign an imagePullSecrets to a serviceaccount that the pod will use to automatically mount the secret without having to declare it in the pod.spec.&lt;/li&gt;
&lt;li&gt;Use CI/CD capabilities to get secrets from a secure vault or encrypted store with a Hardware Security Module (HSM) during the release pipeline.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;rbac-best-practices&#34;&gt;RBAC Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Applications that are developed to run in Kubernetes rarely ever need an RBAC role and role binding associated to it. Only if the application code actually interacts directly with the Kubernetes API directly does the application require RBAC configuration.&lt;/li&gt;
&lt;li&gt;If the application does need to directly access the Kubernetes API to perhaps change configuration depending on endpoints being added to a service, or if it needs to list all of the pods in a specific namespace, the best practice is to create a new service account that is then specified in the pod specification. Then, create a role that has the least amount of privileges needed to accomplish its goal.&lt;/li&gt;
&lt;li&gt;Use an OpenID Connect service that enables identity management and, if needed, two-factor authentication. This will allow for a higher level of identity authentication. Map user groups to roles that have the least amount of privileges needed to accomplish the job.&lt;/li&gt;
&lt;li&gt;Along with the aforementioned practice, you should use Just in Time (JIT) access systems to allow site reliability engineers (SREs), operators, and those who might need to have escalated privileges for a short period of time to accomplish a very specific task. Alternatively, these users should have different identities that are more heavily audited for sign-on, and those accounts should have more elevated privileges assigned by the user account or group bound to a role.&lt;/li&gt;
&lt;li&gt;Specific service accounts should be used for CI/CD tools that deploy into your Kubernetes clusters. This ensures for auditability within the cluster and an understanding of who might have deployed or deleted any objects in a cluster.&lt;/li&gt;
&lt;li&gt;Limit any applications that require watch and list on the Secrets API. This basically allows the application or the person who deployed the pod to view the secrets in that namespace. If an application needs to access the Secrets API for specific secrets, limit using get on any specific secrets that the application needs to read outside of those that it is directly assigned.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Service accounts in Kubernetes are different than user accounts in that they are namespace bound, internally stored in Kubernetes; they are meant to represent processes, not people, and are managed by native Kubernetes controllers.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;best-practices-for-cicd&#34;&gt;Best Practices for CI/CD&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;With CI, focus on automation and providing quick builds. Optimizing the build speed will provide developers quick feedback if their changes have broken the build.&lt;/li&gt;
&lt;li&gt;Focus on providing reliable tests in your pipeline. This will give developers rapid feedback on issues with their code. The faster the feedback loop to developers, the more productive they’ll become in their workflow.&lt;/li&gt;
&lt;li&gt;When deciding on CI/CD tools, ensure that the tools allow you to define the pipeline as code. This will allow you to version-control the pipeline with your application code.&lt;/li&gt;
&lt;li&gt;Ensure that you optimize your images so that you can reduce the size of the image and also reduce the attack surface when running the image in production. Multistage Docker builds allow you to remove packages not needed for the application to run. For example, you might need Maven to build the application, but you don’t need it for the actual running image.&lt;/li&gt;
&lt;li&gt;Avoid using “latest” as an image tag, and utilize a tag that can be referenced back to the buildID or Git commit.&lt;/li&gt;
&lt;li&gt;If you are new to CD, utilize Kubernetes rolling upgrades to start out. They are easy to use and will get you comfortable with deployment. As you become more comfortable and confident with CD, look at utilizing blue/green and canary deployment strategies.&lt;/li&gt;
&lt;li&gt;With CD, ensure that you test how client connections and database schema upgrades are handled in your application.&lt;/li&gt;
&lt;li&gt;Testing in production will help you build reliability into your application, and ensure that you have good monitoring in place. With testing in production, also start at a small scale and limit the blast radius of the experiment.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Including both application code and configuration code, such as a Kubernetes manifest or Helm charts, helps promote good DevOps principles of communication and collaboration. Having both application developers and operation engineers collaborate in a single repository builds confidence in a team to deliver an application to production.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h4 id=&#34;build-the-smallest-image&#34;&gt;Build the smallest image&lt;/h4&gt;
&lt;p&gt;Build the smallest image possible for your application:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multistage builds&lt;/li&gt;
&lt;li&gt;Distroless base images&lt;/li&gt;
&lt;li&gt;Optimized base images&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;container-image-tagging&#34;&gt;Container Image Tagging&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;BuildID&lt;/li&gt;
&lt;li&gt;Build System-BuildID&lt;/li&gt;
&lt;li&gt;Git Hash&lt;/li&gt;
&lt;li&gt;Githsah-buildID&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;deployment-strategies&#34;&gt;Deployment Strategies&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Rolling updates&lt;/li&gt;
&lt;li&gt;Blue/green deployments&lt;/li&gt;
&lt;li&gt;Canary deployments&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;worldwide-rollout-best-practices&#34;&gt;Worldwide Rollout Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Distribute each image around the world. A successful rollout depends on the release bits (binaries, images, etc.) being nearby to where they will be used. This also ensures reliability of the rollout in the presence of networking slowdowns or irregularities. Geographic distribution should be a part of your automated release pipeline for guaranteed consistency.&lt;/li&gt;
&lt;li&gt;Shift as much of your testing as possible to the left by having as much extensive integration and replay testing of your application as possible. You want to start a rollout only with a release that you strongly believe to be correct.&lt;/li&gt;
&lt;li&gt;Begin a release in a canary region, which is a preproduction environment in which other teams or large customers can validate their use of your service before you begin a larger-scale rollout.&lt;/li&gt;
&lt;li&gt;Identify different characteristics of the regions where you are rolling out. Each difference can be one that causes a failure and a full or partial outage. Try to roll out to low-risk regions first.&lt;/li&gt;
&lt;li&gt;Document and practice your response to any problem or process (e.g., a rollback) that you might encounter. Trying to remember what to do in the heat of the moment is a recipe for forgetting something and making a bad problem worse.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;resource-management-best-practices&#34;&gt;Resource Management Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Utilize pod anti-affinity to spread workloads across multiple availability zones to ensure high availability for your application.&lt;/li&gt;
&lt;li&gt;If you’re using specialized hardware, such as GPU-enabled nodes, ensure that only workloads that need GPUs are scheduled to those nodes by utilizing taints.&lt;/li&gt;
&lt;li&gt;Use NodeCondition taints to proactively avoid failing or degraded nodes.&lt;/li&gt;
&lt;li&gt;Apply nodeSelectors to your pod specifications to schedule pods to specialized hardware that you have deployed in the cluster.&lt;/li&gt;
&lt;li&gt;Before going to production, experiment with different node sizes to find a good mix of cost and performance for node types.&lt;/li&gt;
&lt;li&gt;If you’re deploying a mix of workloads with different performance characteristics, utilize node pools to have mixed node types in a single cluster.&lt;/li&gt;
&lt;li&gt;Ensure that you set memory and CPU limits for all pods deployed to your cluster.&lt;/li&gt;
&lt;li&gt;Utilize ResourceQuotas to ensure that multiple teams or applications are alotted their fair share of resources in the cluster.&lt;/li&gt;
&lt;li&gt;Implement LimitRange to set default limits and requests for pod specifications that don’t set limits or requests.&lt;/li&gt;
&lt;li&gt;Start with manual cluster scaling until you understand your workload profiles on Kubernetes. You can use autoscaling, but it comes with additional considerations around node spin-up time and cluster scale down.&lt;/li&gt;
&lt;li&gt;Use the HPA for workloads that are variable and that have unexpected spikes in their usage.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;services-in-kubernetes&#34;&gt;Services in Kubernetes&lt;/h3&gt;
&lt;p&gt;Service Types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ClusterIP&lt;/li&gt;
&lt;li&gt;NodePort&lt;/li&gt;
&lt;li&gt;ExternalName&lt;/li&gt;
&lt;li&gt;LoadBalancer&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;services-and-ingress-controllers-best-practices&#34;&gt;Services and Ingress Controllers Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Limit the number of services that need to be accessed from outside the cluster. Ideally, most services will be ClusterIP, and only external-facing services will be exposed externally to the cluster.&lt;/li&gt;
&lt;li&gt;If the services that need to be exposed are primarily HTTP/HTTPS-based services, it is best to use an Ingress API and Ingress controller to route traffic to backing services with TLS termination. Depending on the type of Ingress controller used, features such as rate limiting, header rewrites, OAuth authentication, observability, and other services can be made available without having to build them into the applications themselves.&lt;/li&gt;
&lt;li&gt;Choose an Ingress controller that has the needed functionality for secure ingress of your web-based workloads. Standardize on one and use it across the enterprise because many of the specific configuration annotations vary between implementations and prevent the deployment code from being portable across enterprise Kubernetes implementations.&lt;/li&gt;
&lt;li&gt;Evaluate cloud service provider-specific Ingress controller options to move the infrastructure management and load of the ingress out of the cluster, but still allow for Kubernetes API configuration.&lt;/li&gt;
&lt;li&gt;When serving mostly APIs externally, evaluate API-specific Ingress controllers, such as Kong or Ambassador, that have more fine-tuning for API-based workloads. Although NGINX, Traefik, and others might offer some API tuning, it will not be as fine-grained as specific API proxy systems.&lt;/li&gt;
&lt;li&gt;When deploying Ingress controllers as pod-based workloads in Kubernetes, ensure that the deployments are designed for high availability and aggregate performance throughput. Use metrics observability to properly scale the ingress, but include enough cushion to prevent client disruptions while the workload scales.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;network-policy-best-practices&#34;&gt;Network Policy Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Start off slow and focus on traffic ingress to pods. Complicating matters with ingress and egress rules can make network tracing a nightmare. As soon as traffic is flowing as expected, you can begin to look at egress rules to further control flow to sensitive workloads. The specification also favors ingress because it defaults many options even if nothing is entered into the ingress rules list.&lt;/li&gt;
&lt;li&gt;Ensure that the network plug-in used either has some of its own interface to the NetworkPolicy API or supports other well-known plug-ins. Example plug-ins include Calico, Cilium, Kube-router, Romana, and Weave Net.&lt;/li&gt;
&lt;li&gt;If the network team is used to having a “default-deny” policy in place, create a network policy such as the following for each namespace in the cluster that will contain workloads to be protected. This ensures that even if another network policy is deleted, no pods are accidentally “exposed”.&lt;/li&gt;
&lt;li&gt;If there are pods that need to be accessed from the internet, use a label to explicitly apply a network policy that allows ingress. Be aware of the entire flow in case the actual IP that a packet is coming from is not the internet, but the internal IP of a load balancer, firewall, or other network device. For example, to allow traffic from all (including external) sources for pods having the allow-internet=true label, do this:&lt;/li&gt;
&lt;li&gt;Try to align application workloads to single namespaces for ease of creating rules because the rules themselves are namespace specific. If cross-namespace communication is needed, try to be as explicit as possible and perhaps use specific labels to identify the flow pattern:&lt;/li&gt;
&lt;li&gt;Have a test bed namespace that has fewer restrictive policies, if any at all, to allow time to investigate the correct traffic patterns needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;service-mesh-best-practices&#34;&gt;Service Mesh Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Rate the importance of the key features service meshes offer and determine which current offerings provide the most important features with the least amount of overhead. Overhead here is both human technical debt and infrastructure resource debt. If all that is really required is mutual TLS between certain pods, would it be easier to perhaps find a CNI that offers that integrated into the plug-in?&lt;/li&gt;
&lt;li&gt;Is the need for a cross-system mesh such as multicloud or hybrid scenarios a key requirement? Not all service meshes offer this capability, and if they do, it is a complicated process that often introduces fragility into the environment.&lt;/li&gt;
&lt;li&gt;Many of the service mesh offerings are open source community-based projects, and if the team that will be managing the environment is new to service meshes, commercially supported offerings might be a better option. There are companies that are beginning to offer commercially supported and managed service meshes based on Istio, which can be helpful because it is almost universally agreed upon that Istio is a complicated system to manage.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;podsecuritypolicy-best-practices&#34;&gt;PodSecurityPolicy Best Practices&lt;/h3&gt;
&lt;p&gt;PodSecurityPolicy is complex and can be error prone.&lt;/p&gt;
&lt;div class=&#34;alert alert-warning&#34;&gt;
  &lt;div&gt;
    &lt;ul&gt;
&lt;li&gt;Proceed with caution when enabling PodSecurityPolicy because it’s potentially workload blocking if adequate preparation isn’t done at the outset.&lt;/li&gt;
&lt;li&gt;If you are enabling PodSecurityPolicy on an existing cluster with running workloads, you must create all necessary policies, service accounts, roles, and role bindings before enabling the admission controller.&lt;/li&gt;
&lt;li&gt;It’s extremely important to remember that having no PodSecurityPolicies defined will result in an implicit deny. This means that without a policy match for the workload, the pod will not be created.&lt;/li&gt;
&lt;li&gt;You should not enable PodSecurityPolicy on a live cluster without considering the warnings provided in the previous section. Proceed with caution.&lt;/li&gt;
&lt;/ul&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;It all comes down to RBAC. Whether you like it or not, PodSecurityPolicy is determined by RBAC. It’s this relationship that actually exposes all of the shortcomings in your current RBAC policy design. We cannot stress just how important it is to automate your RBAC and PodSecurityPolicy creation and maintenance. Specifically locking down access to service accounts is the key to using policy.&lt;/li&gt;
&lt;li&gt;Understand the policy scope. Determining how your policies will be laid out on your cluster is very important. Your policies can be cluster-wide, namespaced, or workload-specific in scope. There will always be workloads on your cluster that are part of the Kubernetes cluster operations that will need more permissive security privileges, so make sure that you have appropriate RBAC in place to stop unwanted workloads using your permissive policies.&lt;/li&gt;
&lt;li&gt;Do you want to enable PodSecurityPolicy on an existing cluster? Use this handy open source tool to generate policies based on your current resources. This is a great start. From there, you can hone your policies.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;policy-and-governance-for-your-cluster&#34;&gt;Policy and Governance for Your Cluster&lt;/h3&gt;
&lt;p&gt;Learn about &lt;strong&gt;Gatekeeper&lt;/strong&gt;&lt;br&gt;
Gatekeeper is an open source customizable Kubernetes admission webhook for cluster policy and governance. Gatekeeper takes advantage of the OPA constraint framework to enforce custom resource definition (CRD)-based policies. Using CRDs allows for an integrated Kubernetes experience that decouples policy authoring from implementation. Policy templates are referred to as constraint templates, which can be shared and reused across clusters. Gatekeeper enables resource validation and audit functionality. One of the great things about Gatekeeper is that it’s portable, which means that you can implement it on any Kubernetes clusters, and if you are already using OPA, you might be able to port that policy over to Gatekeeper.&lt;/p&gt;
&lt;h3 id=&#34;managing-multiple-clusters-best-practices&#34;&gt;Managing Multiple Clusters Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Limit the blast radius of your clusters to ensure cascading failures don’t have a bigger impact on your applications.&lt;/li&gt;
&lt;li&gt;If you have regulatory concerns such as PCI, HIPPA, or HiTrust, think about utilizing multiclusters to ease the complexity of mixing these workloads with general workloads.&lt;/li&gt;
&lt;li&gt;If hard multitenancy is a business requirement, workloads should be deployed to a dedicated cluster.&lt;/li&gt;
&lt;li&gt;If multiple regions are needed for your applications, utilize a Global Load Balancer to manage traffic between clusters.&lt;/li&gt;
&lt;li&gt;You can break out specialized workloads such as HPC into their own individual clusters to ensure that the specialized needs for the workloads are met.&lt;/li&gt;
&lt;li&gt;If you’re deploying workloads that will be spread across multiple regional datacenters, first ensure that there is a data replication strategy for the workload. Multiple clusters across regions can be easy, but replicating data across regions can be complicated, so ensure that there is a sound strategy to handle asynchronous and synchronous workloads.&lt;/li&gt;
&lt;li&gt;Utilize Kubernetes operators like the prometheus-operator or Elasticsearch operator to handle automated operational tasks.&lt;/li&gt;
&lt;li&gt;When designing your multicluster strategy, also consider how you will do service discovery and networking between clusters. Service mesh tools like HashiCorp’s Consul or Istio can help with networking across clusters.&lt;/li&gt;
&lt;li&gt;Be sure that your CD strategy can handle multiple rollouts between regions or multiple clusters.&lt;/li&gt;
&lt;li&gt;Investigate utilizing a GitOps approach to managing multiple cluster operational components to ensure consistency between all clusters in your fleet. The GitOps approach doesn’t always work for everyone’s environment, but you should at least investigate it to ease the operational burden of multicluster environments.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;why-multiple-clusters&#34;&gt;Why Multiple Clusters?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Blast radius&lt;/li&gt;
&lt;li&gt;Compliance&lt;/li&gt;
&lt;li&gt;Security&lt;/li&gt;
&lt;li&gt;Hard multitenancy&lt;/li&gt;
&lt;li&gt;Regional-based workloads&lt;/li&gt;
&lt;li&gt;Specialized workloads&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;multicluster-design-concerns&#34;&gt;Multicluster Design Concerns&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Data replication&lt;/li&gt;
&lt;li&gt;Service discovery&lt;/li&gt;
&lt;li&gt;Network routing&lt;/li&gt;
&lt;li&gt;Operational management&lt;/li&gt;
&lt;li&gt;Continuous deployment&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;managing-multiple-cluster-deployments&#34;&gt;Managing Multiple Cluster Deployments&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Operator&lt;/li&gt;
&lt;li&gt;GitOps&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;multicluster-management-tools&#34;&gt;Multicluster Management Tools&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Rancher&lt;br&gt;
centrally manages multiple Kubernetes clusters in a centrally managed user interface (UI).&lt;/li&gt;
&lt;li&gt;KQueen&lt;br&gt;
multitenant self-service portal for Kubernetes cluster provisioning and focuses on auditing, visibility, and security of multiple Kubernetes clusters.&lt;/li&gt;
&lt;li&gt;Gardener&lt;br&gt;
provide Kubernetes as a Service to your end users&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;kubernetes-federation&#34;&gt;Kubernetes Federation&lt;/h4&gt;
&lt;p&gt;KubeFed is not necessarily about multicluster management, but providing high availability (HA) deployments across multiple clusters. It allows you to combine multiple clusters into a single management endpoint for delivering applications on Kubernetes. For example, if you have a cluster that resides in multiple public cloud environments, you can combine these clusters into a single control plane to manage deployments to all clusters to increase the resiliency of your application.&lt;/p&gt;
&lt;h3 id=&#34;connecting-cluster-and-external-services-best-practices&#34;&gt;Connecting Cluster and External Services Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Establish network connectivity between the cluster and on-premises. Networking can be varied between different sites, clouds, and cluster configurations, but first ensure that pods can talk to on-premises machines and vice versa.&lt;/li&gt;
&lt;li&gt;To access services outside of the cluster, you can use selector-less services and directly program in the IP address of the machine (e.g., the database) with which you want to communicate. If you don’t have fixed IP addressess, you can instead use CNAME services to redirect to a DNS name. If you have neither a DNS name nor fixed services, you might need to write a dynamic operator that periodically synchronizes the external service IP addresses with the Kubernetes Service endpoints.&lt;/li&gt;
&lt;li&gt;To export services from Kubernetes, use internal load balancers or NodePort services. Internal load balancers are typically easier to use in public cloud environments where they can be bound to the Kubernetes Service itself. When such load balancers are unavailable, NodePort services can expose the service on all of the machines in the cluster.&lt;/li&gt;
&lt;li&gt;You can achieve connections between Kubernetes clusters through a combination of these two approaches, exposing a service externally that is then consumed as a selector-less service in the other Kubernetes cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;machine-leaning-on-kubernetes-best-practices&#34;&gt;Machine Leaning on Kubernetes Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Smart scheduling and autoscaling. Given that most stages of the machine learning workflow are batch by nature, we recommend that you utilize a Cluster Autoscaler. GPU-enabled hardware is costly, and you certainly do not want to be paying for it when it’s not in use. We recommend batching jobs to run at specific times using either taints and tolerations or via a time-specific Cluster Autoscaler. That way, the cluster can scale to the needs of the machine learning workloads when needed, and not a moment sooner. Regarding taints and tolerations, upstream convention is to taint the node with the extended resource as the key. For example, a node with NVIDIA GPUs should be tainted as follows: Key: nvidia.com/gpu, Effect: NoSchedule. Using this method means that you can also utilize the ExtendedResourceToleration admission controller, which will automatically add the appropriate tolerations for such taints to pods requesting extended resources so that the users don’t need to manually add them.&lt;/li&gt;
&lt;li&gt;The truth is that model training is a delicate balance. Allowing things to move faster in one area often leads to bottlenecks in others. It’s an endeavor of constant observation and tuning. As a general rule of thumb, we recommend that you try to make the GPU become the bottleneck because it is the most costly resource. Keep your GPUs saturated. Be prepared to always be on the lookout for bottlenecks, and set up your monitoring to track the GPU, CPU, network, and storage utilization.&lt;/li&gt;
&lt;li&gt;Mixed workload clusters. Clusters that are used to run the day-to-day business services might also be used for the purposes of machine learning. Given the high performance requirements of machine learning workloads, we recommend using a separate node pool that’s tainted to accept only machine learning workloads. This will help protect the rest of the cluster from any impact from the machine learning workloads running on the machine learning node pool. Furthermore, you should consider multiple GPU-enabled node pools, each with different performance characteristics to suit the workload types. We also recommend enabling node autoscaling on the machine learning node pool(s). Use mixed mode clusters only after you have a solid understanding of the performance impact that your machine learning workloads have on your cluster.&lt;/li&gt;
&lt;li&gt;Achieving linear scaling with distributed training. This is the holy grail of distributed model training. Most libraries unfortunately don’t scale in a linear fashion when distributed. There is lots of work being done to make scaling better, but it’s important to understand the costs because this isn’t as simple as throwing more hardware at the problem. In our experience, it’s almost always the model itself and not the infrastructure supporting it that is the source of the bottleneck. It is, however, important to review the utilization of the GPU, CPU, network, and storage before pointing fingers at the model itself. Open source tools such as Horovod seek to improve distributed training frameworks and provide better model scaling.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;building-application-platforms-best-practices&#34;&gt;Building Application Platforms Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Use admission controllers to limit and modify API calls to the cluster. An admission controller can validate (and reject invalid) Kubernetes resources. A mutating admission controller can automatically modify API resources to add new sidecars or other changes that users might not even need to know about.&lt;/li&gt;
&lt;li&gt;Use kubectl plug-ins to extend the Kubernetes user experience by adding new tools to the familiar existing command-line tool. In rare occasions, a purpose-built tool might be more appropriate.&lt;/li&gt;
&lt;li&gt;When building platforms on top of Kubernetes, think carefully about the users of the platform and how their needs will evolve. Making things simple and easy to use is clearly a good goal, but if this also leads to users that are trapped and unable to be successful without rewriting everything outside of your platform, it will ultimately be a frustrating (and unsuccessful) experience.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;managing-state-and-stateful-applications&#34;&gt;Managing State and Stateful Applications&lt;/h3&gt;
&lt;h4 id=&#34;volume-best-practices&#34;&gt;Volume Best Practices&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Try to limit the use of volumes to pods requiring multiple containers that need to share data, for example adapter or ambassador type patterns. Use the emptyDir for those types of sharing patterns.&lt;/li&gt;
&lt;li&gt;Use hostDir when access to the data is required by node-based agents or services.&lt;/li&gt;
&lt;li&gt;Try to identify any services that write their critical application logs and events to local disk, and if possible change those to stdout or stderr and let a true Kubernetes-aware log aggregation system stream the logs instead of leveraging the volume map.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;kubernetes-storage-best-practices&#34;&gt;Kubernetes Storage Best Practices&lt;/h4&gt;
&lt;p&gt;Cloud native application design principles try to enforce stateless application design as much as possible; however, the growing footprint of container-based services has created the need for data storage persistence. These best practices around storage in Kubernetes in general will help to design an effective approach to providing the required storage implementations to the application design:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If possible, enable the DefaultStorageClass admission plug-in and define a default storage class. Many times, Helm charts for applications that require PersistentVolumes default to a default storage class for the chart, which allows the application to be installed without too much modification.&lt;/li&gt;
&lt;li&gt;When designing the architecture of the cluster, either on-premises or in a cloud provider, take into consideration zone and connectivity between the compute and data layers using the proper labels for both nodes and PersistentVolumes, and using affinity to keep the data and workload as close as possible. The last thing you want is a pod on a node in zone A trying to mount a volume that is attached to a node in zone B.&lt;/li&gt;
&lt;li&gt;Consider very carefully which workloads require state to be maintained on disk. Can that be handled by an outside service like a database system or, if running in a cloud provider, by a hosted service that is API consistent with currently used APIs, say a mongoDB or mySQL as a service?&lt;/li&gt;
&lt;li&gt;Determine how much effort would be involved in modifying the application code to be more stateless.&lt;/li&gt;
&lt;li&gt;While Kubernetes will track and mount the volumes as workloads are scheduled, it does not yet handle redundancy and backup of the data that is stored in those volumes. The CSI specification has added an API for vendors to plug in native snapshot technologies if the storage backend can support it.&lt;/li&gt;
&lt;li&gt;Verify the proper life cycle of the data that volumes will hold. By default the reclaim policy is set to for dynamically provisioned persistentVolumes which will delete the volume from the backing storage provider when the pod is deleted. Sensitive data or data that can be used for forensic analysis should be set to reclaim.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;statefulset-and-operator-best-practices&#34;&gt;StatefulSet and Operator Best Practices&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The decision to use Statefulsets should be taken judiciously because usually stateful applications require much deeper management that the orchestrator cannot really manage well yet (read the “Operators” section for the possible future answer to this deficiency in Kubernetes).&lt;/li&gt;
&lt;li&gt;The headless Service for the StatefulSet is not automatically created and must be created at deployment time to properly address the pods as individual nodes.&lt;/li&gt;
&lt;li&gt;When an application requires ordinal naming and dependable scaling, it does not always mean it requires the assignment of PersistentVolumes.&lt;/li&gt;
&lt;li&gt;If a node in the cluster becomes unresponsive, any pods that are part of a StatefulSet are not not automatically deleted; they instead will enter a Terminating or Unkown state after a grace period. The only way to clear this pod is to remove the node object from the cluster, the kubelet beginning to work again and deleting the pod directly, or an Operator force deleting the pod. The force delete should be the last option and great care should be taken that the node that had the deleted pod does not come back online, because there will now be two pods with the same name in the cluster. You can use kubectl delete pod nginx-0 &amp;ndash;grace-period=0 &amp;ndash;force to force delete the pod.&lt;/li&gt;
&lt;li&gt;Even after force deleting a pod, it might stay in an Unknown state, so a patch to the API server will delete the entry and cause the StatefulSet controller to create a new instance of the deleted pod: kubectl patch pod nginx-0 -p &amp;lsquo;{&amp;lsquo;metadata&amp;rsquo;:{&amp;lsquo;finalizers&amp;rsquo;:null}}&amp;rsquo;.&lt;/li&gt;
&lt;li&gt;If you’re running a complex data system with some type of leader election or data replication confirmation processes, use preStop hook to properly close any connections, force leader election, or verify data synchronization before the pod is deleted using a graceful shutdown process.&lt;/li&gt;
&lt;li&gt;When the application that requires stateful data is a complex data management system, it might be worth a look to determine whether an Operator exists to help manage the more complicated life cycle components of the application. If the application is built in-house, it might be worth investigating whether it would be useful to package the application as an Operator to add additional manageability to the application. Look at the CoreOS Operator SDK for an example.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;replicaset-vs-statefulsets&#34;&gt;ReplicaSet vs. StatefulSets&lt;/h4&gt;
&lt;p&gt;ReplicaSet:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pods in a ReplicaSet are scaled out and assigned random names when scheduled.&lt;/li&gt;
&lt;li&gt;Pods in a ReplicaSet are scaled down in an arbitrary manner.&lt;/li&gt;
&lt;li&gt;Pods in a ReplicaSet are never called directly through their name or IP address but through their association with a Service.&lt;/li&gt;
&lt;li&gt;Pods in a ReplicaSet can be restarted and moved to another node at any time.&lt;/li&gt;
&lt;li&gt;Pods in a ReplicaSet that have a PersistentVolume mapped are linked only by the claim, but any new pod with a new name can take over the claim if needed when - rescheduled.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StatefulSets:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pods in a StatefulSet are scaled out and assigned sequential names. As the set scales up, the pods get ordinal names, and by default a new pod must be fully - online (pass its liveness and/or readiness probes) before the next pod is added.  -&lt;/li&gt;
&lt;li&gt;Pods in a StatefulSet are scaled down in reverse sequence.  -&lt;/li&gt;
&lt;li&gt;Pods in a StatefulSet can be addressed individually by name behind a headless Service.  -&lt;/li&gt;
&lt;li&gt;Pods in a StatefulSet that require a volume mount must use a defined PersistentVolume template. Volumes claimed by pods in a StatefulSet are not deleted when the StatefulSet is deleted.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;admission-control-best-practices&#34;&gt;Admission Control Best Practices&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Admission plug-in ordering doesn’t matter.&lt;/li&gt;
&lt;li&gt;Don’t mutate the same fields. Configuring multiple mutating admission webhooks also presents challenges.&lt;/li&gt;
&lt;li&gt;Fail open/fail closed. You might recall seeing the failurePolicy field as part of both the mutating and validating webhook configuration resources. This field defines how the API server should proceed in the case where the admission webhooks have access issues or encounter unrecognized errors. You can set this field to either Ignore or Fail. Ignore essentially fails to open, meaning that processing of the request will continue, whereas Fail denies the entire request. This might seem obvious, but the implications in both cases require consideration. Ignoring a critical admission webhook could result in policy that the business relies on not being applied to a resource without the user knowing.&lt;br&gt;
One potential solution to protect against this would be to raise an alert when the API server logs that it cannot reach a given admission webhook. Fail can be even more devastating by denying all requests if the admission webhook is experiencing issues. To protect against this you can scope the rules to ensure that only specific resource requests are set to the admission webhook. As a tenet, you should never have any rules that apply to all resources in the cluster.&lt;/li&gt;
&lt;li&gt;If you have written your own admission webhook, it’s important to remember that user/system requests can be directly affected by the time it takes for your admission webhook to make a decision and respond. All admission webhook calls are configured with a 30-second timeout, after which time the failurePolicy takes effect. Even if it takes several seconds for your admission webhook to make an admit/deny decision, it can severely affect user experience when working with the cluster. Avoid having complex logic or relying on external systems such as databases in order to process the admit/deny logic.&lt;/li&gt;
&lt;li&gt;Scoping admission webhooks. There is an optional field that allows you to scope the namespaces in which the admission webhooks operate on via the NamespaceSelector field. This field defaults to empty, which matches everything, but can be used to match namespace labels via the use of the matchLabels field. We recommend that you always use this field because it allows for an explicit opt-in per namespace.&lt;/li&gt;
&lt;li&gt;The kube-system namespace is a reserved namespace that’s common across all Kubernetes clusters. It’s where all system-level services operate. We recommend never running admission webhooks against the resources in this namespace specifically, and you can achieve this by using the NamespaceSelector field and simply not matching the kube-system namespace. You should also consider it on any system-level namespaces that are required for cluster operation.&lt;/li&gt;
&lt;li&gt;Lock down admission webhook configurations with RBAC. Now that you know about all the fields in the admission webhook configuration, you have probably thought of a really simple way to break access to a cluster. It goes without saying that the creation of both a MutatingWebhookConfiguration and ValidatingWebhookConfiguration is a root-level operation on the cluster and must be locked down appropriately using RBAC. Failure to do so can result in a broken cluster or, even worse, an injection attack on your application workloads.&lt;/li&gt;
&lt;li&gt;Don’t send sensitive data. Admission webhooks are essentially black boxes that accept AdmissionRequests and output AdmissionResponses. How they store and manipulate the request is opaque to the user. It’s important to think about what request payloads you are sending to the admission webhook. In the case of Kubernetes secrets or ConfigMaps, they might contain sensitive information and require strong guarantees about how that information is stored and shared. Sharing these resources with an admission webhook can leak sensitive information, which is why you should scope your resource rules to the minimum resource needed to validate and/or mutate.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;authorization&#34;&gt;Authorization&lt;/h3&gt;
&lt;p&gt;In Kubernetes, the authorization of each request is performed after authentication but before admission.&lt;br&gt;
Unlike admission controllers, if a single authorization module admits the request, the request can proceed. Only for the case in which all modules deny the request will an error be returned to the user.&lt;/p&gt;
&lt;h4 id=&#34;authorization-modules&#34;&gt;Authorization Modules&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Attribute-Based Access Control (ABAC)&lt;br&gt;
Allows authorization policy to be configured via local files&lt;/li&gt;
&lt;li&gt;RBAC&lt;br&gt;
Allows authorization policy to be configured via the Kubernetes API&lt;/li&gt;
&lt;li&gt;Webhook&lt;br&gt;
Allows the authorization of a request to be handled via a remote REST endpoint&lt;/li&gt;
&lt;li&gt;Node&lt;br&gt;
Specialized authorization module that authorizes requests from kubelets&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;authorization-best-practices&#34;&gt;Authorization Best Practices&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Given that the ABAC policies need to be placed on the filesystem of each master node and kept synchronized, we generally recommend against using ABAC in multimaster clusters. The same can be said for the webhook module because the configuration is based on a file and a corresponding flag being present. Furthermore, changes to these policies in the files require a restart of the API server to take effect, which is effectively a control-plane outage in a single master cluster or inconsistent configuration in a multimaster cluster. Given these details, we recommend using only the RBAC module for user authorization because the rules are configured and stored in Kubernetes itself.&lt;/li&gt;
&lt;li&gt;Webhook modules, although powerful, are potentially very dangerous. Given that every request is subject to the authorization process, a failure of a webhook service would be devastating for a cluster. Therefore, we generally recommend not using external authorization modules unless you completely vet and are comfortable with your cluster failure modes if the webhook service becomes unreachable or unavailable.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Please refer to &lt;a href=&#34;https://www.amazon.com/Kubernetes-Best-Practices-Blueprints-Applications/dp/1492056472/ref=sr_1_3?crid=22K492WLGAPTP&amp;amp;dchild=1&amp;amp;keywords=kubernetes+best+practices&amp;amp;qid=1602558033&amp;amp;sprefix=Kubernetes+Best+Practices%2Caps%2C176&amp;amp;sr=8-3&#34;&gt;the book&lt;/a&gt; for details.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.amazon.com/Kubernetes-Best-Practices-Blueprints-Applications/dp/1492056472/ref=sr_1_3?crid=22K492WLGAPTP&amp;amp;dchild=1&amp;amp;keywords=kubernetes&amp;#43;best&amp;#43;practices&amp;amp;qid=1602558033&amp;amp;sprefix=Kubernetes&amp;#43;Best&amp;#43;Practices%2Caps%2C176&amp;amp;sr=8-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Best Practices: Blueprints for Building Successful Applications on Kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>Docker</title>
      <link>/devops/docker/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/devops/docker/</guid>
      <description>&lt;h2 id=&#34;docker&#34;&gt;Docker&lt;/h2&gt;
&lt;h3 id=&#34;what-is-docker&#34;&gt;What is Docker?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt; is basically a container engine which uses the Linux Kernel features like &lt;strong&gt;namespaces&lt;/strong&gt; and &lt;strong&gt;control groups&lt;/strong&gt; to create containers on top of an operating system and automates application deployment on the container. Docker uses &lt;strong&gt;Copy-on-write union file system&lt;/strong&gt; for its &lt;strong&gt;backend storage&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;what-is-the-difference-between-a-docker-container-and-a-docker-image&#34;&gt;What is the difference between a docker container and a docker image?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Docker Image&lt;/strong&gt; is a set of files which has no state, whereas &lt;strong&gt;Docker Container&lt;/strong&gt; is the instantiation of Docker Image. In other words, Docker Container is the &lt;strong&gt;run time instance&lt;/strong&gt; of images.&lt;/p&gt;
&lt;h3 id=&#34;what-is-a-container&#34;&gt;What is a container?&lt;/h3&gt;
&lt;p&gt;In 4 bullet points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Containers share the host kernel&lt;/li&gt;
&lt;li&gt;Containers use the kernel ability to group processes for resource control&lt;/li&gt;
&lt;li&gt;Containers ensure isolation through namespaces&lt;/li&gt;
&lt;li&gt;Containers feel like lightweight VMs (lower footprint, faster), but are not Virtual Machines!&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;namespaces-and-cgroups----------------docker&#34;&gt;Namespaces and Cgroups  - - - - - - &amp;gt;  Docker&lt;/h3&gt;
&lt;p&gt;Docker makes use of kernel &lt;strong&gt;namespaces&lt;/strong&gt; to provide the &lt;strong&gt;isolated workspace&lt;/strong&gt; called the container.&lt;br&gt;
Docker also makes use of kernel &lt;strong&gt;control groups&lt;/strong&gt; for resource allocation and isolation.&lt;br&gt;
&lt;img src=&#34;./namespace.png&#34; alt=&#34;namespace&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;container-format&#34;&gt;Container Format&lt;/h3&gt;
&lt;p&gt;Docker Engine combines the &lt;strong&gt;namespaces&lt;/strong&gt;, &lt;strong&gt;control groups&lt;/strong&gt; and &lt;strong&gt;UnionFS&lt;/strong&gt; into a wrapper called a container format. The default container format is &lt;strong&gt;libcontainer&lt;/strong&gt;.&lt;/p&gt;
&lt;h3 id=&#34;types-of-containers&#34;&gt;Types of Containers&lt;/h3&gt;
&lt;p&gt;Given the above constructs, containers may be divided into 3 types as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;System Containers&lt;/strong&gt; share rootfs, PID, network, IPC and UTS with host system but live inside a cgroup.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Application Containers&lt;/strong&gt; live inside a cgroup and use namespaces (PID, network, IPC, chroot) for isolation from host system&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pods&lt;/strong&gt; use namespaces for isolation from host system but create sub groups which share PID, network, IPC and UTS except the rootfs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;images--layers&#34;&gt;Images &amp;amp; Layers&lt;/h3&gt;
&lt;p&gt;Each Docker image references a list of &lt;strong&gt;read-only layers&lt;/strong&gt; that represent filesystem differences. Layers are stacked on top of each other to form a base for a container’s rootfs.&lt;br&gt;
One big innovation of the Docker engine was the concept of leveraging Copy-On-Write file systems to significantly speed up the preparation of the rootfs.&lt;/p&gt;
&lt;h3 id=&#34;copy-on-write&#34;&gt;Copy-On-Write&lt;/h3&gt;
&lt;p&gt;When &lt;strong&gt;Docker&lt;/strong&gt; creates a container, it &lt;strong&gt;adds a new, thin, writable layer&lt;/strong&gt; on top of the underlying stack of image layers. This layer is often called the “container layer”.&lt;br&gt;
All changes made to the running container - such as writing new files, modifying existing files, and deleting files - are written to this thin writable container layer.&lt;/p&gt;
&lt;h3 id=&#34;union-file-systems&#34;&gt;Union File Systems&lt;/h3&gt;
&lt;p&gt;Union File Systems provide the following features for storage:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Layering&lt;/li&gt;
&lt;li&gt;Copy-On-Write&lt;/li&gt;
&lt;li&gt;Caching&lt;/li&gt;
&lt;li&gt;Diffing&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dangling-images&#34;&gt;Dangling images&lt;/h3&gt;
&lt;p&gt;Docker images consist of multiple layers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dangling images&lt;/strong&gt; are layers that have no relationship to any tagged images.&lt;/li&gt;
&lt;li&gt;They no longer serve a purpose and consume disk space.&lt;/li&gt;
&lt;li&gt;They can be located by adding the filter flag, &lt;strong&gt;-f&lt;/strong&gt; with a value of &lt;strong&gt;dangling=true&lt;/strong&gt; to the docker images command.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Another description:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An unused image means that it has not been assigned or used in a container. For example, when running docker ps -a - it will list all of your exited and currently running containers. Any images shown being used inside any of containers are a &amp;ldquo;used image&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;On the other hand, a dangling image just means that you&amp;rsquo;ve created the new build of the image, but it wasn&amp;rsquo;t given a new name. So the old images you have becomes the &amp;ldquo;dangling image&amp;rdquo;. Those old images are the ones that are untagged and displays &amp;ldquo;&lt;none&gt;&amp;rdquo; on its name when you run docker images.&lt;/li&gt;
&lt;li&gt;When running &lt;strong&gt;docker system prune -a&lt;/strong&gt;, it will remove both unused and dangling images. Therefore any images being used in a container, whether they have been exited or currently running, will NOT be affected.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-difference-between-copy-and-add-in-a-dockerfile&#34;&gt;The Difference between COPY and ADD in a Dockerfile&lt;/h3&gt;
&lt;p&gt;Sometimes you see &lt;strong&gt;COPY&lt;/strong&gt; or &lt;strong&gt;ADD&lt;/strong&gt; being used in a Dockerfile, but 99% of the time you should be using &lt;strong&gt;COPY&lt;/strong&gt;, here&amp;rsquo;s why.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;COPY&lt;/strong&gt; and &lt;strong&gt;ADD&lt;/strong&gt; are both Dockerfile instructions that serve similar purposes. They let you copy files from a specific location into a Docker image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;COPY&lt;/strong&gt; takes in a &lt;strong&gt;src&lt;/strong&gt; and &lt;strong&gt;destination&lt;/strong&gt;. It &lt;strong&gt;only&lt;/strong&gt; lets you copy in a local file or directory from your host (the machine building the Docker image) into the Docker image itself.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ADD&lt;/strong&gt; lets you do that too, but it also supports &lt;strong&gt;2 other sources&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First, you can use a &lt;strong&gt;URL&lt;/strong&gt; instead of a local file / directory.&lt;/li&gt;
&lt;li&gt;Secondly, you can extract a tar file from the source directly into the destination.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In most cases if you’re using a URL, you’re downloading a zip file and are then using the RUN command to extract it. However, you might as well just use RUN with curl instead of ADD here so you chain everything into 1 RUN command to &lt;strong&gt;make a smaller Docker image&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;A valid use case for &lt;strong&gt;ADD&lt;/strong&gt; is when you want to extract a local tar file into a specific directory in your Docker image. This is exactly what the Alpine image does with ADD rootfs.tar.gz /.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you’re copying in local files to your Docker image, always use COPY because it’s more explicit.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cmd-vs-entrypoint&#34;&gt;CMD vs. ENTRYPOINT&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Run or execute something when docker starts&lt;/li&gt;
&lt;li&gt;The main purpose of a CMD is to provide defaults for an executing container&lt;/li&gt;
&lt;li&gt;An ENTRYPOINT helps you to configure a container that you can run as an executable&lt;/li&gt;
&lt;li&gt;CMD can be overridden, The ENTRYPOINT instruction works very similarly to CMD in that it is used to specify the command executed when the container is started. However, where it differs is that ENTRYPOINT doesn’t allow you to override the command.&lt;/li&gt;
&lt;li&gt;CMD will be overridden by the ‘docker run …….’ command line, ENTRYPOINT just gets the parameter from ‘docker run …….’ command line&lt;/li&gt;
&lt;li&gt;One important thing to call out about the ENTRYPOINT instruction is that syntax is critical. Technically, ENTRYPOINT supports both the ENTRYPOINT [&amp;ldquo;command&amp;rdquo;] syntax and the ENTRYPOINT command syntax. However, while both of these are supported, they have two different meanings and change how ENTRYPOINT works.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;cmd-and-entrypoint-syntax&#34;&gt;CMD and ENTRYPOINT syntax&lt;/h3&gt;
&lt;p&gt;Both CMD and ENTRYPOINT are straight forward but they have a hidden, err, &amp;ldquo;feature&amp;rdquo; that can cause issues if you are not aware. Two different syntaxes are supported for these instructions.&lt;/p&gt;
&lt;p&gt;CMD /bin/echo&lt;br&gt;
or&lt;br&gt;
CMD [&amp;quot;/bin/echo&amp;rdquo;]&lt;br&gt;
This may not look like it would be an issues but the devil in the details will trip you up. If you use the second syntax where the CMD ( or ENTRYPOINT ) is an array, it acts exactly like you would expect. If you use the first syntax without the array, docker pre-pends /bin/sh -c to your command. This has always been in docker as far as I can remember.&lt;/p&gt;
&lt;p&gt;Pre-pending /bin/sh -c can cause some unexpected issues and functionality that is not easily understood if you did not know that docker modified your CMD. Therefore, you should always use the array syntax for both instructions because both will be executed exactly how you intended.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Always use the array syntax when using CMD and ENTRYPOINT.
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;docker-commands&#34;&gt;Docker commands&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;docker build&lt;/li&gt;
&lt;li&gt;docker pull&lt;/li&gt;
&lt;li&gt;docker push&lt;/li&gt;
&lt;li&gt;docker images …&lt;/li&gt;
&lt;li&gt;docker commit&lt;/li&gt;
&lt;li&gt;docker exec -it&lt;/li&gt;
&lt;li&gt;docker run -it&lt;/li&gt;
&lt;li&gt;docker system
&lt;ul&gt;
&lt;li&gt;df&lt;/li&gt;
&lt;li&gt;events&lt;/li&gt;
&lt;li&gt;info&lt;/li&gt;
&lt;li&gt;prune&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;docker ps&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;./docker-commands.png&#34; alt=&#34;docker-commands&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;docker-instructions-in-dockerfile&#34;&gt;Docker instructions in dockerfile&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;ENV&lt;/li&gt;
&lt;li&gt;RUN&lt;/li&gt;
&lt;li&gt;CMD&lt;/li&gt;
&lt;li&gt;ENTRYPOINT&lt;/li&gt;
&lt;li&gt;COPY&lt;/li&gt;
&lt;li&gt;ADD&lt;/li&gt;
&lt;li&gt;USER&lt;/li&gt;
&lt;li&gt;WORKDIR&lt;/li&gt;
&lt;li&gt;ARG&lt;/li&gt;
&lt;li&gt;EXPOSE&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;dockerfile-best-practices&#34;&gt;Dockerfile Best Practices&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.docker.com/develop/develop-images/dockerfile_best-practices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Best practices for writing Dockerfiles&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.docker.com/blog/intro-guide-to-dockerfile-best-practices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Intro Guide to Dockerfile Best Practices&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://docs.docker.com/develop/dev-best-practices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Docker development best practices&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://engineering.bitnami.com/articles/best-practices-writing-a-dockerfile.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Best practices writing a Dockerfile&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://nickjanetakis.com/blog/docker-tip-2-the-difference-between-copy-and-add-in-a-dockerile&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Difference between COPY and ADD in a Dockerfile&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>Whatis Locate Which Whereis</title>
      <link>/linux/whatis-locate-which-whereis/</link>
      <pubDate>Thu, 10 Sep 2020 00:00:00 +0000</pubDate>
      <guid>/linux/whatis-locate-which-whereis/</guid>
      <description>&lt;h2 id=&#34;whatis-locate-which-whereis&#34;&gt;Whatis Locate Which Whereis&lt;/h2&gt;
&lt;h3 id=&#34;whatis&#34;&gt;Whatis&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;hongwei@840-g5:~$ whatis whatis
whatis (1)           - display one-line manual page descriptions
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;locate&#34;&gt;Locate&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;hongwei@840-g5:~$ whatis locate
locate (1)           - find files by name
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;locate&lt;/strong&gt; indeed finds all the files that have the pattern specified anywhere in their paths. You can tell it to only find files and directories whose names (rather than full paths) include the pattern with the &lt;strong&gt;-b&lt;/strong&gt; option, which is usually what you want, and gives a less unwieldy list.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;locate&lt;/strong&gt; is fast because it uses a binary database that gets periodically updated (once daily, by cron). You can update it yourself to ensure recently added files are found by running &lt;strong&gt;sudo updatedb&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;One more thing about locate - &lt;strong&gt;it doesn&amp;rsquo;t care whether files still exist or not&lt;/strong&gt;, so to avoid finding recently deleted files, use -e. Often I also pipe to less as the list can be long. Typically I do:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo updatedb &amp;amp;&amp;amp; locate -b -e psql | less
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;which&#34;&gt;Which&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;hongwei@840-g5:~$ whatis which
which (1)            - locate a command
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;which&lt;/strong&gt; finds the binary executable of the program (if it is in your PATH)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;which&lt;/strong&gt; returns the  pathnames of the files (or links) which would be executed in the current environment, had its arguments been given as commands in a strictly POSIX-conformant shell.  It does this by searching the &lt;strong&gt;PATH&lt;/strong&gt; for executable files matching the names of the arguments. It does not canonicalize path names.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hongwei@840-g5:~$ which psql
/usr/bin/psql
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;whereis&#34;&gt;Whereis&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;hongwei@840-g5:~$ whatis whereis
whereis (1)          - locate the binary, source, and manual page files for a command
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;whereis&lt;/strong&gt; finds the binary, the source, and the man page files for a program.&lt;br&gt;
&lt;strong&gt;whereis&lt;/strong&gt; locates the binary, source and manual files for the specified command names.&lt;br&gt;
&lt;strong&gt;whereis&lt;/strong&gt; attempts to locate the desired program in the standard Linux places, and in the places specified by &lt;strong&gt;$PATH&lt;/strong&gt; and &lt;strong&gt;$MANPATH&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;hongwei@840-g5:~$ whereis psql
psql: /usr/bin/psql /usr/share/man/man1/psql.1.gz
hongwei@840-g5:~$ whereis postgresql
postgresql: /usr/lib/postgresql /etc/postgresql /usr/share/postgresql
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://askubuntu.com/questions/799776/what-is-the-difference-between-locate-whereis-which&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What is the difference between locate/whereis/which&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h4&gt;
</description>
    </item>
    
    <item>
      <title>Minikube</title>
      <link>/devops/minikube/</link>
      <pubDate>Wed, 01 Jul 2020 00:00:00 +0000</pubDate>
      <guid>/devops/minikube/</guid>
      <description>&lt;h2 id=&#34;minikube&#34;&gt;Minikube&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Minikube&lt;/strong&gt; is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day.&lt;/p&gt;
&lt;h3 id=&#34;set-up-minikube&#34;&gt;Set up Minikube&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://minikube.sigs.k8s.io/docs/start/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;minikube start&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;minikube-vm-login&#34;&gt;Minikube VM Login&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;On VM console:&lt;br&gt;
username: root
no password&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;from host terminal:&lt;br&gt;
minikube ssh&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;username+IP on host:&lt;br&gt;
username: docker&lt;br&gt;
password: tcuser&lt;br&gt;
i.e.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh docker@192.168.99.103
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./minikube-login.png&#34; alt=&#34;minikube-login&#34;&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh docker@$(minikube ip)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;./minikube-login-1.png&#34; alt=&#34;minikube-login-1&#34;&gt;&lt;/p&gt;
&lt;p&gt;Exit the login:
exit&lt;/p&gt;
&lt;h3 id=&#34;setting-a-vm-driver-by-default&#34;&gt;Setting a VM driver by default&lt;/h3&gt;
&lt;p&gt;minikube config set vm-driver virtualbox&lt;/p&gt;
&lt;h3 id=&#34;use-local-images-by-re-using-the-docker-daemon&#34;&gt;Use local images by re-using the Docker daemon&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How To Install and Use Docker on Ubuntu 18.04&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://kubernetes.io/docs/setup/learning-environment/minikube/#use-local-images-by-re-using-the-docker-daemon&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Use local images by re-using the Docker daemon&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;eval $(minikube docker-env)&lt;/p&gt;
&lt;h3 id=&#34;commands-for-using-minikube&#34;&gt;Commands for using minikube&lt;/h3&gt;
&lt;p&gt;minikube start&lt;br&gt;
minikube delete&lt;br&gt;
minikube status&lt;/p&gt;
&lt;p&gt;minikube start -p test&lt;br&gt;
minikube delete -p test&lt;br&gt;
&lt;br&gt;
minikube ssh&lt;br&gt;
minikube ssh -p test&lt;br&gt;
minikube ip&lt;br&gt;
minikube dashboard&lt;br&gt;
minikube addons list&lt;br&gt;
&lt;br&gt;
kubectl get pods -A&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://blog.pilosus.org/posts/2019/05/18/minikube-cheat-sheet/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Minikube Cheat Sheet: most helpful commands and features I wish I knew from the start&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;minikube-command-auto-completion&#34;&gt;Minikube command auto completion&lt;/h3&gt;
&lt;p&gt;sudo apt install bash-completion&lt;br&gt;

&lt;a href=&#34;https://www.cyberciti.biz/faq/add-bash-auto-completion-in-ubuntu-linux/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to add bash auto completion in Ubuntu Linux&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://stackoverflow.com/questions/57891054/how-can-i-enable-tab-completion-to-minikube&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How can I enable Tab completion to minikube?&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes Notes</title>
      <link>/devops/k8s-notes/</link>
      <pubDate>Tue, 31 Dec 2019 23:59:59 +0000</pubDate>
      <guid>/devops/k8s-notes/</guid>
      <description>&lt;h2 id=&#34;k8s-notes&#34;&gt;K8S NOTES&lt;/h2&gt;
&lt;h3 id=&#34;kubernetes-components&#34;&gt;Kubernetes Components&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Cluster
&lt;ul&gt;
&lt;li&gt;A cluster is a set of machines, called nodes, that run containerized applications managed by Kubernetes.&lt;/li&gt;
&lt;li&gt;A cluster has at least one worker node and at least one master node.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Node
&lt;ul&gt;
&lt;li&gt;Master node(s)&lt;br&gt;
The master node(s) manages the worker nodes and the pods in the cluster. Multiple master nodes are used to provide a cluster with failover and high availability.&lt;/li&gt;
&lt;li&gt;Worker node(s)&lt;br&gt;
The worker node(s) host the pods that are the components of the application.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;master-node-components&#34;&gt;Master Node Components&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;kube-apiserver&lt;/li&gt;
&lt;li&gt;etcd&lt;/li&gt;
&lt;li&gt;kube-scheduler&lt;/li&gt;
&lt;li&gt;kube-controller-manager
&lt;ul&gt;
&lt;li&gt;Node Controller&lt;/li&gt;
&lt;li&gt;Replication Controller&lt;/li&gt;
&lt;li&gt;Endpoints Controller&lt;/li&gt;
&lt;li&gt;Service Account &amp;amp; Token Controllers&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;cloud-controller-manager&lt;br&gt;
The following controllers have cloud provider dependencies:
&lt;ul&gt;
&lt;li&gt;Node Controller&lt;/li&gt;
&lt;li&gt;Route Controller&lt;/li&gt;
&lt;li&gt;Service Controller&lt;/li&gt;
&lt;li&gt;Volume Controller&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;worker-node-components&#34;&gt;Worker Node Components&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Kubelet&lt;/li&gt;
&lt;li&gt;kube-proxy&lt;/li&gt;
&lt;li&gt;Container Runtime&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;addons&#34;&gt;Addons&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;DNS&lt;/li&gt;
&lt;li&gt;Web UI (Dashboard)&lt;/li&gt;
&lt;li&gt;Container Resource Monitoring&lt;/li&gt;
&lt;li&gt;Cluster-level Logging&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;k8s-abbreviation&#34;&gt;K8S Abbreviation&lt;/h3&gt;
&lt;h4 id=&#34;cni&#34;&gt;CNI&lt;/h4&gt;
&lt;p&gt;Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster.&lt;/p&gt;
&lt;h4 id=&#34;cri&#34;&gt;CRI&lt;/h4&gt;
&lt;p&gt;Container Runtime Interface (CRI)&lt;br&gt;
Each container runtime has it own strengths, and many users have asked for Kubernetes to support more runtimes. CRI consists of a protocol buffers and gRPC API, and libraries, with additional specifications and tools under active development.&lt;br&gt;
Kubelet communicates with the container runtime (or a CRI shim for the runtime) over Unix sockets using the gRPC framework, where kubelet acts as a client and the CRI shim as the server.&lt;br&gt;
&lt;img src=&#34;./cri.png&#34; alt=&#34;cri.png&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Docker&lt;/li&gt;
&lt;li&gt;CRI-O&lt;/li&gt;
&lt;li&gt;Containerd&lt;/li&gt;
&lt;li&gt;Other CRI runtimes: frakti&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;csi&#34;&gt;CSI&lt;/h4&gt;
&lt;p&gt;The goal of CSI is to establish a standardized mechanism for Container Orchestration Systems (COs) to expose arbitrary storage systems to their containerized workloads.
Assuming a CSI storage plugin is already deployed on a Kubernetes cluster, users can use CSI volumes through the familiar Kubernetes storage API objects:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PersistentVolumeClaims&lt;/li&gt;
&lt;li&gt;PersistentVolumes&lt;/li&gt;
&lt;li&gt;StorageClasses&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;crd&#34;&gt;CRD&lt;/h4&gt;
&lt;p&gt;CustomResourceDefinition&lt;/p&gt;
&lt;h3 id=&#34;k8s-networking&#34;&gt;K8S Networking&lt;/h3&gt;
&lt;h4 id=&#34;pod-communication&#34;&gt;POD Communication&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Inner POD&lt;br&gt;
Multi-containers in one POD&lt;br&gt;
Containers within a pod share an IP address and port space, and can find each other via localhost. They can also communicate with each other using standard inter-process communications.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Shared volume&lt;/li&gt;
&lt;li&gt;IPC, i.e. queue&lt;/li&gt;
&lt;li&gt;Networking, localhost with different port&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Inter PODs&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;services&#34;&gt;Services&lt;/h3&gt;
&lt;h4 id=&#34;clusterip&#34;&gt;ClusterIP&lt;/h4&gt;
&lt;p&gt;ClusterIP accesses the services through proxy.&lt;br&gt;
ClusterIP can access services only inside the cluster.&lt;br&gt;
&lt;img src=&#34;./clusterip.png&#34; alt=&#34;clusterip&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;nodeport&#34;&gt;Nodeport&lt;/h4&gt;
&lt;p&gt;NodePort opens a specific port on each node of the cluster and traffic on that node is forwarded directly to the service.&lt;br&gt;
&lt;img src=&#34;./nodeport.png&#34; alt=&#34;nodeport&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;loadbalancer&#34;&gt;Loadbalancer&lt;/h4&gt;
&lt;p&gt;All the traffic on the port is forwarded to the service, there&amp;rsquo;s no filtering, no routing.&lt;br&gt;
&lt;img src=&#34;./loadbalancer.png&#34; alt=&#34;loadbalancer&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;ingress-controller&#34;&gt;Ingress Controller&lt;/h3&gt;
&lt;p&gt;Ingress Controller but there are third party solutions like &lt;strong&gt;Traefik&lt;/strong&gt; and &lt;strong&gt;Nginx&lt;/strong&gt; available. Ingress controller also provide L7 load balancing unlike cluster services.&lt;/p&gt;
&lt;h3 id=&#34;network-policies&#34;&gt;Network policies&lt;/h3&gt;
&lt;p&gt;Isolation policies are configured on a per-namespace basis.&lt;br&gt;

&lt;a href=&#34;https://cloudnativelabs.github.io/post/2017-04-18-kubernetes-networking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Networking&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;k8s-objects&#34;&gt;K8S Objects&lt;/h3&gt;
&lt;p&gt;To find all the objects in some specific API version:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;kubectl api-resources | cut -c92-&lt;/li&gt;
&lt;li&gt;kubectl api-resources | cut -c92-150&lt;/li&gt;
&lt;li&gt;kubectl api-resources | cut -c92-150 | wc -l&lt;/li&gt;
&lt;li&gt;kubectl api-resources&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;pod&#34;&gt;Pod&lt;/h4&gt;
&lt;p&gt;A thin wrapper around one or more containers&lt;/p&gt;
&lt;h4 id=&#34;daemonset&#34;&gt;DaemonSet&lt;/h4&gt;
&lt;p&gt;Implements a single instance of a pod on a worker node&lt;/p&gt;
&lt;h4 id=&#34;deployment&#34;&gt;Deployment&lt;/h4&gt;
&lt;p&gt;Details how to roll out (or roll back) across versions of your application&lt;/p&gt;
&lt;h4 id=&#34;replicaset&#34;&gt;ReplicaSet&lt;/h4&gt;
&lt;p&gt;Ensures a defined number of pods are always running&lt;/p&gt;
&lt;h4 id=&#34;job&#34;&gt;Job&lt;/h4&gt;
&lt;p&gt;Ensures a pod properly runs to completion&lt;/p&gt;
&lt;h4 id=&#34;service&#34;&gt;Service&lt;/h4&gt;
&lt;p&gt;Maps a fixed IP address to a logical group of pods&lt;/p&gt;
&lt;h4 id=&#34;label&#34;&gt;Label&lt;/h4&gt;
&lt;p&gt;Key/Value pairs used for association and filtering&lt;br&gt;
Labels in Kubernetes are intended to be used to specify identifying attributes for objects. They are used by selector queries or with label selectors. Since they are used internally by Kubernetes, the structure of keys and values is constrained, to optimize queries.&lt;/p&gt;
&lt;h4 id=&#34;annotations&#34;&gt;Annotations&lt;/h4&gt;
&lt;p&gt;annotations are a way to attach non-identifying metadata to objects. This metadata is not used internally by Kubernetes, so they cannot be used to identify within k8s. Instead, they are used by external tools and libraries. Examples of annotations include build/release timestamps, client library information for debugging, or fields managed by a network policy like Calico in this case.&lt;/p&gt;
&lt;h4 id=&#34;label-vs-annotation&#34;&gt;Label vs. Annotation&lt;/h4&gt;
&lt;p&gt;You can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects.&lt;/p&gt;
&lt;h4 id=&#34;taints-and-tolerations&#34;&gt;Taints and Tolerations&lt;/h4&gt;
&lt;p&gt;Node affinity, described here, is a property of pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite – they allow a node to repel a set of pods.&lt;/p&gt;
&lt;p&gt;Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints. Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.&lt;/p&gt;
&lt;h4 id=&#34;node-isolationrestriction&#34;&gt;Node isolation/restriction&lt;/h4&gt;
&lt;p&gt;Adding labels to Node objects allows targeting pods to specific nodes or groups of nodes. This can be used to ensure specific pods only run on nodes with certain isolation, security, or regulatory properties.&lt;/p&gt;
&lt;h4 id=&#34;nodeselector&#34;&gt;NODESELECTOR&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Attach a label to the node&lt;/li&gt;
&lt;li&gt;Add a nodeSelector field to your pod configuration&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;affinity-and-anti-affinity&#34;&gt;Affinity and anti-affinity&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;• Node affinity
• Inter-pod affinity and anti-affinity
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;nodeSelector provides a very simple way to constrain pods to nodes with particular labels. The affinity/anti-affinity feature, currently in beta, greatly extends the types of constraints you can express. The key enhancements are:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;• The language is more expressive (not just “AND of exact match”)
• You can indicate that the rule is “soft”/“preference” rather than a hard requirement, so if the scheduler can’t satisfy it, the pod will still be scheduled
• You can constrain against labels on other pods running on the node (or other topological domain), rather than against labels on the node itself, which allows rules about which pods can and cannot be co-located
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The affinity feature consists of two types of affinity, “node affinity” and “inter-pod affinity/anti-affinity”. Node affinity is like the existing nodeSelector (but with the first two benefits listed above), while inter-pod affinity/anti-affinity constrains against pod labels rather than node labels, as described in the third item listed above, in addition to having the first and second properties listed above.&lt;/p&gt;
&lt;p&gt;Node affinity is conceptually similar to nodeSelector – it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node.&lt;/p&gt;
&lt;h3 id=&#34;deploy&#34;&gt;Deploy&lt;/h3&gt;
&lt;p&gt;When you wish to deploy an application in Kubernetes, you usually define three components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a &lt;strong&gt;Deployment&lt;/strong&gt; — which is a recipe for creating copies of your application called Pods&lt;/li&gt;
&lt;li&gt;a &lt;strong&gt;Service&lt;/strong&gt; — an internal load balancer that routes the traffic to Pods&lt;/li&gt;
&lt;li&gt;an &lt;strong&gt;Ingress&lt;/strong&gt; — a description of how the traffic should flow from outside the cluster to your Service&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ingress&#34;&gt;Ingress&lt;/h3&gt;
&lt;h4 id=&#34;what-is-an-ingress&#34;&gt;What is an ingress?&lt;/h4&gt;
&lt;p&gt;In Kubernetes, an Ingress is an object that allows access to your Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services.&lt;br&gt;
&lt;br&gt;
Ingress, added in Kubernetes v1.1, exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.&lt;br&gt;
&lt;br&gt;
Internet&amp;mdash;[ Ingress ]&amp;ndash;|&amp;ndash;|&amp;ndash;[ Services ]
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;An Ingress can be configured to give services externally-reachable URLs, load balance traffic, terminate SSL, and offer name based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a loadbalancer, though it may also configure your edge router or additional frontends to help handle the traffic.&lt;/p&gt;
&lt;p&gt;An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer.&lt;/p&gt;
&lt;h4 id=&#34;what-is-an-ingress-controller&#34;&gt;What is an ingress controller?&lt;/h4&gt;
&lt;p&gt;Kubernetes supports a high level abstraction called Ingress, which allows simple host or URL based HTTP routing. An ingress is a core concept (in beta) of Kubernetes, but is always implemented by a third party proxy. These implementations are known as ingress controllers.&lt;br&gt;
&lt;br&gt;
In order for the Ingress resource to work, the cluster must have an ingress controller running.&lt;br&gt;
Unlike other types of controllers which run as part of the kube-controller-manager binary, Ingress controllers are not started automatically with a cluster. Let’s see some options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ALB Ingress Controller&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;ingress-vs-ingress-controller&#34;&gt;Ingress vs. Ingress controller&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ingress should be the rules for the traffic, which indicate the destination of a request will go through in the cluster.&lt;/li&gt;
&lt;li&gt;Ingress Controller is the implementation for the Ingress. GCE and Nginx are both supported by k8s. They will take care of L4 or L7 proxy.&lt;/li&gt;
&lt;li&gt;Just like other objects in K8s, ingress is also a type of object, which is mainly referred as set of redirection rules.&lt;/li&gt;
&lt;li&gt;Where as ingress controller is like other deployment objects(could be deamon set as well) which listen and configure those ingress rules.&lt;/li&gt;
&lt;li&gt;If I talk in terms of Nginx, Ingress controller is Nginx software itself where as ingress(ingress rules) are nginx configurations.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;the-ingress-resource&#34;&gt;The Ingress Resource&lt;/h4&gt;
&lt;p&gt;A minimal ingress resource example:&lt;br&gt;
&lt;img src=&#34;./ingress-resource-examle.png&#34; alt=&#34;ingress-resource-examle&#34;&gt;&lt;/p&gt;
&lt;p&gt;As with all other Kubernetes resources, an Ingress needs apiVersion, kind, and metadata fields. Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which is the rewrite-target annotation. Different Ingress controller support different annotations. Review the documentation for your choice of Ingress controller to learn which annotations are supported.&lt;br&gt;
The Ingress spec has all the information needed to configure a loadbalancer or proxy server. Most importantly, it contains a list of rules matched against all incoming requests. Ingress resource only supports rules for directing HTTP traffic.&lt;/p&gt;
&lt;h4 id=&#34;ingress-rules&#34;&gt;Ingress Rules&lt;/h4&gt;
&lt;p&gt;Each http rule contains the following information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An optional host. In this example, no host is specified, so the rule applies to all inbound HTTP traffic through the IP address specified. If a host is provided (for example, foo.bar.com), the rules apply to that host.&lt;/li&gt;
&lt;li&gt;A list of paths (for example, /testpath), each of which has an associated backend defined with a serviceName and servicePort. Both the host and path must match the content of an incoming request before the loadbalancer will direct traffic to the referenced service.&lt;/li&gt;
&lt;li&gt;A backend is a combination of service and port names as described in the services doc. HTTP (and HTTPS) requests to the Ingress matching the host and path of the rule will be sent to the listed backend.&lt;/li&gt;
&lt;li&gt;A default backend is often configured in an Ingress controller that will service any requests that do not match a path in the spec.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;default-backend&#34;&gt;Default Backend&lt;/h4&gt;
&lt;p&gt;An Ingress with no rules sends all traffic to a single default backend. The default backend is typically a configuration option of the Ingress controller and is not specified in your Ingress resources.&lt;/p&gt;
&lt;p&gt;If none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend.&lt;/p&gt;
&lt;h3 id=&#34;configmap&#34;&gt;ConfigMap&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://matthewpalmer.net/kubernetes-app-developer/articles/ultimate-configmap-guide-kubernetes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ultimate Guide to ConfigMaps in Kubernetes&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;secrets&#34;&gt;Secrets&lt;/h3&gt;
&lt;p&gt;Secret data should be stored and handled in a way that can be easily hidden and possibly encrypted at rest if the environment is configured as such. The Secret data is represented as base64-encoded information, and it is critical to understand that this is not encrypted. As soon as the secret is injected into the pod, the pod itself can see the secret data in plain text.&lt;/p&gt;
&lt;p&gt;Secret data is meant to be small amounts of data, limited by default in Kubernetes to 1 MB in size, for the base64-encoded data, so ensure that the actual data is approximately 750 KB because of the overhead of the encoding.&lt;/p&gt;
&lt;p&gt;There are three types of secrets in Kubernetes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;generic&lt;br&gt;
regular key/value pairs that are created from a file, a directory, or from string literals using the &amp;ndash;from-literal= parameter.&lt;/li&gt;
&lt;li&gt;docker-registry&lt;br&gt;
This is used by the kubelet when passed in a pod template if there is an imagePullsecret to provide the credentials needed to authenticate to a private Docker registry:&lt;/li&gt;
&lt;li&gt;tls&lt;br&gt;
This creates a Transport Layer Security (TLS) secret from a valid public/private key pair. As long as the cert is in a valid PEM format, the key pair will be encoded as a secret and can be passed to the pod to use for SSL/TLS needs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;configmaps-vs-secrets&#34;&gt;ConfigMaps vs Secrets&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;the ConfigMap API is meant more for string data that is not really sensitive data. If your application requires more sensitive data, the Secrets API is more appropriate.&lt;/li&gt;
&lt;li&gt;ConfigMap data can be injected as either a volume mounted into the pod or as environment variables.&lt;/li&gt;
&lt;li&gt;Secrets are also mounted into tmpfs only on the nodes that have a pod that requires the secret and are deleted when the pod that needs it is gone. This prevents any secrets from being left behind on the disk of the node.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;roles&#34;&gt;Roles&lt;/h3&gt;
&lt;p&gt;Kubernetes has two types of roles, role and clusterRole, the difference being that role is specific to a namespace, and clusterRole is a cluster-wide role across all namespaces.&lt;/p&gt;
&lt;h3 id=&#34;rolebindings&#34;&gt;RoleBindings&lt;/h3&gt;
&lt;p&gt;The RoleBinding allows a mapping of a subject like a user or group to a specific role. Bindings also have two modes: roleBinding, which is specific to a namespace, and clusterRoleBinding, which is across the entire cluster.&lt;/p&gt;
&lt;h3 id=&#34;storage&#34;&gt;Storage&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;PersistentVolume – the low level representation of a storage volume&lt;/li&gt;
&lt;li&gt;Volume Driver – the code used to communicate with the backend storage provider&lt;/li&gt;
&lt;li&gt;Pod – a running container that will consume a PersistentVolume&lt;/li&gt;
&lt;li&gt;PersistentVolumeClaim – the binding between a Pod and PersistentVolume&lt;/li&gt;
&lt;li&gt;StorageClass – allows for dynamic provisioning of PersistentVolumes&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;controllers-and-kubelets&#34;&gt;Controllers and Kubelets&lt;/h4&gt;
&lt;p&gt;Kubernetes has a number of controllers that run on the masters, monitor the state of the cluster and initiate actions in response to events.&lt;br&gt;
It also runs a kubelet process on all of the worker nodes. The kubelet stays in constant contact with the controllers, submitting metrics about current running pods and listening for new instructions.&lt;/p&gt;
&lt;h5 id=&#34;kubelet&#34;&gt;Kubelet:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Mount and format new PersistentVolumes that are scheduled to this host&lt;/li&gt;
&lt;li&gt;Start containers with PersistentVolume hostpath mounted inside the container&lt;/li&gt;
&lt;li&gt;Stop containers and unmount the associated PersistentVolume&lt;/li&gt;
&lt;li&gt;Constantly send metrics to the controllers about container &amp;amp; PersistentVolume state&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;controller&#34;&gt;Controller:&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;to match a PersistentVolumeClaim to a PersistentVolume&lt;/li&gt;
&lt;li&gt;to dynamically provision a new PersistentVolume if a claim cannot be met (if enabled)
&lt;ul&gt;
&lt;li&gt;in the case of EBS this is done via the AWS api from the masters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;to attach the backend storage to a specific node if needed
&lt;ul&gt;
&lt;li&gt;in the case of EBS this is done via the AWS api from the masters&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;to instruct the kubelet for a node to mount (and potentially format) the volume
&lt;ul&gt;
&lt;li&gt;this is done on the actual node&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;to instruct the kubelet to start a container that uses the volume&lt;br&gt;
The kubelet itself performs the low-level mount and mkfs commands when instructed by the controller.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kubernetes-operator&#34;&gt;Kubernetes Operator&lt;/h3&gt;
&lt;p&gt;An Operator is an application-specific controller that extends the Kubernetes API to create, configure, and manage instances of complex stateful applications on behalf of a Kubernetes user. It builds upon the basic Kubernetes resource and controller concepts but includes domain or application-specific knowledge to automate common tasks.&lt;br&gt;
Examples are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;elasticsearch-operator&lt;/li&gt;
&lt;li&gt;prometheus-operator&lt;/li&gt;
&lt;li&gt;cassandra-operator&lt;/li&gt;
&lt;li&gt;Flux  The GitOps Kubernetes Operator&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    TBD
  &lt;/div&gt;
&lt;/div&gt;
&lt;h3 id=&#34;troubleshooting&#34;&gt;Troubleshooting&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://learnk8s.io/troubleshooting-deployments&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A visual guide on troubleshooting Kubernetes deployments&lt;/a&gt; | 
&lt;a href=&#34;./A-visual-guide-on-troubleshooting-Kubernetes.pdf&#34;&gt;PDF&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://cloudnativelabs.github.io/post/2017-04-18-kubernetes-networking/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes Networking&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://matthewpalmer.net/kubernetes-app-developer/articles/ultimate-configmap-guide-kubernetes.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ultimate Guide to ConfigMaps in Kubernetes&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://learnk8s.io/troubleshooting-deployments&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A visual guide on troubleshooting Kubernetes deployments&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://portworx.com/basic-guide-kubernetes-storage/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Basic Guide to Kubernetes Storage: PVS, PVCs, Statefulsets and More&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
&lt;h4 id=&#34;did-you-find-this-page-helpful-consider-sharing-it-&#34;&gt;Did you find this page helpful? Consider sharing it 🙌&lt;/h4&gt;
</description>
    </item>
    
  </channel>
</rss>
