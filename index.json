[{"authors":["admin"],"categories":null,"content":"Hey, My name is Hongwei Li, a software engineer and network engineer. This site is a hub of my documents for learning, experience. I hope you find something awesome, useful and helpful. Feel free to follow me on GitHub, send me a message on Twitter, or drop me an email.\n","date":1604188799,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":253402214400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/hongwei-li/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hongwei-li/","section":"authors","summary":"Hey, My name is Hongwei Li, a software engineer and network engineer. This site is a hub of my documents for learning, experience. I hope you find something awesome, useful and helpful.","tags":null,"title":"Hongwei Li","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Hongwei Li"],"categories":["Java"],"content":"Checked vs Unchecked Exceptions in Java Exception in Java What is an Exception?  Definition: An exception is an event, which occurs during the execution of a program, that disrupts the normal flow of the program\u0026rsquo;s instructions.\nThe general meaning of an exception is the breaching of predefined assumption of the application.\n In Java, all errors and exceptions are represented with Throwable class. When an error occurs within a method, the method creates an object (of any subtype of Throwable) and hands it off to the runtime system. The object, called an exception object, contains information about the error, including its type and the state of the program when the error occurred. Creating an exception object and handing it to the runtime system is called throwing an exception.\nCall Stack After a method throws an exception, the runtime system attempts to find something to handle it. The set of possible \u0026ldquo;somethings\u0026rdquo; to handle the exception is the ordered list of methods that had been called to get to the method where the error occurred. The list of methods is known as the call stack (see the next figure).\nException Handler The runtime system searches the call stack for a method that contains a block of code that can handle the exception. This block of code is called an exception handler. The search begins with the method in which the error occurred and proceeds through the call stack in the reverse order in which the methods were called. When an appropriate handler is found, the runtime system passes the exception to the handler. An exception handler is considered appropriate if the type of the exception object thrown matches the type that can be handled by the handler.\nThe exception handler chosen is said to catch the exception. If the runtime system exhaustively searches all the methods on the call stack without finding an appropriate exception handler, as shown in the next figure, the runtime system (and, consequently, the program) terminates.\nException Handling When an exception object is created in application, there are two choices:\n Either handle it within method Or pass it to the caller method to handle.  In reality, there are 3 basic options to react when a exception occurs at runtime:\n Ignore the exception and let it move up the call stack to another catch block for the exception. Catch the exception and perform required action for your application. If you can not recover from the exception, rethrow it. Catch the exception and wrap it with another exception which is more relevant for your application. Exception wrapping is used to avoid breaking the layer abstraction caught in the exception. You can assign the original exception to the InnerException property of the Exception object you throw. This enables the original exception to be wrapped inside a new exception (which is more relevant for your application). To understand the wrapping of an exception lets consider a method that caught an exception called IOException. You can wrap the original exception (IOException) with a application level exception called LoadingException or FailtoLoadInfoException, as examples, rather than alerting the lower-level IOException to the user.   If an exception is not handled in the application, then it will be propagated to JVM and JVM will usually terminate the program itself.   Exception Hierarchy in Java Note: The terms \u0026ldquo;exception\u0026rdquo; and \u0026ldquo;error\u0026rdquo; are not the same.\nWhether should a general error in application be handled as Error or Exception in Java, depends on the context in the application.\nIn Java, exceptions are broadly categorized into two sections:\n  Checked exceptions\nChecked Exceptions are subclasses of java.lang.Exception, i.e. IOException, SQLException, ClassNotFoundException, etc.\n  Unchecked exceptions.\nUnchecked Exceptions are subclasses of java.lang.RuntimeException, i.e. NullPointerException, NumberFormatException, etc.\n   Errors are serious runtime environment problems that are almost certainly not recoverable, i.e. OutOfMemoryError, LinkageError, and StackOverflowError. They generally crash you program or part of program. Only a good logging practice will help you in determining the exact causes of errors.   Checked Exceptions The exceptions that are checked at compile time (A.K.A.compile-time exceptions). If some code within a method throws a checked exception, then the method must either handle the exception by using try-catch block or it must specify the exception using throws keyword.\nGenerally, checked exceptions denote error scenarios which are outside the immediate control of the program. They occur usually interacting with outside resources/network resources e.g. database problems, network connection errors, missing files etc.\nUnchecked Exceptions The exceptions that are not checked at compiled time.\nThese are exceptions that do not need to be declared in a throws clause. JVM simply doesn’t force you to handle them as they are mostly generated at runtime due to programmatic errors.\nChecked vs Unchecked Exceptions Class definition  Checked exceptions are subclasses of Exception class. All other Exceptions and Throwable are unchecked exceptions. RuntimeException, Error, and their subclasses are unchecked Exceptions. (From Unchecked Exceptions — The Controversy) As the javadoc for Throwable says: \u0026ldquo;Throwable and any subclass of Throwable that is not also a subclass of either RuntimeException or Error are regarded as checked exceptions\u0026rdquo;.  Compiler check  Checked exceptions are forced by compiler and used to indicate exceptional conditions that are out of the control of the program (for example, I/O errors). Unchecked exceptions are occurred during runtime and used to indicate programming errors (for example, a null pointer). Compiler checks each method call and deceleration to determine whether the method throws checked exception.  Contract There\u0026rsquo;s also an important bit of information in the Java Language Specification:\nThe checked exception classes named in the throws clause are part of the contract between the implementor and user of the method or constructor.\nExperience: When and How The below statements are some experiences of developers. These can provide a feel on the use of checked and unchecked exceptions, though they are not so accurate or absolutely correct. In reality, this all depends on the app and specific context. Just feel it\u0026hellip;\n  Checked exceptions ideally should never be used for programming errors, but absolutely should be used for resource errors and for flow control in such cases. Throw only those exceptions which a method can not handle by any mean. Method should first try to handle it as soon as it encounters. Throw the exception only if it is not possible to handle inside method.\n  Checked exceptions can be used when a method cannot do what its name says it does. e.g. A method named prepareSystem() which pre-populate configuration files and do some configuration using them, can declare throwing FileNotFoundException which implies that method uses configuration files from file system.\n  Rule is if a client can reasonably be expected to recover from an exception, make it a checked exception. If a client cannot do anything to recover from the exception, make it an unchecked exception.\n  The only time it is normally legal to shut down an application is during startup. For example, if a configuration file is missing and the application cannot do anything sensible without it, then it is legal to shut down the application.\n  Myself, I prefer using checked exceptions as I can.\n  If you are an API Developer (back-end developer), use checked exceptions, otherwise, use Runtime exceptions.\n  My rules are:\n if statements for business logic errors (like your code) cheched exceptions for environment errors where the application can recover uncheched exception for environment errors where there is no recovery    Line is not always clear, but for me usually RuntimeException = programming errors, checked exceptions = external errors. This is very rough categorization though. Like others say, checked exceptions force you to handle, or at least think for a very tiny fraction of time, about it.\n  In general I use the following rules:\n In my components \u0026amp; libraries I only catch an exception if I intend to handle it or do something based on it. Or if I want to provide additional contextual information in an exception. I use a general try catch at the application entry point, or the highest level possible. If an exception gets here I just log it and let it fail. Ideally exceptions should never get here.     In Effective Java by Joshua Bloch:\n Use checked expections for recoverable conditions and runtime exceptions for programming errors (Item 58 in 2nd edition). Avoid unnecessary use of checked exceptions (Item 59) Use exceptions only for exceptional conditions (Item 57). As others have pointed out, this case may not warrant an exception at all. Consider returning false (or perhaps a status object with details about what happened) if there is not enough credit.    The Controversy  Checked exceptions are a controversial issue in general, and in Java in particular.\n   Some say checked exceptions should be eliminated completely, given the verbosity, tediousness and error-proneness they introduce.\n  Some believe the decision to have these two types of exceptions has its plus(es). It forces the developer to think that an exception is likely to happen in this situation, so he must take measures. The API declares that it will throw exceptions and the developer sees this compile-time. It enforces compile-time safety. You should not wait until the code goes to production to discover that something might fail. Javadoc? Well, a good option to say that, but I bet no one will read the javadoc until the exception actually happens.\n  Fundamentally the only difference between checked exceptions and unchecked exceptions is that the compiler forces you to document checked exceptions. You should be documenting any exceptions your code can throw anyway; I can\u0026rsquo;t see why anyone would be against this being statically checkable to prevent it from being stale or from any exceptions being missed out. Exceptions are part of your API whether checked or not; checked just ensures you declare your API accurately.\n  Like every other trade, there is a trade-off. The advantage of having the compiler do the checks means that there is no way to accidentally leak exceptions (and humans do make mistakes). If the calling code does not handle the exception, it has to leak it explicitly. This is exactly the same trade-off we get from loosely-typed vs strongly-typed languages.\n    Unchecked Exceptions — The Controversy\nBecause the Java programming language does not require methods to catch or to specify unchecked exceptions (RuntimeException, Error, and their subclasses), programmers may be tempted to write code that throws only unchecked exceptions or to make all their exception subclasses inherit from RuntimeException. Both of these shortcuts allow programmers to write code without bothering with compiler errors and without bothering to specify or to catch any exceptions. Although this may seem convenient to the programmer, it sidesteps the intent of the catch or specify requirement and can cause problems for others using your classes.\nWhy did the designers decide to force a method to specify all uncaught checked exceptions that can be thrown within its scope? Any Exception that can be thrown by a method is part of the method\u0026rsquo;s public programming interface. Those who call a method must know about the exceptions that a method can throw so that they can decide what to do about them. These exceptions are as much a part of that method\u0026rsquo;s programming interface as its parameters and return value.\nThe next question might be: \u0026ldquo;If it\u0026rsquo;s so good to document a method\u0026rsquo;s API, including the exceptions it can throw, why not specify runtime exceptions too?\u0026rdquo; Runtime exceptions represent problems that are the result of a programming problem, and as such, the API client code cannot reasonably be expected to recover from them or to handle them in any way. Such problems include arithmetic exceptions, such as dividing by zero; pointer exceptions, such as trying to access an object through a null reference; and indexing exceptions, such as attempting to access an array element through an index that is too large or too small.\nRuntime exceptions can occur anywhere in a program, and in a typical one they can be very numerous. Having to add runtime exceptions in every method declaration would reduce a program\u0026rsquo;s clarity. Thus, the compiler does not require that you catch or specify runtime exceptions (although you can).\nOne case where it is common practice to throw a RuntimeException is when the user calls a method incorrectly. For example, a method can check if one of its arguments is incorrectly null. If an argument is null, the method might throw a NullPointerException, which is an unchecked exception.\nGenerally speaking, do not throw a RuntimeException or create a subclass of RuntimeException simply because you don\u0026rsquo;t want to be bothered with specifying the exceptions your methods can throw.\nHere\u0026rsquo;s the bottom line guideline: If a client can reasonably be expected to recover from an exception, make it a checked exception. If a client cannot do anything to recover from the exception, make it an unchecked exception.\n Best Practices   If a client can reasonably be expected to recover from an exception, make it a checked exception. If a client cannot do anything to recover from the exception, make it an unchecked exception.     Never swallow the exception in catch block Declare the specific checked exceptions that your method can throw Do not catch the Exception class, always catch a specific exception class Never catch Throwable class Never (ever) catch Errors\nErrors are problems you will most likely never be able to handle (e.g. OutOfMemory, or other JVM issues) Always correctly wrap the exceptions in custom exceptions so that stack trace is not lost Either log the exception or throw it but never do the both Never throw any exception from finally block Always catch only those exceptions that you can actually handle Catch exceptions and handle them, don\u0026rsquo;t just throw them up the stack Don\u0026rsquo;t use printStackTrace() statement or similar\nChances are one of your users will get one of those stack traces eventually, and have exactly zero knowledge as to what to do with it. Pass all relevant information to exceptions to make them informative as much as possible\nAt least rethrow an exception that explains what was happening at the time and has the caught exception inside of it. Always log exceptions for the sake of maintainability\nWhen starting seeing bugs, the log will assist in pointing you to the place your bug has likely started. Always include all information about an exception in single log message Use finally blocks instead of catch blocks if you are not going to handle exception Remember \u0026ldquo;Throw early catch late\u0026rdquo; principle Always clean up after handling the exception Throw only relevant exception from a method Never use exceptions for flow control in your program Validate user input to catch adverse conditions very early in request processing Always terminate the thread which it is interrupted Use template methods for repeated try-catch Document all exceptions in your application in javadoc Do something useful with the errors (logs, restarts, recoveries, etc) Use one big handler at the higher-level to manage any or all of the weird conditions arising in the code that aren\u0026rsquo;t caught at a low-level Think about validation check vs exception\nThrowing an exception is a expensive process.  NullPointerException\nCatching a NullPointerException should replaced with a graceful null-check. NumberFormatException\nCatching a NumberFormatException explicitly to avoid possible introduction of error prone code to handle different number formats.   Avoid exception handling inside loops\nIf it is truly necessary implement a try/catch block arround the entire loop. Adopt a standard way of handling exceptions through try/catch/finally blocks\nThis is the recommended approach to handle exceptions in managed code. Finally blocks ensure that resources are closed in the event of an exception.   Be respectful of the language and how it traditionally manages such issues. For example, don\u0026rsquo;t bring a C mindset into a Java world.   FAQ   Is RuntimeException an unchecked exception?\nYes, exactly.\n  Why do people add class Exception in the throws clause?\nMost often because people are lazy to consider what to catch and what to rethrow. Throwing Exception is a bad practice and should be avoided.\nBut it\u0026rsquo;s common when implementing frameworks, let users of the framework be able to throw any exception, e.g. the signature of the Callable interface in JSE.\n  Should I bubble up the exact exception or mask it using Exception?\nYes and No.\nYes for bubble up the exact exception and always declare the most precise exception possible, or a list of such.\nNo for mask any exception.\nThe exceptions you declare your method as capable of throwing are a part of the contract between your method and the caller. Throwing \u0026ldquo;FileNotFoundException\u0026rdquo; means that something wrong with the file, either the file name isn\u0026rsquo;t valid or the file is not there. Throwing Exception causes very difficult to find the root cuase.\n  Why bubble up?\nShould the error be handled sooner better?\nWhat it means is to bubble it up to the top most responsible layer that controls the logic flow and oversees the business logic for the application. It would be impossible, for example, for the database layer to communicate to the client that something critical is missing or unresponsive. When it bubbles up to the top most server layer then it is straight forward to refresh the client\u0026rsquo;s view with a critical error message.\n  How to handle exception?\nBasically it depends on context of the application, such as where this code is and what you want to happen. Here are some examples:\n If it is in the UI layer - catch it and show a warning. If it\u0026rsquo;s in the service layer - don\u0026rsquo;t catch it at all - let it bubble. Just don\u0026rsquo;t swallow the exception. If an exception occurs in most of the cases you should choose one of these:  log it and return rethrow it (declare it to be thrown by the method) construct a new exception by passing the current one in constructor      Why did the designers decide to force a method to specify all uncaught checked exceptions that can be thrown within its scope?\nAny Exception that can be thrown by a method is part of the method\u0026rsquo;s public programming interface. Those who call a method must know about the exceptions that a method can throw so that they can decide what to do about them. These exceptions are as much a part of that method\u0026rsquo;s programming interface as its parameters and return value.\n  If it\u0026rsquo;s so good to document a method\u0026rsquo;s API, including the exceptions it can throw, why not specify runtime exceptions too?\nRuntime exceptions represent problems that are the result of a programming problem, and as such, the API client code cannot reasonably be expected to recover from them or to handle them in any way. Such problems include arithmetic exceptions, such as dividing by zero; pointer exceptions, such as trying to access an object through a null reference; and indexing exceptions, such as attempting to access an array element through an index that is too large or too small.\n  Wrapping Exceptions wrapping exceptions (checked or otherwise) has several benefits that are worth the cost:\n  It encourages you to think about the failure modes for the code you write. Basically, you have to consider the exceptions that the code you call may throw, and in turn you\u0026rsquo;ll consider the exceptions you\u0026rsquo;ll throw for the code that calls yours.\n  It gives you the opportunity to add additional debugging information into the exception chain. For instance, if you have a method that throws an exception on a duplicate username, you might wrap that exception with one that includes additional information about the circumstances of the failure (for example, the IP of the request that provided the dupe username) that wasn\u0026rsquo;t available to the lower-level code. The cookie trail of exceptions may help you debug a complex problem (it certainly has for me).\n  It lets you become implementation-independent from the lower level code. If you\u0026rsquo;re wrapping exceptions and need to swap out Hibernate for some other ORM, you only have to change your Hibernate-handling code. All the other layers of code will still be successfully using the wrapped exceptions and will interpret them in the same way, even though the underlying circumstances have changed. Note that this applies even if Hibernate changes in some way (ex: they switch exceptions in a new version); it\u0026rsquo;s not just for wholesale technology replacement.\n  It encourages you use different classes of exceptions to represent different situations. For example, you may have a DuplicateUsernameException when the user tries to reuse a username, and a DatabaseFailureException when you can\u0026rsquo;t check for dupe usernames due to a broken DB connection. This, in turn, lets you answer your question (\u0026ldquo;how do I recover?\u0026quot;) in flexible and powerful ways. If you get a DuplicateUsernameException, you may decide to suggest a different username to the user. If you get a DatabaseFailureException, you may let it bubble up to the point where it displays a \u0026ldquo;down for maintenance\u0026rdquo; page to the user and send off a notification email to you. Once you have custom exceptions, you have customizeable responses \u0026ndash; and that\u0026rsquo;s a good thing.\n  Exception Management Architecture The exception management architecture of an application should be capable of:\n Detecting exceptions Performing code clean up Wrapping one exception inside another Replacing one exception with another Logging and reporting error information Generating events that can be monitored externally to assist system operation  At the beginning of the design you must plan for a consist and robust exception management architecture. It should be well encapsulated and abstract the details of logging and reporting throughout all of your application.\nOnce you embrace exceptions, you should find that it makes your code simpler and clearer, by moving your error handling code away from your core logic. Aim to have lots of statements in a single try block.\nLinks  Understanding checked vs unchecked exceptions in Java\nCHECKED AND UNCHECKED EXCEPTIONS IN JAVA\nBest practices for exception management in Java or C#\nBest Practices: Exception Management\nUnchecked Exceptions — The Controversy\nLesson: Exceptions\nJava Checked vs Unchecked Exceptions\nFailure and Exceptions\nTop 20 Java Exception Handling Best Practices\nHow do I recover from an unchecked exception?\nWhen to choose checked and unchecked exceptions\nDid you find this page helpful? Consider sharing it 🙌 ","date":1604188799,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"4c1140317fd7e9965b89a3d6184924e4","permalink":"/coding/java/checked-vs-unchecked-exceptions-in-java/","publishdate":"2020-10-31T23:59:59Z","relpermalink":"/coding/java/checked-vs-unchecked-exceptions-in-java/","section":"coding","summary":"A summary of checked and unchecked exceptions in Java.","tags":["Java","Exception"],"title":"Checked vs Unchecked Exceptions in Java","type":"coding"},{"authors":["Hongwei Li"],"categories":["Python","Java","Algorithm"],"content":"Algorithm Description To find a longest palindrome in a string in linear time, an algorithm may take advantage of the following characteristics or observations about a palindrome and a sub-palindrome:\n The left side of a palindrome is a mirror image of its right side. (Case 1) A third palindrome whose center is within the right side of a first palindrome will have exactly the same length as a second palindrome anchored at the mirror center on the left side, if the second palindrome is within the bounds of the first palindrome by at least one character (not meeting the left bound of the first palindrome). Such as \u0026ldquo;dacabacad\u0026rdquo;, the whole string is the first palindrome, \u0026ldquo;aca\u0026rdquo; in the left side as second palindrome, \u0026ldquo;aca\u0026rdquo; in the right side as third palindrome. In this case, the second and third palindrome have exactly the same length. (Case 2) If the second palindrome meets or extends beyond the left bound of the first palindrome, then the distance from the center of the second palindrome to the left bound of the first palindrome is exactly equal to the distance from the center of the third palindrome to the right bound of the first palindrome. To find the length of the third palindrome under Case 2, the next character after the right outermost character of the first palindrome would then be compared with its mirror character about the center of the third palindrome, until there is no match or no more characters to compare. (Case 3) Neither the first nor second palindrome provides information to help determine the palindromic length of a fourth palindrome whose center is outside the right side of the first palindrome. It is therefore desirable to have a palindrome as a reference (i.e., the role of the first palindrome) that possesses characters farthest to the right in a string when determining from left to right the palindromic length of a substring in the string (and consequently, the third palindrome in Case 2 and the fourth palindrome in Case 3 could replace the first palindrome to become the new reference). Regarding the time complexity of palindromic length determination for each character in a string: there is no character comparison for Case 1, while for Cases 2 and 3 only the characters in the string beyond the right outermost character of the reference palindrome are candidates for comparison (and consequently Case 3 always results in a new reference palindrome while Case 2 does so only if the third palindrome is actually longer than its guaranteed minimum length). For even-length palindromes, the center is at the boundary of the two characters in the middle.  TL;DR Manacher\u0026rsquo;s algorithm fills in a table P[i] which contains how far the palindrome centered at i extends. If P[5]=3, then three characters on either side of position five are part of the palindrome. The algorithm takes advantage of the fact that if you\u0026rsquo;ve found a long palindrome, you can fill in values of P on the right side of the palindrome quickly by looking at the values of P on the left side, since they should mostly be the same.\nSimply put:\n makes either odd length or even lenght as odd length only by adding a special character, which is not in the original string, to the both sides of each character in original string. utilize the mirroring charateristic of a palindrome and a sub-palindrome to fill the table p[i].   NOTE\nTo simplify the desciption, define:\n String S is the original string with inserted special characters. C is the center position of panlindrome sub(C) of string S. L is the leftmost position of sub(C). R is the rightmost position of sub(C). P[i] is the array to store the length of panlindrome, which has center position x. i is the position that needs to calculate the length of panlindrom, that is to calculate P[i] of sub(i). j is the mirroring position of i about center position C. P[j] is the calculated/known lenght of panlindrome sub(j).   Odd + Even \u0026mdash;\u0026gt; Odd only aba ---\u0026gt; #a#b#a# abba ---\u0026gt; #a#b#b#a#  Mirroring   i and j are mirroring about C\n  sub(i) and sub(j) are mirroring about C\n C - j = i - C ---\u0026gt; j = 2 * C - i    3 Cases Case 1: i \u0026lt; R and j-P[j] \u0026gt; L i \u0026lt; R and j-P[j] \u0026gt; L means whole sub(j) within left side of sub(C)\nj - P[j] \u0026gt; L ---\u0026gt; R - i \u0026gt; P[j]  Based on mirroring:\nP[i] = P[j]  Case 2: i \u0026lt; R and j-P[j] \u0026lt;= L leftmost side of sub(j) is at or outside of leftmost of sub(C) j - P[j] \u0026lt;= L ---\u0026gt; R - i \u0026lt;= P[j]  From the picture, we can see P[i] is at least R - i\nP[i] \u0026gt;= R - i ---\u0026gt; P[i] = R - i as initial value of P[i] and do calculation for the part outside R and update P[i]  Code for case 1 and case 2:\nFrom programming convenience perspective, here make case 1 and case 2 in 1 situation i \u0026lt; R.\nlength is the intermidiate variable of P[i].\nif i \u0026lt; R: j = 2 * C - i length = min(P[j], R - i) while i + length \u0026lt; len(s) and i - length \u0026gt;= 0: if s[i + length] != s[i - length]: break length += 1  Case 3: i \u0026gt;= R No shortcuts for this situation other than brute force.\nCalculate from length of 1 at the position i as the center.\nlength is the intermidiate variable of P[i].\nCode for case 3:\nlength = 1 while i + length \u0026lt; len(s) and i - length \u0026gt;= 0: if s[i + length] != s[i - length]: break length += 1  Full Implementation def longestPalindrome(self, s: str) -\u0026gt; str: if not s: return # Using manacher's algorithm # abba =\u0026gt; #a#b#b#a# # s: original string # t: transformed string t = [] for c in s: t.append('#') t.append(c) t.append('#') n = len(t) # p: P[i] table p = [0] * n p[0] = 1 mid, longest = 0, 1 for i in range(1, n): length = 1 if mid + longest \u0026gt; i: mirror = mid - (i - mid) length = min(p[mirror], mid + longest - i) while i + length \u0026lt; len(t) and i - length \u0026gt;= 0: if t[i + length] != t[i - length]: break; length += 1 if length \u0026gt; longest: longest = length mid = i p[i] = length # remove the extra # longest = longest - 1 start = (mid - 1) // 2 - (longest - 1) // 2 return s[start:start + longest]  Time Complexition O(N)\n For case 1: no comparison For case 2: take the example in below picture, i\u0026rsquo; is the following position of i.\nEven there is comparison for range X, but for the following any i\u0026rsquo;, no comparison again for range X\u0026rsquo;. So ther is no repeated comparison for the whole process.\n For case 3: similar to case 2.  So the time complexity is O(N).\nSpace Complexition O(N)\nArray P to store the lengths of sub-panlindrome strings.\nLinks  Manacher\u0026rsquo;s algorithm\nManacher\u0026rsquo;s ALGORITHM\nLongest Palindromic Substring\nManacher\u0026rsquo;s Algorithm - Finding all sub-palindromes in O(N)\n","date":1604016000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604016000,"objectID":"951605b72fddbf1bc19a1470eb898c5e","permalink":"/coding/python/manachers-algorithm/","publishdate":"2020-10-30T00:00:00Z","relpermalink":"/coding/python/manachers-algorithm/","section":"coding","summary":"Summary of the Manacher's algorithm.","tags":["Python","Manacher's Algorithm","Algorithm"],"title":"Manacher's Algorithm","type":"coding"},{"authors":["Hongwei Li"],"categories":["System","DevOps","SRE"],"content":"Kubernetes Best Practices Best Practices for Image Management  Build your images you base them on only well-known and trusted image providers. Some combination of the semantic version and the SHA hash of the commit where the image was built is a good practice for naming images (e.g., v1.0.1-bfeda01f). It is a bad idea for production usage because latest is clearly being mutated every time a new image is built.  Deploying Services Best Practices  Most services should be deployed as Deployment resources. Deployments create identical replicas for redundancy and scale. Deployments can be exposed using a Service, which is effectively a load balancer. A Service can be exposed either within a cluster (the default) or externally. If you want to expose an HTTP application, you can use an Ingress controller to add things like request routing and SSL. Eventually you will want to parameterize your application to make its configuration more reusable in different environments. Packaging tools like Helm are the best choice for this kind of parameterization.  Setting Up a Development Environment Best Practices  Think about developer experience in three phases: onboarding, developing, and testing. Make sure that the development environment you build supports all three of these phases. When building a development cluster, you can choose between one large cluster and a cluster per developer. There are pros and cons to each, but generally a single large cluster is a better approach. When you add users to a cluster, add them with their own identity and access to their own namespace. Use resource limits to restrict how much of the cluster they can use. When managing namespaces, think about how you can reap old, unused resources. Developers will have bad hygiene about deleting unused things. Use automation to clean it up for them. Think about cluster-level services like logs and monitoring that you can set up for all users. Sometimes, cluster-level dependencies like databases are also useful to set up on behalf of all users using templates like Helm charts.  Best Practices for Monitoring, Logging, and Alerting Following are the best practices that you should adopt regarding monitoring, logging, and alerting.\nMonitoring  Monitor nodes and all Kubernetes components for utilization, saturation, and error rates, and monitor applications for rate, errors, and duration. Use black-box monitoring to monitor for symptoms and not predictive health of a system. Use white-box monitoring to inspect the system and its internals with instrumentation. Implement time-series-based metrics to gain high-precision metrics that also allow you to gain insight within the behavior of your application. Utilize monitoring systems like Prometheus that provide key labeling for high dimensionality; this will give a better signal to symptoms of an impacting issue. Use average metrics to visualize subtotals and metrics based on factual data. Utilize sum metrics to visualize the distribution across a specific metric.  Logging  You should use logging in combination with metrics monitoring to get the full picture of how your environment is operating. Be cautious of storing logs for more than 30 to 45 days and, if needed, use cheaper resources for long-term archiving. Limit usage of log forwarders in a sidecar pattern, as they will utilize a lot more resources. Opt for using a DaemonSet for the log forwarder and sending logs to STDOUT.  Alerting  Be cautious of alert fatigue because it can lead to bad behaviors in people and processes. Always look at incrementally improving upon alerting and accept that it will not always be perfect. Alert for symptoms that affect your SLO and customers and not for transient issues that don’t need immediate human attention.    It’s a good practice to monitor your cluster from a “utility cluster” to avoid a production issue also affecting your monitoring system. Black-box monitoring gives you symptoms. White-box monitoring gives you \u0026ldquo;Why\u0026rdquo;. The USE and RED methods are complementary to each other given that the USE method focuses on the infrastructure components and the RED method focuses on monitoring the end-user experience for the application. cAdvisor and metrics server are used to provide detailed metrics on resource usage, kube-state-metrics is focused on identifying conditions on Kubernetes objects deployed to your cluster.    Best Practices for ConfigMaps and Secrets  To support dynamic changes to your application without having to redeploy new versions of the pods, mount your ConfigMaps/Secrets as a volume and configure your application with a file watcher to detect the changed file data and reconfigure itself as needed. ConfigMap/Secrets must exist in the namespace for the pods that will consume them prior to the pod being deployed. The optional flag can be used to prevent the pods from not starting if the ConfigMap/Secret is not present. Use an admission controller to ensure specific configuration data or to prevent deployments that do not have specific configuration values set. An example would be if you require all production Java workloads to have certain JVM properties set in production environments. There is an alpha API called PodPresets that will allow ConfigMaps and secrets to be applied to all pods based on an annotation, without needing to write a custom admission controller. If you’re using Helm to release applications into your environment, you can use a life cycle hook to ensure the ConfigMap/Secret template is deployed before the Deployment is applied. Some applications require their configuration to be applied as a single file such as a JSON or YAML file. ConfigMap/Secrets allows an entire block of raw data by using the | symbol. If the application uses system environment variables to determine its configuration, you can use the injection of the ConfigMap data to create an environment variable mapping into the pod. There are two main ways to do this: mounting every key/value pair in the ConfigMap as a series of environment variables into the pod using envFrom and then using configMapRef or secretRef, or assigning individual keys with their respective values using the configMapKeyRef or secretKeyRef. If you’re using the configMapKeyRef or secretKeyRef method, be aware that if the actual key does not exist, this will prevent the pod from starting. If you’re loading all of the key/value pairs from the ConfigMap/Secret into the pod using envFrom, any keys that are considered invalid environment values will be skipped; however, the pod will be allowed to start. The event for the pod will have an event with reason InvalidVariableNames and the appropriate message about which key was skipped. The following code is an example of a Deployment with a ConfigMap and Secret reference as an environment variable If there is a need to pass command-line arguments to your containers, environment variable data can be sourced using $(ENV_KEY) interpolation syntax. When consuming ConfigMap/Secret data as environment variables, it is very important to understand that updates to the data in the ConfigMap/Secret will not update in the pod and will require a pod restart either through deleting the pods and letting the ReplicaSet controller create a new pod, or triggering a Deployment update, which will follow the proper application update strategy as declared in the Deployment specification. It is easier to assume that all changes to a ConfigMap/Secret require an update to the entire deployment; this ensures that even if you’re using environment variables or volumes, the code will take the new configuration data. To make this easier, you can use a CI/CD pipeline to update the name property of the ConfigMap/Secret and also update the reference in the deployment, which will then trigger an update through normal Kubernetes update strategies of your deployment. We will explore this in the following example code. If you’re using Helm to release your application code into Kubernetes, you can take advantage of an annotation in the Deployment template to check the sha256 checksum of the ConfigMap/Secret. This triggers Helm to update the Deployment using the helm upgrade command when the data within a ConfigMap/Secret is changed.  Best Practices Specific to Secrets  Use 3rd party solutions to allow the use of external storage systems for secret data, such as HashiCorp Vault, Aqua Security, Twistlock, AWS Secrets Manager, Google Cloud KMS, or Azure Key Vault Assign an imagePullSecrets to a serviceaccount that the pod will use to automatically mount the secret without having to declare it in the pod.spec. Use CI/CD capabilities to get secrets from a secure vault or encrypted store with a Hardware Security Module (HSM) during the release pipeline.  RBAC Best Practices  Applications that are developed to run in Kubernetes rarely ever need an RBAC role and role binding associated to it. Only if the application code actually interacts directly with the Kubernetes API directly does the application require RBAC configuration. If the application does need to directly access the Kubernetes API to perhaps change configuration depending on endpoints being added to a service, or if it needs to list all of the pods in a specific namespace, the best practice is to create a new service account that is then specified in the pod specification. Then, create a role that has the least amount of privileges needed to accomplish its goal. Use an OpenID Connect service that enables identity management and, if needed, two-factor authentication. This will allow for a higher level of identity authentication. Map user groups to roles that have the least amount of privileges needed to accomplish the job. Along with the aforementioned practice, you should use Just in Time (JIT) access systems to allow site reliability engineers (SREs), operators, and those who might need to have escalated privileges for a short period of time to accomplish a very specific task. Alternatively, these users should have different identities that are more heavily audited for sign-on, and those accounts should have more elevated privileges assigned by the user account or group bound to a role. Specific service accounts should be used for CI/CD tools that deploy into your Kubernetes clusters. This ensures for auditability within the cluster and an understanding of who might have deployed or deleted any objects in a cluster. Limit any applications that require watch and list on the Secrets API. This basically allows the application or the person who deployed the pod to view the secrets in that namespace. If an application needs to access the Secrets API for specific secrets, limit using get on any specific secrets that the application needs to read outside of those that it is directly assigned.   Service accounts in Kubernetes are different than user accounts in that they are namespace bound, internally stored in Kubernetes; they are meant to represent processes, not people, and are managed by native Kubernetes controllers.   Best Practices for CI/CD  With CI, focus on automation and providing quick builds. Optimizing the build speed will provide developers quick feedback if their changes have broken the build. Focus on providing reliable tests in your pipeline. This will give developers rapid feedback on issues with their code. The faster the feedback loop to developers, the more productive they’ll become in their workflow. When deciding on CI/CD tools, ensure that the tools allow you to define the pipeline as code. This will allow you to version-control the pipeline with your application code. Ensure that you optimize your images so that you can reduce the size of the image and also reduce the attack surface when running the image in production. Multistage Docker builds allow you to remove packages not needed for the application to run. For example, you might need Maven to build the application, but you don’t need it for the actual running image. Avoid using “latest” as an image tag, and utilize a tag that can be referenced back to the buildID or Git commit. If you are new to CD, utilize Kubernetes rolling upgrades to start out. They are easy to use and will get you comfortable with deployment. As you become more comfortable and confident with CD, look at utilizing blue/green and canary deployment strategies. With CD, ensure that you test how client connections and database schema upgrades are handled in your application. Testing in production will help you build reliability into your application, and ensure that you have good monitoring in place. With testing in production, also start at a small scale and limit the blast radius of the experiment.   Including both application code and configuration code, such as a Kubernetes manifest or Helm charts, helps promote good DevOps principles of communication and collaboration. Having both application developers and operation engineers collaborate in a single repository builds confidence in a team to deliver an application to production.   Build the smallest image Build the smallest image possible for your application:\n Multistage builds Distroless base images Optimized base images  Container Image Tagging  BuildID Build System-BuildID Git Hash Githsah-buildID  Deployment Strategies  Rolling updates Blue/green deployments Canary deployments  Worldwide Rollout Best Practices  Distribute each image around the world. A successful rollout depends on the release bits (binaries, images, etc.) being nearby to where they will be used. This also ensures reliability of the rollout in the presence of networking slowdowns or irregularities. Geographic distribution should be a part of your automated release pipeline for guaranteed consistency. Shift as much of your testing as possible to the left by having as much extensive integration and replay testing of your application as possible. You want to start a rollout only with a release that you strongly believe to be correct. Begin a release in a canary region, which is a preproduction environment in which other teams or large customers can validate their use of your service before you begin a larger-scale rollout. Identify different characteristics of the regions where you are rolling out. Each difference can be one that causes a failure and a full or partial outage. Try to roll out to low-risk regions first. Document and practice your response to any problem or process (e.g., a rollback) that you might encounter. Trying to remember what to do in the heat of the moment is a recipe for forgetting something and making a bad problem worse.  Resource Management Best Practices  Utilize pod anti-affinity to spread workloads across multiple availability zones to ensure high availability for your application. If you’re using specialized hardware, such as GPU-enabled nodes, ensure that only workloads that need GPUs are scheduled to those nodes by utilizing taints. Use NodeCondition taints to proactively avoid failing or degraded nodes. Apply nodeSelectors to your pod specifications to schedule pods to specialized hardware that you have deployed in the cluster. Before going to production, experiment with different node sizes to find a good mix of cost and performance for node types. If you’re deploying a mix of workloads with different performance characteristics, utilize node pools to have mixed node types in a single cluster. Ensure that you set memory and CPU limits for all pods deployed to your cluster. Utilize ResourceQuotas to ensure that multiple teams or applications are alotted their fair share of resources in the cluster. Implement LimitRange to set default limits and requests for pod specifications that don’t set limits or requests. Start with manual cluster scaling until you understand your workload profiles on Kubernetes. You can use autoscaling, but it comes with additional considerations around node spin-up time and cluster scale down. Use the HPA for workloads that are variable and that have unexpected spikes in their usage.  Services in Kubernetes Service Types:\n ClusterIP NodePort ExternalName LoadBalancer  Services and Ingress Controllers Best Practices  Limit the number of services that need to be accessed from outside the cluster. Ideally, most services will be ClusterIP, and only external-facing services will be exposed externally to the cluster. If the services that need to be exposed are primarily HTTP/HTTPS-based services, it is best to use an Ingress API and Ingress controller to route traffic to backing services with TLS termination. Depending on the type of Ingress controller used, features such as rate limiting, header rewrites, OAuth authentication, observability, and other services can be made available without having to build them into the applications themselves. Choose an Ingress controller that has the needed functionality for secure ingress of your web-based workloads. Standardize on one and use it across the enterprise because many of the specific configuration annotations vary between implementations and prevent the deployment code from being portable across enterprise Kubernetes implementations. Evaluate cloud service provider-specific Ingress controller options to move the infrastructure management and load of the ingress out of the cluster, but still allow for Kubernetes API configuration. When serving mostly APIs externally, evaluate API-specific Ingress controllers, such as Kong or Ambassador, that have more fine-tuning for API-based workloads. Although NGINX, Traefik, and others might offer some API tuning, it will not be as fine-grained as specific API proxy systems. When deploying Ingress controllers as pod-based workloads in Kubernetes, ensure that the deployments are designed for high availability and aggregate performance throughput. Use metrics observability to properly scale the ingress, but include enough cushion to prevent client disruptions while the workload scales.  Network Policy Best Practices  Start off slow and focus on traffic ingress to pods. Complicating matters with ingress and egress rules can make network tracing a nightmare. As soon as traffic is flowing as expected, you can begin to look at egress rules to further control flow to sensitive workloads. The specification also favors ingress because it defaults many options even if nothing is entered into the ingress rules list. Ensure that the network plug-in used either has some of its own interface to the NetworkPolicy API or supports other well-known plug-ins. Example plug-ins include Calico, Cilium, Kube-router, Romana, and Weave Net. If the network team is used to having a “default-deny” policy in place, create a network policy such as the following for each namespace in the cluster that will contain workloads to be protected. This ensures that even if another network policy is deleted, no pods are accidentally “exposed”. If there are pods that need to be accessed from the internet, use a label to explicitly apply a network policy that allows ingress. Be aware of the entire flow in case the actual IP that a packet is coming from is not the internet, but the internal IP of a load balancer, firewall, or other network device. For example, to allow traffic from all (including external) sources for pods having the allow-internet=true label, do this: Try to align application workloads to single namespaces for ease of creating rules because the rules themselves are namespace specific. If cross-namespace communication is needed, try to be as explicit as possible and perhaps use specific labels to identify the flow pattern: Have a test bed namespace that has fewer restrictive policies, if any at all, to allow time to investigate the correct traffic patterns needed.  Service Mesh Best Practices  Rate the importance of the key features service meshes offer and determine which current offerings provide the most important features with the least amount of overhead. Overhead here is both human technical debt and infrastructure resource debt. If all that is really required is mutual TLS between certain pods, would it be easier to perhaps find a CNI that offers that integrated into the plug-in? Is the need for a cross-system mesh such as multicloud or hybrid scenarios a key requirement? Not all service meshes offer this capability, and if they do, it is a complicated process that often introduces fragility into the environment. Many of the service mesh offerings are open source community-based projects, and if the team that will be managing the environment is new to service meshes, commercially supported offerings might be a better option. There are companies that are beginning to offer commercially supported and managed service meshes based on Istio, which can be helpful because it is almost universally agreed upon that Istio is a complicated system to manage.  PodSecurityPolicy Best Practices PodSecurityPolicy is complex and can be error prone.\n  Proceed with caution when enabling PodSecurityPolicy because it’s potentially workload blocking if adequate preparation isn’t done at the outset. If you are enabling PodSecurityPolicy on an existing cluster with running workloads, you must create all necessary policies, service accounts, roles, and role bindings before enabling the admission controller. It’s extremely important to remember that having no PodSecurityPolicies defined will result in an implicit deny. This means that without a policy match for the workload, the pod will not be created. You should not enable PodSecurityPolicy on a live cluster without considering the warnings provided in the previous section. Proceed with caution.     It all comes down to RBAC. Whether you like it or not, PodSecurityPolicy is determined by RBAC. It’s this relationship that actually exposes all of the shortcomings in your current RBAC policy design. We cannot stress just how important it is to automate your RBAC and PodSecurityPolicy creation and maintenance. Specifically locking down access to service accounts is the key to using policy. Understand the policy scope. Determining how your policies will be laid out on your cluster is very important. Your policies can be cluster-wide, namespaced, or workload-specific in scope. There will always be workloads on your cluster that are part of the Kubernetes cluster operations that will need more permissive security privileges, so make sure that you have appropriate RBAC in place to stop unwanted workloads using your permissive policies. Do you want to enable PodSecurityPolicy on an existing cluster? Use this handy open source tool to generate policies based on your current resources. This is a great start. From there, you can hone your policies.  Policy and Governance for Your Cluster Learn about Gatekeeper\nGatekeeper is an open source customizable Kubernetes admission webhook for cluster policy and governance. Gatekeeper takes advantage of the OPA constraint framework to enforce custom resource definition (CRD)-based policies. Using CRDs allows for an integrated Kubernetes experience that decouples policy authoring from implementation. Policy templates are referred to as constraint templates, which can be shared and reused across clusters. Gatekeeper enables resource validation and audit functionality. One of the great things about Gatekeeper is that it’s portable, which means that you can implement it on any Kubernetes clusters, and if you are already using OPA, you might be able to port that policy over to Gatekeeper.\nManaging Multiple Clusters Best Practices  Limit the blast radius of your clusters to ensure cascading failures don’t have a bigger impact on your applications. If you have regulatory concerns such as PCI, HIPPA, or HiTrust, think about utilizing multiclusters to ease the complexity of mixing these workloads with general workloads. If hard multitenancy is a business requirement, workloads should be deployed to a dedicated cluster. If multiple regions are needed for your applications, utilize a Global Load Balancer to manage traffic between clusters. You can break out specialized workloads such as HPC into their own individual clusters to ensure that the specialized needs for the workloads are met. If you’re deploying workloads that will be spread across multiple regional datacenters, first ensure that there is a data replication strategy for the workload. Multiple clusters across regions can be easy, but replicating data across regions can be complicated, so ensure that there is a sound strategy to handle asynchronous and synchronous workloads. Utilize Kubernetes operators like the prometheus-operator or Elasticsearch operator to handle automated operational tasks. When designing your multicluster strategy, also consider how you will do service discovery and networking between clusters. Service mesh tools like HashiCorp’s Consul or Istio can help with networking across clusters. Be sure that your CD strategy can handle multiple rollouts between regions or multiple clusters. Investigate utilizing a GitOps approach to managing multiple cluster operational components to ensure consistency between all clusters in your fleet. The GitOps approach doesn’t always work for everyone’s environment, but you should at least investigate it to ease the operational burden of multicluster environments.  Why Multiple Clusters?  Blast radius Compliance Security Hard multitenancy Regional-based workloads Specialized workloads  Multicluster Design Concerns  Data replication Service discovery Network routing Operational management Continuous deployment  Managing Multiple Cluster Deployments  Operator GitOps  Multicluster Management Tools  Rancher\ncentrally manages multiple Kubernetes clusters in a centrally managed user interface (UI). KQueen\nmultitenant self-service portal for Kubernetes cluster provisioning and focuses on auditing, visibility, and security of multiple Kubernetes clusters. Gardener\nprovide Kubernetes as a Service to your end users  Kubernetes Federation KubeFed is not necessarily about multicluster management, but providing high availability (HA) deployments across multiple clusters. It allows you to combine multiple clusters into a single management endpoint for delivering applications on Kubernetes. For example, if you have a cluster that resides in multiple public cloud environments, you can combine these clusters into a single control plane to manage deployments to all clusters to increase the resiliency of your application.\nConnecting Cluster and External Services Best Practices  Establish network connectivity between the cluster and on-premises. Networking can be varied between different sites, clouds, and cluster configurations, but first ensure that pods can talk to on-premises machines and vice versa. To access services outside of the cluster, you can use selector-less services and directly program in the IP address of the machine (e.g., the database) with which you want to communicate. If you don’t have fixed IP addressess, you can instead use CNAME services to redirect to a DNS name. If you have neither a DNS name nor fixed services, you might need to write a dynamic operator that periodically synchronizes the external service IP addresses with the Kubernetes Service endpoints. To export services from Kubernetes, use internal load balancers or NodePort services. Internal load balancers are typically easier to use in public cloud environments where they can be bound to the Kubernetes Service itself. When such load balancers are unavailable, NodePort services can expose the service on all of the machines in the cluster. You can achieve connections between Kubernetes clusters through a combination of these two approaches, exposing a service externally that is then consumed as a selector-less service in the other Kubernetes cluster.  Machine Leaning on Kubernetes Best Practices  Smart scheduling and autoscaling. Given that most stages of the machine learning workflow are batch by nature, we recommend that you utilize a Cluster Autoscaler. GPU-enabled hardware is costly, and you certainly do not want to be paying for it when it’s not in use. We recommend batching jobs to run at specific times using either taints and tolerations or via a time-specific Cluster Autoscaler. That way, the cluster can scale to the needs of the machine learning workloads when needed, and not a moment sooner. Regarding taints and tolerations, upstream convention is to taint the node with the extended resource as the key. For example, a node with NVIDIA GPUs should be tainted as follows: Key: nvidia.com/gpu, Effect: NoSchedule. Using this method means that you can also utilize the ExtendedResourceToleration admission controller, which will automatically add the appropriate tolerations for such taints to pods requesting extended resources so that the users don’t need to manually add them. The truth is that model training is a delicate balance. Allowing things to move faster in one area often leads to bottlenecks in others. It’s an endeavor of constant observation and tuning. As a general rule of thumb, we recommend that you try to make the GPU become the bottleneck because it is the most costly resource. Keep your GPUs saturated. Be prepared to always be on the lookout for bottlenecks, and set up your monitoring to track the GPU, CPU, network, and storage utilization. Mixed workload clusters. Clusters that are used to run the day-to-day business services might also be used for the purposes of machine learning. Given the high performance requirements of machine learning workloads, we recommend using a separate node pool that’s tainted to accept only machine learning workloads. This will help protect the rest of the cluster from any impact from the machine learning workloads running on the machine learning node pool. Furthermore, you should consider multiple GPU-enabled node pools, each with different performance characteristics to suit the workload types. We also recommend enabling node autoscaling on the machine learning node pool(s). Use mixed mode clusters only after you have a solid understanding of the performance impact that your machine learning workloads have on your cluster. Achieving linear scaling with distributed training. This is the holy grail of distributed model training. Most libraries unfortunately don’t scale in a linear fashion when distributed. There is lots of work being done to make scaling better, but it’s important to understand the costs because this isn’t as simple as throwing more hardware at the problem. In our experience, it’s almost always the model itself and not the infrastructure supporting it that is the source of the bottleneck. It is, however, important to review the utilization of the GPU, CPU, network, and storage before pointing fingers at the model itself. Open source tools such as Horovod seek to improve distributed training frameworks and provide better model scaling.  Building Application Platforms Best Practices  Use admission controllers to limit and modify API calls to the cluster. An admission controller can validate (and reject invalid) Kubernetes resources. A mutating admission controller can automatically modify API resources to add new sidecars or other changes that users might not even need to know about. Use kubectl plug-ins to extend the Kubernetes user experience by adding new tools to the familiar existing command-line tool. In rare occasions, a purpose-built tool might be more appropriate. When building platforms on top of Kubernetes, think carefully about the users of the platform and how their needs will evolve. Making things simple and easy to use is clearly a good goal, but if this also leads to users that are trapped and unable to be successful without rewriting everything outside of your platform, it will ultimately be a frustrating (and unsuccessful) experience.  Managing State and Stateful Applications Volume Best Practices  Try to limit the use of volumes to pods requiring multiple containers that need to share data, for example adapter or ambassador type patterns. Use the emptyDir for those types of sharing patterns. Use hostDir when access to the data is required by node-based agents or services. Try to identify any services that write their critical application logs and events to local disk, and if possible change those to stdout or stderr and let a true Kubernetes-aware log aggregation system stream the logs instead of leveraging the volume map.  Kubernetes Storage Best Practices Cloud native application design principles try to enforce stateless application design as much as possible; however, the growing footprint of container-based services has created the need for data storage persistence. These best practices around storage in Kubernetes in general will help to design an effective approach to providing the required storage implementations to the application design:\n If possible, enable the DefaultStorageClass admission plug-in and define a default storage class. Many times, Helm charts for applications that require PersistentVolumes default to a default storage class for the chart, which allows the application to be installed without too much modification. When designing the architecture of the cluster, either on-premises or in a cloud provider, take into consideration zone and connectivity between the compute and data layers using the proper labels for both nodes and PersistentVolumes, and using affinity to keep the data and workload as close as possible. The last thing you want is a pod on a node in zone A trying to mount a volume that is attached to a node in zone B. Consider very carefully which workloads require state to be maintained on disk. Can that be handled by an outside service like a database system or, if running in a cloud provider, by a hosted service that is API consistent with currently used APIs, say a mongoDB or mySQL as a service? Determine how much effort would be involved in modifying the application code to be more stateless. While Kubernetes will track and mount the volumes as workloads are scheduled, it does not yet handle redundancy and backup of the data that is stored in those volumes. The CSI specification has added an API for vendors to plug in native snapshot technologies if the storage backend can support it. Verify the proper life cycle of the data that volumes will hold. By default the reclaim policy is set to for dynamically provisioned persistentVolumes which will delete the volume from the backing storage provider when the pod is deleted. Sensitive data or data that can be used for forensic analysis should be set to reclaim.  StatefulSet and Operator Best Practices  The decision to use Statefulsets should be taken judiciously because usually stateful applications require much deeper management that the orchestrator cannot really manage well yet (read the “Operators” section for the possible future answer to this deficiency in Kubernetes). The headless Service for the StatefulSet is not automatically created and must be created at deployment time to properly address the pods as individual nodes. When an application requires ordinal naming and dependable scaling, it does not always mean it requires the assignment of PersistentVolumes. If a node in the cluster becomes unresponsive, any pods that are part of a StatefulSet are not not automatically deleted; they instead will enter a Terminating or Unkown state after a grace period. The only way to clear this pod is to remove the node object from the cluster, the kubelet beginning to work again and deleting the pod directly, or an Operator force deleting the pod. The force delete should be the last option and great care should be taken that the node that had the deleted pod does not come back online, because there will now be two pods with the same name in the cluster. You can use kubectl delete pod nginx-0 \u0026ndash;grace-period=0 \u0026ndash;force to force delete the pod. Even after force deleting a pod, it might stay in an Unknown state, so a patch to the API server will delete the entry and cause the StatefulSet controller to create a new instance of the deleted pod: kubectl patch pod nginx-0 -p \u0026lsquo;{\u0026lsquo;metadata\u0026rsquo;:{\u0026lsquo;finalizers\u0026rsquo;:null}}\u0026rsquo;. If you’re running a complex data system with some type of leader election or data replication confirmation processes, use preStop hook to properly close any connections, force leader election, or verify data synchronization before the pod is deleted using a graceful shutdown process. When the application that requires stateful data is a complex data management system, it might be worth a look to determine whether an Operator exists to help manage the more complicated life cycle components of the application. If the application is built in-house, it might be worth investigating whether it would be useful to package the application as an Operator to add additional manageability to the application. Look at the CoreOS Operator SDK for an example.  ReplicaSet vs. StatefulSets ReplicaSet:\n Pods in a ReplicaSet are scaled out and assigned random names when scheduled. Pods in a ReplicaSet are scaled down in an arbitrary manner. Pods in a ReplicaSet are never called directly through their name or IP address but through their association with a Service. Pods in a ReplicaSet can be restarted and moved to another node at any time. Pods in a ReplicaSet that have a PersistentVolume mapped are linked only by the claim, but any new pod with a new name can take over the claim if needed when - rescheduled.  StatefulSets:\n Pods in a StatefulSet are scaled out and assigned sequential names. As the set scales up, the pods get ordinal names, and by default a new pod must be fully - online (pass its liveness and/or readiness probes) before the next pod is added. - Pods in a StatefulSet are scaled down in reverse sequence. - Pods in a StatefulSet can be addressed individually by name behind a headless Service. - Pods in a StatefulSet that require a volume mount must use a defined PersistentVolume template. Volumes claimed by pods in a StatefulSet are not deleted when the StatefulSet is deleted.  Admission Control Best Practices  Admission plug-in ordering doesn’t matter. Don’t mutate the same fields. Configuring multiple mutating admission webhooks also presents challenges. Fail open/fail closed. You might recall seeing the failurePolicy field as part of both the mutating and validating webhook configuration resources. This field defines how the API server should proceed in the case where the admission webhooks have access issues or encounter unrecognized errors. You can set this field to either Ignore or Fail. Ignore essentially fails to open, meaning that processing of the request will continue, whereas Fail denies the entire request. This might seem obvious, but the implications in both cases require consideration. Ignoring a critical admission webhook could result in policy that the business relies on not being applied to a resource without the user knowing.\nOne potential solution to protect against this would be to raise an alert when the API server logs that it cannot reach a given admission webhook. Fail can be even more devastating by denying all requests if the admission webhook is experiencing issues. To protect against this you can scope the rules to ensure that only specific resource requests are set to the admission webhook. As a tenet, you should never have any rules that apply to all resources in the cluster. If you have written your own admission webhook, it’s important to remember that user/system requests can be directly affected by the time it takes for your admission webhook to make a decision and respond. All admission webhook calls are configured with a 30-second timeout, after which time the failurePolicy takes effect. Even if it takes several seconds for your admission webhook to make an admit/deny decision, it can severely affect user experience when working with the cluster. Avoid having complex logic or relying on external systems such as databases in order to process the admit/deny logic. Scoping admission webhooks. There is an optional field that allows you to scope the namespaces in which the admission webhooks operate on via the NamespaceSelector field. This field defaults to empty, which matches everything, but can be used to match namespace labels via the use of the matchLabels field. We recommend that you always use this field because it allows for an explicit opt-in per namespace. The kube-system namespace is a reserved namespace that’s common across all Kubernetes clusters. It’s where all system-level services operate. We recommend never running admission webhooks against the resources in this namespace specifically, and you can achieve this by using the NamespaceSelector field and simply not matching the kube-system namespace. You should also consider it on any system-level namespaces that are required for cluster operation. Lock down admission webhook configurations with RBAC. Now that you know about all the fields in the admission webhook configuration, you have probably thought of a really simple way to break access to a cluster. It goes without saying that the creation of both a MutatingWebhookConfiguration and ValidatingWebhookConfiguration is a root-level operation on the cluster and must be locked down appropriately using RBAC. Failure to do so can result in a broken cluster or, even worse, an injection attack on your application workloads. Don’t send sensitive data. Admission webhooks are essentially black boxes that accept AdmissionRequests and output AdmissionResponses. How they store and manipulate the request is opaque to the user. It’s important to think about what request payloads you are sending to the admission webhook. In the case of Kubernetes secrets or ConfigMaps, they might contain sensitive information and require strong guarantees about how that information is stored and shared. Sharing these resources with an admission webhook can leak sensitive information, which is why you should scope your resource rules to the minimum resource needed to validate and/or mutate.  Authorization In Kubernetes, the authorization of each request is performed after authentication but before admission.\nUnlike admission controllers, if a single authorization module admits the request, the request can proceed. Only for the case in which all modules deny the request will an error be returned to the user.\nAuthorization Modules  Attribute-Based Access Control (ABAC)\nAllows authorization policy to be configured via local files RBAC\nAllows authorization policy to be configured via the Kubernetes API Webhook\nAllows the authorization of a request to be handled via a remote REST endpoint Node\nSpecialized authorization module that authorizes requests from kubelets  Authorization Best Practices  Given that the ABAC policies need to be placed on the filesystem of each master node and kept synchronized, we generally recommend against using ABAC in multimaster clusters. The same can be said for the webhook module because the configuration is based on a file and a corresponding flag being present. Furthermore, changes to these policies in the files require a restart of the API server to take effect, which is effectively a control-plane outage in a single master cluster or inconsistent configuration in a multimaster cluster. Given these details, we recommend using only the RBAC module for user authorization because the rules are configured and stored in Kubernetes itself. Webhook modules, although powerful, are potentially very dangerous. Given that every request is subject to the authorization process, a failure of a webhook service would be devastating for a cluster. Therefore, we generally recommend not using external authorization modules unless you completely vet and are comfortable with your cluster failure modes if the webhook service becomes unreachable or unavailable.   Please refer to the book for details.   Links  Kubernetes Best Practices: Blueprints for Building Successful Applications on Kubernetes\nDid you find this page helpful? Consider sharing it 🙌 ","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602288000,"objectID":"376a2eaa759b78f9c57019f6a902a76e","permalink":"/devops/kubernetes-best-practices/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/devops/kubernetes-best-practices/","section":"devops","summary":"A summary of best practices of Kubernetes.","tags":["System","Kubernetes","K8S"],"title":"Kubernetes Best Practices","type":"devops"},{"authors":["Hongwei Li"],"categories":["Linux","Coding","DevOps","Networking","Courses"],"content":"The Latest and Updated Posts  Coding-Python\nCoding-Java\nCoding-Go\nCoding-Javascript\nLinux\nDevOps | SRE\nNetworking\nCourses\n","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":253402214400,"objectID":"d0c1153bc00cda8f6a82e96f46af39f5","permalink":"/post/latest-and-updates/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/post/latest-and-updates/","section":"post","summary":"Please check each category for latest and updated posts.","tags":["Post"],"title":"-- Latest and Updated Posts -- ","type":"post"},{"authors":["Hongwei Li"],"categories":["Linux","System","DevOps","SRE"],"content":"Docker What is Docker? Docker is basically a container engine which uses the Linux Kernel features like namespaces and control groups to create containers on top of an operating system and automates application deployment on the container. Docker uses Copy-on-write union file system for its backend storage.\nWhat is the difference between a docker container and a docker image? Docker Image is a set of files which has no state, whereas Docker Container is the instantiation of Docker Image. In other words, Docker Container is the run time instance of images.\nWhat is a container? In 4 bullet points:\n Containers share the host kernel Containers use the kernel ability to group processes for resource control Containers ensure isolation through namespaces Containers feel like lightweight VMs (lower footprint, faster), but are not Virtual Machines!  Namespaces and Cgroups - - - - - - \u0026gt; Docker Docker makes use of kernel namespaces to provide the isolated workspace called the container.\nDocker also makes use of kernel control groups for resource allocation and isolation.\nContainer Format Docker Engine combines the namespaces, control groups and UnionFS into a wrapper called a container format. The default container format is libcontainer.\nTypes of Containers Given the above constructs, containers may be divided into 3 types as follows:\n System Containers share rootfs, PID, network, IPC and UTS with host system but live inside a cgroup. Application Containers live inside a cgroup and use namespaces (PID, network, IPC, chroot) for isolation from host system Pods use namespaces for isolation from host system but create sub groups which share PID, network, IPC and UTS except the rootfs.  Images \u0026amp; Layers Each Docker image references a list of read-only layers that represent filesystem differences. Layers are stacked on top of each other to form a base for a container’s rootfs.\nOne big innovation of the Docker engine was the concept of leveraging Copy-On-Write file systems to significantly speed up the preparation of the rootfs.\nCopy-On-Write When Docker creates a container, it adds a new, thin, writable layer on top of the underlying stack of image layers. This layer is often called the “container layer”.\nAll changes made to the running container - such as writing new files, modifying existing files, and deleting files - are written to this thin writable container layer.\nUnion File Systems Union File Systems provide the following features for storage:\n Layering Copy-On-Write Caching Diffing  Dangling images Docker images consist of multiple layers.\n Dangling images are layers that have no relationship to any tagged images. They no longer serve a purpose and consume disk space. They can be located by adding the filter flag, -f with a value of dangling=true to the docker images command.  Another description:\n An unused image means that it has not been assigned or used in a container. For example, when running docker ps -a - it will list all of your exited and currently running containers. Any images shown being used inside any of containers are a \u0026ldquo;used image\u0026rdquo;. On the other hand, a dangling image just means that you\u0026rsquo;ve created the new build of the image, but it wasn\u0026rsquo;t given a new name. So the old images you have becomes the \u0026ldquo;dangling image\u0026rdquo;. Those old images are the ones that are untagged and displays \u0026ldquo;\u0026rdquo; on its name when you run docker images. When running docker system prune -a, it will remove both unused and dangling images. Therefore any images being used in a container, whether they have been exited or currently running, will NOT be affected.  The Difference between COPY and ADD in a Dockerfile Sometimes you see COPY or ADD being used in a Dockerfile, but 99% of the time you should be using COPY, here\u0026rsquo;s why.\n  COPY and ADD are both Dockerfile instructions that serve similar purposes. They let you copy files from a specific location into a Docker image.\n  COPY takes in a src and destination. It only lets you copy in a local file or directory from your host (the machine building the Docker image) into the Docker image itself.\n  ADD lets you do that too, but it also supports 2 other sources.\n First, you can use a URL instead of a local file / directory. Secondly, you can extract a tar file from the source directly into the destination.    In most cases if you’re using a URL, you’re downloading a zip file and are then using the RUN command to extract it. However, you might as well just use RUN with curl instead of ADD here so you chain everything into 1 RUN command to make a smaller Docker image.\n  A valid use case for ADD is when you want to extract a local tar file into a specific directory in your Docker image. This is exactly what the Alpine image does with ADD rootfs.tar.gz /.\n  If you’re copying in local files to your Docker image, always use COPY because it’s more explicit.\n  CMD vs. ENTRYPOINT  Run or execute something when docker starts The main purpose of a CMD is to provide defaults for an executing container An ENTRYPOINT helps you to configure a container that you can run as an executable CMD can be overridden, The ENTRYPOINT instruction works very similarly to CMD in that it is used to specify the command executed when the container is started. However, where it differs is that ENTRYPOINT doesn’t allow you to override the command. CMD will be overridden by the ‘docker run …….’ command line, ENTRYPOINT just gets the parameter from ‘docker run …….’ command line One important thing to call out about the ENTRYPOINT instruction is that syntax is critical. Technically, ENTRYPOINT supports both the ENTRYPOINT [\u0026ldquo;command\u0026rdquo;] syntax and the ENTRYPOINT command syntax. However, while both of these are supported, they have two different meanings and change how ENTRYPOINT works.  CMD and ENTRYPOINT syntax Both CMD and ENTRYPOINT are straight forward but they have a hidden, err, \u0026ldquo;feature\u0026rdquo; that can cause issues if you are not aware. Two different syntaxes are supported for these instructions.\nCMD /bin/echo\nor\nCMD [\u0026quot;/bin/echo\u0026rdquo;]\nThis may not look like it would be an issues but the devil in the details will trip you up. If you use the second syntax where the CMD ( or ENTRYPOINT ) is an array, it acts exactly like you would expect. If you use the first syntax without the array, docker pre-pends /bin/sh -c to your command. This has always been in docker as far as I can remember.\nPre-pending /bin/sh -c can cause some unexpected issues and functionality that is not easily understood if you did not know that docker modified your CMD. Therefore, you should always use the array syntax for both instructions because both will be executed exactly how you intended.\n Always use the array syntax when using CMD and ENTRYPOINT.   Docker commands  docker build docker pull docker push docker images … docker commit docker exec -it docker run -it docker system  df events info prune   docker ps  Docker instructions in dockerfile  ENV RUN CMD ENTRYPOINT COPY ADD USER WORKDIR ARG EXPOSE  Dockerfile Best Practices  Best practices for writing Dockerfiles\nIntro Guide to Dockerfile Best Practices\nDocker development best practices\nBest practices writing a Dockerfile\nLinks  The Difference between COPY and ADD in a Dockerfile\nDid you find this page helpful? Consider sharing it 🙌 ","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601856000,"objectID":"d0ea8b33801b8e785599e98aba54c8c2","permalink":"/devops/docker/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/devops/docker/","section":"devops","summary":"A short summary of Docker.","tags":["Linux","System","Docker"],"title":"Docker","type":"devops"},{"authors":["Hongwei Li"],"categories":["Java"],"content":"Java Collection  Any group of individual objects which are represented as a single unit is known as the collection of the objects. The Collection in Java is a framework that provides an architecture to store and manipulate the group of objects. The Collection interface (java.util.Collection) and Map interface (java.util.Map) are the two main “root” interfaces of Java collection classes. Java Collections can achieve all the operations that you perform on a data such as searching, sorting, insertion, manipulation, and deletion.  Java Collection Hierarchy The utility package, (java.util) contains all the classes and interfaces that are required by the collection framework. The collection framework contains an interface named as an iterable interface which provides the iterator to iterate through all the collections. This interface is extended by the main collection interface which acts as a root for the collection framework. All the collections extend this collection interface thereby extending the properties of the iterator and the methods of this interface.\nIterable Interface This is the root interface for the entire collection framework. The collection interface extends the iterable interface. Therefore, inherently, all the interfaces and classes implement this interface. The main functionality of this interface is to provide an iterator for the collections. Therefore, this interface contains only one abstract method which is the iterator. It returns the\nIterator iterator();  List Interface  This interface is dedicated to the data of the list type in which we can store all the ordered collection of the objects. This also allows duplicate data to be present in it. It is implemented by various classes like ArrayList, Vector, Stack, etc.  ArrayList  ArrayList provides us with dynamic arrays in Java. Though, it may be slower than standard arrays but can be helpful in programs where lots of manipulation in the array is needed. The size of an ArrayList is increased automatically if the collection grows or shrinks if the objects are removed from the collection. It is like an array, but there is no size limit. Java ArrayList allows us to randomly access the list. ArrayList can not be used for primitive types, like int, char, etc. We will need a wrapper class for such cases. The ArrayList class maintains the insertion order and is non-synchronized. The elements stored in the ArrayList class can be randomly accessed.  LinkedList  LinkedList is a linear data structure where the elements are not stored in contiguous locations and every element is a separate object with a data part and address part. The elements are linked using pointers and addresses. Each element is known as a node. It uses a doubly linked list internally to store the elements. It can store the duplicate elements. It maintains the insertion order and is not synchronized. In LinkedList, the manipulation is fast because no shifting is required. It can be used as a list, stack or queue.  Vector  Vector uses a dynamic array to store the data elements. It may be slower than standard arrays but can be helpful in programs where lots of manipulation in the array is needed. The primary difference between a vector and an ArrayList is that a Vector is synchronized and an ArrayList is non-synchronized. It is synchronized and contains many methods that are not the part of Collection framework.  Stack  It implements the last-in-first-out data structure. The stack is the subclass of Vector. The stack contains all of the methods of Vector class and also provides its methods like boolean push(), boolean peek(), boolean push(object o), which defines its properties.   Stack is a subclass of Vector and a legacy class. It is thread safe which might be an overhead in an environment where thread safety is not needed.\nAn alternate to Stack is to use ArrayDequeue which is not thread safe and faster array implementation.   Queue Interface A queue interface maintains the FIFO(First In First Out) order similar to a real-world queue line. This interface is dedicated to storing all the elements where the order of the elements matter. There are various classes like PriorityQueue, Deque, ArrayDeque, etc.\nPriority Queue  A PriorityQueue is used when the objects are supposed to be processed based on the priority. It is known that a queue follows the First-In-First-Out algorithm, but sometimes the elements of the queue are needed to be processed according to the priority and this class is used in these cases. The PriorityQueue is based on the priority heap. The elements of the priority queue are ordered according to the natural ordering, or by a Comparator provided at queue construction time, depending on which constructor is used. PriorityQueue doesn\u0026rsquo;t allow null values to be stored in the queue. The elements in PriorityQueue must be of Comparable type. String and Wrapper classes are Comparable by default. To add user-defined objects in PriorityQueue, you need to implement Comparable interface.  Deque Interface Deque, also known as a double-ended queue, is a data structure where we can add and remove the elements from both the ends of the queue.\n This interface extends the queue interface. The class which implements this interface is ArrayDeque.  ArrayDeque  The ArrayDeque class provides the facility of using deque and resizable-array. This is a special kind of array that grows and allows users to add or remove an element from both sides of the queue. Array deques have no capacity restrictions and they grow as necessary to support usage. ArrayDeque is faster than ArrayList and Stack. Unlike Queue, we can add or remove elements from both sides. Null elements are not allowed in the ArrayDeque. ArrayDeque is not thread safe, in the absence of external synchronization.  Set Interface  A set is an unordered collection of objects in which duplicate values cannot be stored. This collection is used when we wish to avoid the duplication of the objects and wish to store only the unique objects. This set interface is implemented by various classes like HashSet, TreeSet, LinkedHashSet, etc.  HashSet  The HashSet class is an inherent implementation of the hash table data structure. HashSet stores the elements by using a mechanism called hashing. HashSet contains unique elements only. HashSet allows null value. HashSet class is non synchronized. HashSet doesn\u0026rsquo;t maintain the insertion order. Here, elements are inserted on the basis of their hashcode. HashSet is the best approach for search operations. The initial default capacity of HashSet is 16, and the load factor is 0.75.  LinkedHashSet  LinkedHashSet uses a doubly linked list to store the data and retains the ordering of the elements. Like HashSet, It also contains unique elements only. It maintains the insertion order. It permits null elements. LinkedHashSet class is non synchronized.  SortedSet Interface  This interface is very similar to the set interface. The only difference is that this interface has extra methods that maintain the ordering of the elements. The sorted set interface extends the set interface and is used to handle the data which needs to be sorted. The class which implements this interface is TreeSet.  TreeSet  TreeSet class contains unique elements only like HashSet. TreeSet class doesn\u0026rsquo;t allow null element. The TreeSet class uses a Tree for storage. The ordering of the elements is maintained by a set using their natural ordering whether or not an explicit comparator is provided. It can also be ordered by a Comparator provided at set creation time, depending on which constructor is used. the access and retrieval time of TreeSet is quite fast. The elements in TreeSet stored in ascending order. TreeSet class is non synchronized.  Map Interface A map is a data structure which supports the key-value pair mapping for the data. This interface doesn’t support duplicate keys because the same key cannot have multiple mappings. A map is useful if there is a data and we wish to perform operations on the basis of the key. This map interface is implemented by various classes like HashMap, TreeMap, etc.\nThere are two interfaces for implementing Map in java: Map and SortedMap, and three classes: HashMap, LinkedHashMap, and TreeMap. The hierarchy of Java Map is given below:\nHashMap HashMap provides the basic implementation of the Map interface of Java. It stores the data in (Key, Value) pairs. To access a value in a HashMap, we must know its key. HashMap uses a technique called Hashing. Hashing is a technique of converting a large String to small String that represents the same String so that the indexing and search operations are faster. HashSet also uses HashMap internally.\n Java HashMap contains values based on the key. Java HashMap contains only unique keys. Java HashMap may have one null key and multiple null values. Java HashMap is non synchronized. Java HashMap maintains no order. The initial default capacity of Java HashMap class is 16 with a load factor of 0.75.  HashMap implementation\nWorking of HashMap in Java\n It stores the data in the pair of Key and Value. HashMap contains an array of the nodes, and the node is represented as a class. It uses an array and LinkedList data structure internally for storing Key and Value.  LinkedHashMap  LinkedHashMap contains values based on the key. LinkedHashMap contains unique elements. LinkedHashMap may have one null key and multiple null values. LinkedHashMap is non synchronized. LinkedHashMap maintains insertion order. The initial default capacity of Java HashMap class is 16 with a load factor of 0.75.  TreeMap Java TreeMap class is a red-black tree based implementation. It provides an efficient means of storing key-value pairs in sorted order.\n TreeMap contains values based on the key. It implements the NavigableMap interface and extends AbstractMap class. TreeMap contains only unique elements. TreeMap cannot have a null key but can have multiple null values. TreeMap is non synchronized. TreeMap maintains ascending order.  HashTable Java Hashtable class inherits Dictionary class and implements the Map interface.\n A Hashtable is an array of a list. Each list is known as a bucket. The position of the bucket is identified by calling the hashcode() method. A Hashtable contains values based on the key. Hashtable class contains unique elements. Hashtable class doesn\u0026rsquo;t allow null key or value. Hashtable class is synchronized. The initial default capacity of Hashtable class is 11 whereas loadFactor is 0.75.  EnumSet EnumSet class is the specialized Set implementation for use with enum types. It inherits AbstractSet class and implements the Set interface.\nEnumMap EnumMap class is the specialized Map implementation for enum keys. It inherits Enum and AbstractMap classes.\nCollections class This class consists exclusively of static methods that operate on or return collections. It contains polymorphic algorithms that operate on collections, \u0026ldquo;wrappers\u0026rdquo;, which return a new collection backed by a specified collection, and a few other odds and ends.\nThe methods of this class all throw a NullPointerException if the collections or class objects provided to them are null.\nSorting in collection  String objects Collections.sort() Wrapper class objects Collections.sort() User-defined class objects\nCollections.sort() with user-defined class implements comparable interface (compareTo() method)  Comaprable  Java Comparable interface is used to order the objects of the user-defined class. This interface is found in java.lang package and contains only one method named compareTo(Object). It provides a single sorting sequence only, i.e., you can sort the elements on the basis of single data member only. For example, it may be rollno, name, age or anything else.  compareTo(Object obj) method public int compareTo(Object obj): It is used to compare the current object with the specified object. It returns\n positive integer, if the current object is greater than the specified object. negative integer, if the current object is less than the specified object. zero, if the current object is equal to the specified object.  Comparator  Java Comparator interface is used to order the objects of a user-defined class. This interface is found in java.util package and contains 2 methods compare(Object obj1,Object obj2) and equals(Object element). It provides multiple sorting sequences, i.e., you can sort the elements on the basis of any data member, for example, rollno, name, age or anything else.  Comparator Implementation  Non-generic Old Style Generic style  Java 8 Comparator Interface Java 8 has more comparator methods\nProperties class  The properties object contains key and value pair both as a string. The java.util.Properties class is the subclass of Hashtable. It can be used to get property value based on the property key. The Properties class provides methods to get data from the properties file and store data into the properties file. Moreover, it can be used to get the properties of a system. Recompilation is not required if the information is changed from a properties file: If any information is changed from the properties file, you don\u0026rsquo;t need to recompile the java class. It is used to store information which is to be changed frequently.  A Little Deep In Queues The queue (a FIFO list) Implementation of a queue\n A queue (of bounded size) can be efficiently implemented in an array. A queue can be efficiently implemented using any linked list that supports deletion in the front and insertion at the end in constant time. The first (last) element of the queue is at the front (end) of the linked list.  The stack (a LIFO list) Implementation of a stack\n A stack (of bounded size) can be efficiently implemented using an array b and an int variable n: The n elements of the stack are in b[0..n-1], with b[0] being the bottom element and b[n-1] being the top element. A stack can be efficiently implemented using a linked list. The first element is the top of the stack and the last element is the bottom. It’s easy to push (prepend) an element and pop (remove) the first element in constant time.  The deque The word deque, usually pronounced deck, is short for double-ended queue. A deque is a list that supports insertion and removal at both ends. Thus, a deque can be used as a queue or as a stack.\nStacks, queues, and deques in the Java Collection framework   Java has interface Deque. It is implemented by classes ArrayDeque (which implements a list in an expandable array) and LinkedList, so these two classes can be used for a queue and for a stack.\n  Both ArrayDeque and LinkedList also implement interface Queue, so you can use this interface to restrict operations to queue operations. For example, create a LinkedList and assign it to a Queue variable.\n Queue\u0026lt;E\u0026gt; q= new LinkedList\u0026lt;\u0026gt;();    Thereafter, use only q for the LinkedList and operations are restricted to queue operations.\n Java also has a class Stack, which implements a stack in an expandable array. However, the Java API would rather you use an ArrayDeque. The problem is that there is no suitable way to restrict the operations of an Array-Deque to stack operations, so we prefer to use class Stack.  Some Implementations  LinkedList: Although mainly known to be a List implementation, this class also implements the Queue interface. This implementation works by linking its elements together and going through that chain when iterating or searching for elements. ArrayDeque: An implementation of both Queue and Deque. It\u0026rsquo;s backed up by an array, which can be increased when the number of elements increase over its current capacity. DelayQueue: Can only contain elements which implement the Delayed interface - elements that become active after a certain time. The DelayQueue will only deliver elements whose delays have expired. PriorityQueue: Orders its elements according to their natural order or a Comparator (if provided). This means it doesn\u0026rsquo;t work using the FIFO principle, but rather returns the element with the highest priority (defined by how they compare to each other).  Non-generic vs Generic   Java collection framework was non-generic before JDK 1.5. Since 1.5, it is generic.\n  Java new generic collection allows you to have only one type of object in a collection. Now it is type safe so typecasting is not required at runtime.\n  In a generic collection, the type is specified in angular braces.\n  If trying to add another type of object into generic collection, it gives compile time error.\n ArrayList list=new ArrayList();//creating old non-generic arraylist ArrayList\u0026lt;String\u0026gt; list=new ArrayList\u0026lt;String\u0026gt;();//creating new generic arraylist    Ways to iterate the elements of the collection in Java  By Iterator interface. By for-each loop. By ListIterator interface. By for loop. By forEach() method. By forEachRemaining() method.  Comparison Array vs ArrayList    Basis Array ArrayList     Definition An array is a dynamically-created object. It serves as a container that holds the constant number of values of the same type. It has a contiguous memory location. The ArrayList is a class of Java Collections framework. It contains popular classes like Vector, HashTable, and HashMap.   Size Array is static in size. ArrayList is dynamic in size.   Resizable An array is a fixed-length data structure. ArrayList is a variable-length data structure. It can be resized itself when needed.   Initialization It is mandatory to provide the size of an array while initializing it directly or indirectly. We can create an instance of ArrayList without specifying its size. Java creates ArrayList of default size.   Performance It performs fast in comparison to ArrayList because of fixed size. ArrayList is internally backed by the array in Java. The resize operation in ArrayList slows down the performance.   Primitive/ Generic type An array can store both objects and primitives type. We cannot store primitive type in ArrayList. It automatically converts primitive type to object.   Iterating Values We use for loop or for each loop to iterate over an array. We use an iterator to iterate over ArrayList.   Type-Safety We cannot use generics along with array because it is not a convertible type of array. ArrayList allows us to store only generic/ type, that\u0026rsquo;s why it is type-safe.   Length Array provides a length variable which denotes the length of an array. ArrayList provides the size() method to determine the size of ArrayList.   Adding Elements We can add elements in an array by using the assignment operator. Java provides the add() method to add elements in the ArrayList.   Single/ Multi-Dimensional Array can be multi-dimensional. ArrayList is always single-dimensional.         ArrayList vs LinkedList    ArrayList LinkedList     ArrayList internally uses a dynamic array to store the elements. LinkedList internally uses a doubly linked list to store the elements.   Manipulation with ArrayList is slow because it internally uses an array. If any element is removed from the array, all the bits are shifted in memory. Manipulation with LinkedList is faster than ArrayList because it uses a doubly linked list, so no bit shifting is required in memory.   An ArrayList class can act as a list only because it implements List only. LinkedList class can act as a list and queue both because it implements List and Deque interfaces.   ArrayList is better for storing and accessing data. LinkedList is better for manipulating data.    ArrayList vs Vector    ArrayList Vector     ArrayList is not synchronized. Vector is synchronized.   ArrayList increments 50% of current array size if the number of elements exceeds from its capacity. Vector increments 100% means doubles the array size if the total number of elements exceeds than its capacity.   ArrayList is not a legacy class. It is introduced in JDK 1.2. Vector is a legacy class.   ArrayList is fast because it is non-synchronized. Vector is slow because it is synchronized, i.e., in a multithreading environment, it holds the other threads in runnable or non-runnable state until current thread releases the lock of the object.   ArrayList uses the Iterator interface to traverse the elements. A Vector can use the Iterator interface or Enumeration interface to traverse the elements.    List vs Set A list can contain duplicate elements whereas Set contains unique elements only.\nHashMap vs LinkedHashMap vs TreeMap  A Map doesn\u0026rsquo;t allow duplicate keys, but you can have duplicate values. HashMap and LinkedHashMap allow null keys and values, but TreeMap doesn\u0026rsquo;t allow any null key or value. A Map can\u0026rsquo;t be traversed, so you need to convert it into Set using keySet() or entrySet() method.     Class Description     HashMap HashMap is the implementation of Map, but it doesn\u0026rsquo;t maintain any order.   LinkedHashMap LinkedHashMap is the implementation of Map. It inherits HashMap class. It maintains insertion order.   TreeMap TreeMap is the implementation of Map and SortedMap. It maintains ascending order.       HashMap TreeMap     HashMap can contain one null key. TreeMap cannot contain any null key.   HashMap maintains no order. TreeMap maintains ascending order.    HashMap vs Hashtable    HashMap Hashtable     HashMap and Hashtable both are used to store data in key and value form. Both are using hashing technique to store unique keys.    1) HashMap is non synchronized. It is not-thread safe and can\u0026rsquo;t be shared between many threads without proper synchronization code. Hashtable is synchronized. It is thread-safe and can be shared with many threads.   2) HashMap allows one null key and multiple null values. Hashtable doesn\u0026rsquo;t allow any null key or value.   3) HashMap is fast. Hashtable is slow.   4) We can make the HashMap as synchronized by calling this code Map m = Collections.synchronizedMap(hashMap); Hashtable is internally synchronized and can\u0026rsquo;t be unsynchronized.   5) HashMap is traversed by Iterator. Hashtable is traversed by Enumerator and Iterator.   6) Iterator in HashMap is fail-fast. Enumerator in Hashtable is not fail-fast.   7) HashMap inherits AbstractMap class. Hashtable inherits Dictionary class.    Collections Class vs Collection Framework    Collection Collections     The Collection is an interface whereas Collections is a utility class.   The Collection interface provides the standard functionality of data structure to List, Set, and Queue. However, Collections class contains only static methods like sort(), min(), max(), fill(), copy(), reverse() etc.   The Collection interface provides the methods that can be used for data structure whereas Collections class provides the static methods which can be used for various operation on a collection.    Comparable vs Comparator    key Comparable Comparator     Package Comparable interface belongs to java.lang package. Comparator interface belongs to java.util package.   Method The comparable interface has a method compareTo(Object a ) The comparator has a method compare(Object o1, Object O2)   Sorting uses Comparable provides single sorting sequence. In other words, we can sort the collection on the basis of single element such as id or title or rating etc. Collection.sort(List, Comparator) method can be used to sort the collection of Comparator type objects.   Sorting sequence Comparable provides single sorting sequence. Comparator provides multiple sorting sequence. In other words, we can sort the collection on the basis of multiple elements such as id, title and rating etc.   Original class Comparable affects the original class, i.e., the actual class is modified. Comparator provides compare() method to sort elements.    Links  Collections in Java\nJava Collections: Queue and Deque Interfaces\nJava ArrayList\n","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600560000,"objectID":"f6ee37a8bee5ec47ea054bab7e4a5bb7","permalink":"/coding/java/java-collection/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/coding/java/java-collection/","section":"coding","summary":"A brief description of Java collections.","tags":["Java","Collection","Collections"],"title":"Java Collection","type":"coding"},{"authors":["Hongwei Li"],"categories":["Python"],"content":"Type Hints What is the challenge Due to the dynamic nature of Python, inferring or checking the type of an object being used is especially hard. This fact makes it hard for developers to understand what exactly is going on in code they haven\u0026rsquo;t written and, most importantly, for type checking tools found in many IDEs [PyCharm, PyDev come to mind] that are limited due to the fact that they don\u0026rsquo;t have any indicator of what type the objects are. As a result they resort to trying to infer the type with (as mentioned in the presentation) around 50% success rate.\nType hinting Type hinting is a formal solution to statically indicate the type of a value within your Python code. It was specified in PEP 484 and introduced in Python 3.5.\nHere’s an example of adding type information to a function. You annotate the arguments and the return value:\ndef greet(name: str) -\u0026gt; str: return \u0026quot;Hello, \u0026quot; + name  The name: str syntax indicates the name argument should be of type str. The -\u0026gt; syntax indicates the greet() function will return a string.\nThe below two statements can be found in the Type Hinting presentation:\nWhy Type Hints  Helps Type Checkers: By hinting at what type you want the object to be the type checker can easily detect if, for instance, you\u0026rsquo;re passing an object with a type that isn\u0026rsquo;t expected. Helps with documentation: A third person viewing your code will know what is expected where, ergo, how to use it without getting them TypeErrors. Helps IDEs develop more accurate and robust tools: Development Environments will be better suited at suggesting appropriate methods when know what type your object is. You have probably experienced this with some IDE at some point, hitting the . and having methods/attributes pop up which aren\u0026rsquo;t defined for an object.  Why use Static Type Checkers?  Find bugs sooner: This is self evident, I believe. The larger your project the more you need it: Again, makes sense. Static languages offer a robustness and control that dynamic languages lack. The bigger and more complex your application becomes the more control and predictability (from a behavioral aspect) you require. Large teams are already running static analysis: I\u0026rsquo;m guessing this verifies the first two points.  Type Hinting with Mypy  mypy is an optional static type checker for Python that aims to combine the benefits of dynamic (or \u0026ldquo;duck\u0026rdquo;) typing and static typing. Mypy combines the expressive power and convenience of Python with a powerful type system and compile-time type checking. Mypy type checks standard Python programs; run them using any Python VM with basically no runtime overhead.\nPEP 484 doesn\u0026rsquo;t enforce anything; it is simply setting a direction for function annotations and proposing guidelines for how type checking can/should be performed. You can annotate your functions and hint as many things as you want; your scripts will still run regardless of the presence of annotations because Python itself doesn\u0026rsquo;t use them.\nAs noted in the PEP, hinting types should generally take three forms:\n Function annotations. PEP 3107 Stub files for built-in/user modules. Special # type: type comments that complement the first two forms. (See: What are variable annotations in Python 3.6? for a Python 3.6 update for # type: type comments) Additionally, you\u0026rsquo;ll want to use type hints in conjunction with the new typing module introduced in Py3.5.   Additionally, you\u0026rsquo;ll want to use type hints in conjunction with the new typing module introduced in Py3.5.   The Typing Module The Typing Module supports type hints as specified by PEP 484.\nThe typing module also supports:\n  Type aliasing Type hinting for callback functions: Callable  Generics - Abstract base classes have been extended to support subscription to denote expected types for container elements  User-defined generic types - A user-defined class can be defined as a generic class  Any type - Every type is a subtype of Any  Type Hinting Generics from typing import List class Solution: def twoSum(self, numbers: List[int], target: int) -\u0026gt; List[int]:  List, Tuple/etc vs list/tuple/etc typing.Tuple and typing.List are Generic types; this means you can specify what type their contents must be:\ndef f(points: Tuple[float, float]): return map(do_stuff, points)  This specifies that the tuple passed in must contain two float values.\nYou can\u0026rsquo;t do this with the built-in tuple type before Python 3.9.\nPython 3.9 has the change that built-in types support hints\nSummary  You should always pick the typing generics even when you are not currently restricting the contents. It is easier to add that constraint later with a generic type as the resulting change will be smaller.    From Python 3.9 (PEP 585) onwards tuple, list and various other classes are now generic types. Using these rather than their typing counterpart is now preferred.    You should always pick then non-typing generics whenever possible as the old typing.Tuple, typing.List and other generics are deprecated and will be removed in a later version of Python.   Links  What are type hints in Python 3.5? PEP 585 \u0026ndash; Type Hinting Generics In Standard Collections typing — Support for type hints Using List/Tuple/etc. from typing vs directly referring type as list/tuple/etc Static typing in python3: list vs List\nPEP 483 \u0026ndash; The Theory of Type Hints\nPEP 484 \u0026ndash; Type Hints\nType Hints - Guido van Rossum - PyCon 2015\nType Hinting\nType Checking With Mypy\nmypy\nDid you find this page helpful? Consider sharing it 🙌 ","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601769600,"objectID":"d0051d8d34b461ed78fcba9ab069a197","permalink":"/coding/python/list-tuple-dict-vs-list-tuple-dict/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/coding/python/list-tuple-dict-vs-list-tuple-dict/","section":"coding","summary":"A little bit about Type Hints.","tags":["Python","Type hints"],"title":"list, tuple, dict vs List, Tuple, Dict","type":"coding"},{"authors":["Hongwei Li"],"categories":["Linux","System"],"content":"Whatis Locate Which Whereis Whatis hongwei@840-g5:~$ whatis whatis whatis (1) - display one-line manual page descriptions  Locate hongwei@840-g5:~$ whatis locate locate (1) - find files by name  locate indeed finds all the files that have the pattern specified anywhere in their paths. You can tell it to only find files and directories whose names (rather than full paths) include the pattern with the -b option, which is usually what you want, and gives a less unwieldy list.\nlocate is fast because it uses a binary database that gets periodically updated (once daily, by cron). You can update it yourself to ensure recently added files are found by running sudo updatedb.\nOne more thing about locate - it doesn\u0026rsquo;t care whether files still exist or not, so to avoid finding recently deleted files, use -e. Often I also pipe to less as the list can be long. Typically I do:\nsudo updatedb \u0026amp;\u0026amp; locate -b -e psql | less  Which hongwei@840-g5:~$ whatis which which (1) - locate a command  which finds the binary executable of the program (if it is in your PATH)\nwhich returns the pathnames of the files (or links) which would be executed in the current environment, had its arguments been given as commands in a strictly POSIX-conformant shell. It does this by searching the PATH for executable files matching the names of the arguments. It does not canonicalize path names.\nhongwei@840-g5:~$ which psql /usr/bin/psql  Whereis hongwei@840-g5:~$ whatis whereis whereis (1) - locate the binary, source, and manual page files for a command  whereis finds the binary, the source, and the man page files for a program.\nwhereis locates the binary, source and manual files for the specified command names.\nwhereis attempts to locate the desired program in the standard Linux places, and in the places specified by $PATH and $MANPATH.\nhongwei@840-g5:~$ whereis psql psql: /usr/bin/psql /usr/share/man/man1/psql.1.gz hongwei@840-g5:~$ whereis postgresql postgresql: /usr/lib/postgresql /etc/postgresql /usr/share/postgresql  Links  What is the difference between locate/whereis/which\nDid you find this page helpful? Consider sharing it 🙌 ","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600819200,"objectID":"3c30a5c9381e05d0cf04ffbe9c5e540e","permalink":"/linux/whatis-locate-which-whereis/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/linux/whatis-locate-which-whereis/","section":"linux","summary":"A short summary of the commands 'Whatis', 'Locate', 'Which', 'Whereis'.","tags":["Linux","System"],"title":"Whatis Locate Which Whereis","type":"linux"},{"authors":["Hongwei Li"],"categories":["Linux","System"],"content":"This document preassumes that you are familiar with VirtualBox, Vagrant on Linux.\nPrerequisites  VirtualBox Vagrant  What to Build  3 Virtual Machines 3 MySQL instances running on the 3 VMs Employees Sample Database Data replication  IP Settings  Master - 192.168.0.201 Slave-1 - 192.168.0.202 Slave-2 - 192.168.0.203  Vagrantfiles   Vagrantfile-Master  Vagrantfile-Slave-1  Vagrantfile-Slave-2  Steps Boot up Virtual Machines  Use \u0026lsquo;vagrant up\u0026rsquo; to boot up 3 virtual machines. Use \u0026lsquo;vagrant halt\u0026rsquo; to power off the VMs Open VirtualBox GUI and change the MAC addresses of the 3 VMs\nMake sure the 3 VMs have different MAC addresses  Install MySQL Server Install MySQL server on the 3 VMs separately.\nThe guide How To Install MySQL on CentOS 8 can be referenced.\nsudo dnf install mysql-server sudo systemctl start mysqld.service sudo systemctl status mysqld sudo systemctl enable mysqld sudo mysql_secure_installation mysqladmin -u root -p version mysql -u root -p  Configure MySQL Server on Master VM  Edit MySQL configuration file \u0026lsquo;/etc/my.cnf\u0026rsquo;. Add the below configuration.   [mysqld]\nlog-bin=mysql-bin # Optinal\nserver-id=1　# Must\nbind-address = 0.0.0.0 # Must\n  Restart MySQL server service  System command:\nsudo systemctl restart mysqld   Create user for syncing data  MySQL commands:\nCREATE USER 'repl'@'192.168.0.%' IDENTIFIED BY '123456'; GRANT REPLICATION SLAVE ON *.* TO 'repl'@'192.168.0.%'; FLUSH PRIVILEGES;   Check master status  Configure MySQL Server on Slave VM  Edit MySQL configuration file \u0026lsquo;/etc/my.cnf\u0026rsquo;. Add the below configuration.   [mysqld]\nserver-id=2\n  Restart MySQL server service  System command:\nsudo systemctl restart mysqld   Configure syncronization  MySQL command:\nchange master to master_host='192.168.0.201', master_user='repl', master_password='123456', master_log_file='mysql-bin.000001', master_log_pos=155; start slave;   Check slave status  Both Slave_IO_Running and Slave_SQL_Running should be \u0026lsquo;Yes\u0026rsquo;.\n Same configuration and steps on Slave-2 VM, except the server-id=3.   Import the \u0026lsquo;Employees Sample Database\u0026rsquo;  Sync folder   Note that when you vagrant ssh into your machine, you\u0026rsquo;re in /home/vagrant. /home/vagrant is a different directory from the synced /vagrant directory.    By default, Vagrant only syncs the folders on vagrant up or vagrant reload.\nThe tool rsync can be used for data syncronization.    Download the \u0026lsquo;Employees Sample Database\u0026rsquo;  Download the zip file from Employees DB on GitHub, unzip the file and put the unzipped folder into the master VM folder of the host machine.\n Reload the master VM  Here we use the simple way, just reload the master VM.\n Import the database  On master VM:\ncd test_db-master/ mysql -u root -p -t \u0026lt; employees.sql   Validating the Employee Data  time mysql -u root -p -t \u0026lt; test_employees_sha.sql\n Check data on slaves  Now you can see all the data has been replicated on the slave servers.\nErrors Slave_IO_Running: No Possible reasons:\n Forget to restart mysqld service Forget to bind address.  Solution:\n Restart mysqld service Use netstat to check the status, i.e.  Links  Synced Folders\nHow To Install MySQL on CentOS 8\nEmployees Sample Database\nMySQL Cluster COnfiguration\nHow To Import SQL File Did you find this page helpful? Consider sharing it 🙌 ","date":1596585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596672000,"objectID":"a7b339d2cfa9706a95dadf7d9038d2cd","permalink":"/linux/set-up-mysql-master-slave-cluster/","publishdate":"2020-08-05T00:00:00Z","relpermalink":"/linux/set-up-mysql-master-slave-cluster/","section":"linux","summary":"This guide covers how to set up MySQL cluster by VirtualBox and Vagrant. This can be helpful for development and test.","tags":["MySQL","Linux","VirtualBox","Vagrant","CentOS"],"title":"Set Up MySQL Cluster","type":"linux"},{"authors":["Hongwei Li"],"categories":["Basic","Web Development"],"content":"The website has been up and running for a while. Today I want to update a document on another machine. The steps below describe how to set up the environment on the machine.\nSteps  Install Hugo Extended version Gitclone the repository Delete the \u0026lsquo;public\u0026rsquo; folder Update the submodule Checkout the branch \u0026lsquo;master\u0026rsquo; Work on the document Add, Commit and Push as before  Step 1: Install Hugo Extended version wget https://github.com/gohugoio/hugo/releases/download/v0.74.3/hugo_extended_0.74.3_Linux-64bit.deb sudo apt install ./hugo_extended_0.74.3_Linux-64bit.deb  Step 2: Gitclone the repository git clone https://github.com/flycoolman/academic-kickstart.git  Step 3: Delete the \u0026lsquo;public\u0026rsquo; folder cd academic-kickstart/ rm -rf public  Step 4: Update the submodule git submodule update --init --recursive  Step 5: Checkout the branch \u0026lsquo;master\u0026rsquo; cd public git checkout master  Step 6: Work on the document blahblahblah...  Step 7: Build the website cd academic-kickstart/ hugo  Step 8: Add, Commit and Push as before cd public git add . git commit -m \u0026quot;blahblah...\u0026quot; git push cd .. git add . git commit -m \u0026quot;blahblah...\u0026quot; git push  ALL SET!\nErrors and Tricks Error 1: Unable to locate template for shortcode  hongwei@HP840G1:~/Documents/academic-kickstart$ hugo server\nBuilding sites … ERROR 2020/08/04 07:51:50 Unable to locate template for shortcode \u0026ldquo;fragment\u0026rdquo; in page \u0026ldquo;slides/example/index.md\u0026rdquo;\nERROR 2020/08/04 07:51:50 Unable to locate template for shortcode \u0026ldquo;alert\u0026rdquo; in page \u0026ldquo;talk/example/index.md\u0026rdquo;\nERROR 2020/08/04 07:51:50 Unable to locate template for shortcode \u0026ldquo;alert\u0026rdquo; in page \u0026ldquo;publication/preprint/index.md\u0026rdquo;\nERROR 2020/08/04 07:51:50 Unable to locate template for shortcode \u0026ldquo;alert\u0026rdquo; in page \u0026ldquo;post/build-a-free-website-in-minutes-part-1/index.md\u0026rdquo;\n Solution\nInstall Hugo Extended version.\nError 2: failed to extract shortcode  hongwei@HP840G1:~/Documents/academic-kickstart$ hugo server\nBuilt in 11 ms\nError: Error building site: \u0026ldquo;/home/hongwei/Documents/academic-kickstart/content/home/demo.md:61:1\u0026rdquo;: failed to extract shortcode: template for shortcode \u0026ldquo;alert\u0026rdquo; not found\n Solution\nIf the Hugo Extended version has been installed, update the git submodule, see Step 4 above.\nError 3: \u0026lsquo;public\u0026rsquo; already exists  hongwei@HP840G1:~/Documents/academic-kickstart$ git submodule update \u0026ndash;init \u0026ndash;recursive\nSubmodule \u0026lsquo;public\u0026rsquo; (https://github.com/flycoolman/flycoolman.github.io.git) registered for path \u0026lsquo;public\u0026rsquo;\nSubmodule \u0026lsquo;themes/academic\u0026rsquo; (https://github.com/gcushen/hugo-academic.git) registered for path \u0026lsquo;themes/academic\u0026rsquo;\nfatal: destination path \u0026lsquo;/home/hongwei/Documents/academic-kickstart/public\u0026rsquo; already exists and is not an empty directory.\nfatal: clone of \u0026lsquo;https://github.com/flycoolman/flycoolman.github.io.git' into submodule path \u0026lsquo;/home/hongwei/Documents/academic-kickstart/public\u0026rsquo; failed\n Solution\nDelete the \u0026lsquo;public\u0026rsquo; folder, and update the git submodule again.\nError 4: forgot to check out before working on document  hongwei@HP840G1:~/Documents/academic-kickstart/public$ git status\nHEAD detached from f82be3e\nnothing to commit, working tree clean\n  hongwei@HP840G1:~/Documents/academic-kickstart/public$ git push\nfatal: You are not currently on a branch.\nTo push the history leading to the current (detached HEAD)\nstate now, use\ngit push origin HEAD:\n  hongwei@HP840G1:~/Documents/academic-kickstart/public$ git branch\n (HEAD detached from f82be3e)\nmaster    hongwei@HP840G1:~/Documents/academic-kickstart/public$ git checkout master\nWarning: you are leaving 1 commit behind, not connected to\nany of your branches:\n1090675 revise Upgrade to VirtualBox 6.1 and Vagrant 2.9.9 on Ubuntu 18.04\nIf you want to keep it by creating a new branch, this may be a good time\nto do so with:\ngit branch  1090675\nSwitched to branch \u0026lsquo;master\u0026rsquo;\nYour branch is up to date with \u0026lsquo;origin/master\u0026rsquo;.\n Solution\ngit branch temp 1090675 git push --set-upstream origin temp git checkout temp git push  Merge the branch temp to master\ngit checkout master git pull git branch --merged git branch -vv git branch -d temp  Did you find this page helpful? Consider sharing it 🙌 ","date":1596499200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598054400,"objectID":"1051b3fdf48a6997b9263b7d257e9969","permalink":"/post/build-a-free-website-in-minutes-part-5/","publishdate":"2020-08-04T00:00:00Z","relpermalink":"/post/build-a-free-website-in-minutes-part-5/","section":"post","summary":"Learn how to create the environment on another machine.","tags":["Hugo","Web Development","Static Website","Github Pages","Academic"],"title":"Build A Free Website In Minutes - Part 5","type":"post"},{"authors":["Hongwei Li"],"categories":["Java","Shardingsphere","Database"],"content":"Set Up Shardingsphere Development Environment This document used the official release version to set up and verify development environment. This way could help to rule out any unstable issues of source code and to focus the issues on environment.\nPrerequisites  Linux (Ubuntu 18.04) Source code 4.1.1 Eclipse IntelliJ IDEA   Choose the proper IDE (Eclipse or IntelliJ IDEA), even No IDE   Java Development Environment (No IDE) Install JDK 8 sudo apt install openjdk-8-jdk $ java -version openjdk version \u0026quot;1.8.0_252\u0026quot; OpenJDK Runtime Environment (build 1.8.0_252-8u252-b09-1~18.04-b09) OpenJDK 64-Bit Server VM (build 25.252-b09, mixed mode)  Install Maven (Optional) sudo apt install maven $ mvn -version Apache Maven 3.6.0 Maven home: /usr/share/maven Java version: 1.8.0_252, vendor: Private Build, runtime: /usr/lib/jvm/java-8-openjdk-amd64/jre Default locale: en_US, platform encoding: UTF-8 OS name: \u0026quot;linux\u0026quot;, version: \u0026quot;5.4.0-42-generic\u0026quot;, arch: \u0026quot;amd64\u0026quot;, family: \u0026quot;unix\u0026quot;  Unzip Source Code   Download Source Code\n  Unzip the source code\n unzip apache-shardingsphere-4.1.1-src.zip    Change file permissions\n chmod -R 755 apache-shardingsphere-4.1.1-src-release/    Build and Test Based the Github page Build Apache ShardingSphere, there is a script to do the build\n./mvnw clean install -Prelease   Make sure all the tests pass   Issues and Tricks  Lombok in the project not support Java 11\nIf Java 11 is set as default, this may cause the error below for Lombok does not support Java 11.   Solution\nUse Java 8.\n Multiple Java version installed\nIf multiple versions of Java are installed on the system, such as Java 11 and Java 8, in the mean time, Java 11 is set as default, the above error will cause the build failure. Even the default JDK is changed to Java 8 by update-alternatives \u0026ndash;config java, the above issue may still be there.  Solution\nUninstall all the JDKs by the command\nsudo apt-get remove openjdk*  and reinstall JDK 8.\nEclipse Import the Project Follow the steps below to import the project.\nImport projects\u0026hellip; or File \u0026mdash;\u0026gt; Import\u0026hellip;  The import is done by m2e plugin.\nThe warning shown below can be ignored.\nmaven-remote-resources-plugin (goal \u0026ldquo;process\u0026rdquo;) is ignored by m2e.   Build and Test Build and test as separate steps, i.e.\n Run As \u0026mdash;\u0026gt; Maven clean Run As \u0026mdash;\u0026gt; Maven build Run As \u0026mdash;\u0026gt; Maven test Run As \u0026mdash;\u0026gt; Maven install  Or define the goals at one time, i.e. Run As \u0026mdash;\u0026gt; Maven build\u0026hellip; \u0026mdash;\u0026gt; Goals: (clean install)  Specific module can be chosen and do the same build.\nMaven will build the dependencies automatically.   Issues and Tricks  Too many files with unapproved license\nWhen doing \u0026lsquo;install\u0026rsquo;, the below error occurs. No issue with \u0026lsquo;build\u0026rsquo; and \u0026lsquo;test\u0026rsquo;, but with \u0026lsquo;install\u0026rsquo;   [INFO] BUILD FAILURE Too many files with unapproved license\n Solution\nUse or check out clean source code, then do \u0026lsquo;install\u0026rsquo;.\n Build failed with 8 threads\nWhen setting 8 threads for build, the build failed.  Solution\nSet build threads as 1.\nIntelliJ IDEA Import the Project Follow the steps below to import the project.\nOn the Welcome window, select Import Project, then select the project POM file in the next window.  The errors and warnings of the POM file analysis can be ignored.   Build and Test As there are errors in the POM file, the tool window of Maven lifecyle may not show up automatically.\nTo add Maven support, right click on the project name in the project view, then select Add Framework Support\u0026hellip;.\nIn the Maven window, different phase of lifecyle of project or module can be chosen to build.\n Specific module can be chosen and do the build.\nMaven will build the dependencies automatically.   Links  Maven - Build Life Cycle\nIntroduction to the Build Lifecycle\nDid you find this page helpful? Consider sharing it 🙌 ","date":1596326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599350400,"objectID":"6935f293305b4d3f67eb45f653166f3e","permalink":"/coding/java/shardingsphere-dev-env-setup/","publishdate":"2020-08-02T00:00:00Z","relpermalink":"/coding/java/shardingsphere-dev-env-setup/","section":"coding","summary":"This documents wrote down the experience of setting up development environment for Shardingsphere.","tags":["Java","Shardingsphere","Database"],"title":"Set Up Shardingsphere Development Environment","type":"coding"},{"authors":["Hongwei Li"],"categories":["Java","Spring"],"content":"Inversion of Control (IoC) A real system might have dozens of services and components. To make a loosely coupled application, the way is to plug in the plugins (components and services) at some point. So the core problem is how to assemble the plugins into an application. Then frameworks aim to resolve the problem. Usually Inversion of Control is used in frameworks, so does Spring Framework. That\u0026rsquo;s why Inversion of Control (IoC) is the core technology of Spring Framework.\nInversion of Control (IoC) vs. Traditional Control   Traditional Control\nIn traditional programming, the custom code that expresses the purpose of the program calls into reusable libraries to take care of generic tasks.\nI.e.\nthe custom object instantiates its dependent objects, then uses the objects.\n  Inversion of Control (IoC)\nIoC inverts the flow of control as compared to traditional control flow. In IoC, custom-written portions of a computer program receive the flow of control from a generic framework. Usually it is the framework that calls into the custom, or task-specific, code.\nI.e.\nthe custom object receives the instantiated dependent objects from framework.\n  What Can IoC Serve IoC as a design guideline, is used to increase modularity of the program and make it extensible. It serves the following purposes:\n To decouple the execution of a task from implementation. To make every module focus on what it is designed for. To free modules from assumptions about how and what other systems do, and instead rely on contracts. To prevent side effects on other modules when replacing a module.  Spring Implementation of IoC Principle IoC is also known as dependency injection (DI). It is a process whereby objects define their dependencies (that is, the other objects they work with) only through constructor arguments, arguments to a factory method, or properties that are set on the object instance after it is constructed or returned from a factory method. The container then injects those dependencies when it creates the bean. This process is fundamentally the inverse (hence the name, Inversion of Control) of the bean itself controlling the instantiation or location of its dependencies by using direct construction of classes or a mechanism such as the Service Locator pattern.\nThis is common characteristic of frameworks, IoC manages java objects:\n from instantiation to destruction through its BeanFactory. Java components that are instantiated by the IoC container are called beans, and the IoC container manages a bean\u0026rsquo;s scope, lifecycle events, and any AOP features for which it has been configured and coded.   In Spring framework, the IoC Container does that job of injecting dependancies (DI) and not us, The flow of control is reversed, (Framework to Application) it is IoC with DI.   Spring IoC Container Central to the Spring Framework is its inversion of control (IoC) container, which provides a consistent means of configuring and managing Java objects using reflection. The container is responsible for managing object lifecycles of specific objects: creating these objects, calling their initialization methods, and configuring these objects by wiring them together.\nThe interface org.springframework.context.ApplicationContext represents the Spring IoC container and is responsible for instantiating, configuring, and assembling the aforementioned beans. The container gets its instructions on what objects to instantiate, configure, and assemble by reading configuration metadata.\n  Representation - org.springframework.context.ApplicationContext Responsibilities - instantiating, configuring, and assembling Beans Tool: configuration metadata    Types of IoC Containers The org.springframework.beans and org.springframework.context packages provide the basis for the Spring Framework’s IoC container.\nThe BeanFactory interface provides an advanced configuration mechanism capable of managing objects of any nature.\nThe ApplicationContext interface builds on top of the BeanFactory (it is a sub-interface) and adds other functionality such as easier integration with Spring’s AOP features, message resource handling (for use in internationalization), event propagation, and application-layer specific contexts such as the WebApplicationContext for use in web applications.\n  BeanFactory container\n The BeanFactory is the actual representation of the Spring IoC container that is responsible for containing and otherwise managing the aforementioned beans. The BeanFactory interface is the central IoC container interface in Spring. The BeanFactory API provides the underlying basis for Spring’s IoC functionality. Its specific contracts are mostly used in integration with other parts of Spring and related third-party frameworks.    ApplicationContext container\nBecause an ApplicationContext includes all the functionality of a BeanFactory, it is generally recommended over a plain BeanFactory, except for scenarios where full control over bean processing is needed.\n    In short, the BeanFactory provides the configuration framework and basic functionality, and the ApplicationContext adds more enterprise-specific functionality. The ApplicationContext is a complete superset of the BeanFactory. You should use an ApplicationContext unless you have a good reason for not doing so, with GenericApplicationContext and its subclass AnnotationConfigApplicationContext as the common implementations for custom bootstrapping.    Dependency Injection (DI) Inversion of Control is too generic a term, and thus people find it confusing. As a result with a lot of discussion with various IoC advocates, the name was settled on Dependency Injection.\nDependency injection generally means passing a dependent object as a parameter to a method, rather than having the method create the dependent object. What it means in practice is that the method does not have a direct dependency on a particular implementation; any implementation that meets the requirements can be passed as a parameter.\n The Spring IoC Container is the leading dependency injection framework.   Dependency Lookup vs. Dependency Injection Objects can be obtained by means of either dependency lookup or dependency injection.\n Dependency lookup is a pattern where a caller asks the container object for an object with a specific name or of a specific type. Dependency injection is a pattern where the container passes objects by name to other objects, via either constructors, properties, or factory methods.  The Styles of DI Dependency Injection can be done by:\n  Constructor-based dependency injection\nConstructor-based DI is accomplished by the container invoking a constructor with a number of arguments, each representing a dependency.\n public class SimpleMovieLister { // the SimpleMovieLister has a dependency on a MovieFinder private MovieFinder movieFinder; // a constructor so that the Spring container can inject a MovieFinder public SimpleMovieLister(MovieFinder movieFinder) { this.movieFinder = movieFinder; } // business logic that actually uses the injected MovieFinder is omitted... }    Setter-based dependency injection\nSetter-based DI is accomplished by the container calling setter methods on your beans after invoking a no-argument constructor or a no-argument static factory method to instantiate your bean.\n public class SimpleMovieLister { // the SimpleMovieLister has a dependency on the MovieFinder private MovieFinder movieFinder; // a setter method so that the Spring container can inject a MovieFinder public void setMovieFinder(MovieFinder movieFinder) { this.movieFinder = movieFinder; } // business logic that actually uses the injected MovieFinder is omitted... }    Constructor-based or Setter-based DI  Constructor-based and setter-based DI can be mixed Constructors for mandatory dependencies and setter methods or configuration methods for optional dependencies  Why\n Constructor injection lets you implement application components as immutable objects and ensures that required dependencies are not null. Furthermore, constructor-injected components are always returned to the client (calling) code in a fully initialized state. Setter injection should primarily only be used for optional dependencies that can be assigned reasonable default values within the class. Otherwise, not-null checks must be performed everywhere the code uses the dependency. One benefit of setter injection is that setter methods make objects of that class amenable to reconfiguration or re-injection later.   Use the DI style that makes the most sense for a particular class.   IoC vs. DI  Interchangable\nIoC and DI are used interchangeably. Process and Result\nIoC is achieved through DI. DI is the process of providing the dependencies and IoC is the end result of DI. One to Many\nDI is a specific type of IoC and is not the only way to achieve IoC. There are other ways as well, such as:  Service Locator pattern Template method design pattern Strategy design pattern      IoC basically facilitates having different components designed and coded separately and later used together by defining their relation with DI. By DI, the responsibility of creating objects is shifted from our application code to the Spring container; this phenomenon is called IoC.    Links  The IoC Container\nInversion of Control Containers and the Dependency Injection pattern\nSpring Framework\nWhat is Dependency Injection and Inversion of Control in Spring Framework?\nSpring – Inversion of Control vs Dependency Injection\nSpring – IoC Containers\nInversion of control\nWhat is Dependency Injection and Inversion of Control in Spring Framework?\nInterface BeanFactory\nInterface ApplicationContext\nThe BeanFactory\nDid you find this page helpful? Consider sharing it 🙌 ","date":1596240000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598745600,"objectID":"d5cf26ea5db4620f0e302da8f272b7c8","permalink":"/coding/java/ioc-di/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/coding/java/ioc-di/","section":"coding","summary":"A summary of Spring Inversion of Control and Dependency Injection.","tags":["Java","Spring","IoC","DI"],"title":"Inversion of Control and Dependency Injection","type":"coding"},{"authors":["Hongwei Li"],"categories":["Python","Linux"],"content":"Prerequisites  Linux host machine (i.e. Ubuntu 18.04) Vagrant (i.e. 2.9.9) VirtualBox (i.e. 6.1.12)   The version numbers are just for my environment.\nFind the original log of my setup here.   Overview The steps are:\n Use Vagrant to boot up a VM Install GUI on the VM Install Docker on the VM Install SVN Check out the branch Set up database Set up virtual environment Run the tests Start the Development Server Mailserver and Rsync Data  Step 1: Use Vagrant to boot up a VM Put the below Vagrantfile into working directory and execute \u0026lsquo;Vagrant up\u0026rsquo;. Vagrantfile\nStep 2: Install GUI on the VM Ssh to the VM,\nvagrant ssh  then use the below commands to intall GUI on the VM.\nsudo yum -y groupinstall \u0026quot;GNOME Desktop\u0026quot; echo \u0026quot;exec gnome-session\u0026quot; \u0026gt;\u0026gt; ~/.xinitrc systemctl set-default graphical.target  Use command\nstartx  in VirtualBox console to start the GUI.\nStep 3: Install Docker on the VM Install docker and start the service.\nsudo yum -y install docker sudo systemctl status docker sudo systemctl start docker sudo systemctl enable docker  Create user group \u0026lsquo;docker\u0026rsquo; and add you into the group.\nsudo groupadd docker sudo usermod -aG docker $(whoami) sudo usermod -aG docker vagrant  Reevaluate the group and restart the docker service.\nlogout sudo systemctl restart docker  Check if you can run docker commands without sudo.\ndocker info  Step 4: Install SVN Add the repository and install SVN.\nsudo vim /etc/yum.repos.d/wandisco-svn.repo   [WandiscoSVN] name=Wandisco SVN Repo baseurl=http://opensource.wandisco.com/centos/$releasever/svn-1.8/RPMS/$basearch/ enabled=1 gpgcheck=0\n sudo yum remove subversion* sudo yum clean all sudo yum install subversion svn --version  Step 5: Check out the branch UTF-8 settings:\nexport LC_ALL=C sudo vi .bashrc  Add below to the file ~/.bashrc\n LANG=en_US.UTF-8 export LANG\n Change SELinux settings, so that docker image can access your home directory.\nchcon -Rt svirt_sandbox_file_t /home/vagrant/  Checkout the branch\nmkdir -p ietf cd ietf svn co https://svn.tools.ietf.org/svn/tools/ietfdb/personal/flycoolman/7.10.1.dev0  Step 6: Set up database cd 7.10.1.dev0/ ./docker/setupdb  Step 7: Set up virtual environment ./docker/run   rsyslog error can be ignored!\n[FAIL] rsyslogd is not running \u0026hellip; failed!   In virtual environment of the container\npip install --upgrade -r requirements.txt ./ietf/manage.py migrate   The below operation might be needed if the migration fails.\nsudo cp docker/settings_local.py ietf/  Then run the migrate command again.\n  Step 8: Run the tests In the virtual environment to run the tests:\n./ietf/manage.py test \u0026ndash;settings=settings_sqlitetest\n Make sure that one of the following commands runs to completion without errors.   Step 9: Start the Development Server ./ietf/manage.py runserver 0.0.0.0:8000 \u0026amp;  Test the access to datatracker.\nStep 10: Mailserver and Rsync Data Go to the original page for details about:\n  (Optional) Run the mailserver  Manually Edit or rsync Datatracker Data Directories  Setup Complete For other workflow things, please refer to the original setup guide.\nLinks  How To Install and Use Docker on CentOS 7 How to fix docker: Got permission denied while trying to connect to the Docker daemon socket How to Install Subversion (SVN) 1.8.19 on CentOS/RHEL 7/6/5 Sprint Coder Setup\nHow to install a GUI on top of CentOS 7\nSprintCoderSetupTroubleshooting\nUpgrade to VirtualBox 6.1 and Vagrant 2.9.9 on Ubuntu 18.04\nDid you find this page helpful? Consider sharing it 🙌 ","date":1595808000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595889600,"objectID":"7bee0e7f57f79baabed0c8f4a56d989b","permalink":"/coding/python/ietf-datatracker-environment-setup/","publishdate":"2020-07-27T00:00:00Z","relpermalink":"/coding/python/ietf-datatracker-environment-setup/","section":"coding","summary":"This guide describes the steps of setting up codesprints development environment for IETF Datatracker.","tags":["Python","Django","IETF"],"title":"IETF Datatracker Environment Setup","type":"coding"},{"authors":["Hongwei Li"],"categories":["Java","Spring"],"content":"Spring Beans  The objects that are managed by Spring IoC container   The objects that form the backbone of your application and that are managed by the Spring IoC container are called beans. A bean is an object that is instantiated, assembled, and otherwise managed by a Spring IoC container. These beans are created with the configuration metadata that you supply to the container, for example, in the form of XML  definitions.\nIn Spring, the objects that form the backbone of your application and that are managed by the Spring IoC container are called beans. A bean is an object that is instantiated, assembled, and otherwise managed by a Spring IoC container. Otherwise, a bean is simply one of many objects in your application. Beans, and the dependencies among them, are reflected in the configuration metadata used by a container.\nSpring Bean is nothing special, any object in the Spring framework that we initialize through Spring container is called Spring Bean. Any normal Java POJO class can be a Spring Bean if it’s configured to be initialized via container by providing configuration metadata information.\nSpring IoC Container   Representation - org.springframework.context.ApplicationContext Responsibilities - instantiating, configuring, and assembling Beans Tool: configuration metadata    The interface org.springframework.context.ApplicationContext represents the Spring IoC container and is responsible for instantiating, configuring, and assembling the aforementioned beans. The container gets its instructions on what objects to instantiate, configure, and assemble by reading configuration metadata.\nConfiguration Metadata  This configuration metadata represents how you as an application developer tell the Spring container to instantiate, configure, and assemble the objects in your application. The configuration metadata is represented in XML, Java annotations, or Java code. It allows you to express the objects that compose your application and the rich interdependencies between such objects. Spring configuration consists of at least one and typically more than one bean definition that the container must manage. Consumed by Spring IoC container  Spring Bean Scopes 7 scopes (Spring 4.2.x) are supported out of the box. You can also create a custom scope.\n Singleton – Only one instance of the bean will be created for each container. This is the default scope for the spring beans. While using this scope, make sure bean doesn’t have shared instance variables otherwise it might lead to data inconsistency issues. Prototype – A new instance will be created every time the bean is requested. Request – This is same as prototype scope, however it’s meant to be used for web applications. A new instance of the bean will be created for each HTTP request. Session – A new bean will be created for each HTTP session by the container. Global-session – This is used to create global session beans for Portlet applications. Application - Scopes a single bean definition to the lifecycle of a ServletContext. Only valid in the context of a web-aware Spring ApplicationContext. Websocket - Scopes a single bean definition to the lifecycle of a WebSocket. Only valid in the context of a web-aware Spring ApplicationContext.   As of Spring 3.0, a thread scope is available, but is not registered by default.     As a rule, use the prototype scope for all stateful beans and the singleton scope for stateless beans. The client code must clean up prototype-scoped objects and release expensive resources that the prototype bean(s) are holding.    5 of the above are available only if you use a web-aware ApplicationContext\n request session globalSession application websocket  Spring Singleton vs Singleton Pattern Spring’s concept of a singleton bean differs from the Singleton pattern as defined in the Gang of Four (GoF) patterns book. The GoF Singleton hard-codes the scope of an object such that one and only one instance of a particular class is created per ClassLoader.\nThe scope of the Spring singleton is best described as per container and per bean. This means that if you define one bean for a particular class in a single Spring container, then the Spring container creates one and only one instance of the class defined by that bean definition.\nSpring Bean Lifecycle Lifecycle Overview The lifecycle of any object means when \u0026amp; how it is born, how it behaves throughout its life, and when \u0026amp; how it dies. The lifecycle of Spring Beans is not different from normal beans or objects. The below picture shows the overview of the lifecycle of objects, or Spring Beans.\nAs the Spring Beans are managed by Spring IoC containers. This makes the application developers not able to control the full lifecycle of Spring Beans. But Spring provides ways to add customizations into the lifecycle of Spring Beans, such as aware interfaces and callback methods, which are added in the below picture to show the lifecycle. Aware Interfaces   BeanNameAware\nThe BeanNameAware interface is implemented by beans that need access to its name defined in the Spring container.\n import org.springframework.beans.factory.BeanNameAware; public class BeanNameAwareImpl implements BeanNameAware { @Override public void setBeanName(String s) { System.out.println(\u0026quot;Bean Name: \u0026quot; + s); } }    BeanFactoryAware\nBeans might need access to the bean factory that created it, say to call any service from the bean factory.\n import org.springframework.beans.BeansException; import org.springframework.beans.factory.BeanFactory; import org.springframework.beans.factory.BeanFactoryAware; public class BeanFactoryAwareImpl implements BeanFactoryAware { @Override public void setBeanFactory(BeanFactory beanFactory) throws BeansException { System.out.println(beanFactory.getBean(\u0026quot;fooBean\u0026quot;)); } }    ApplicationContextAware\nIn Spring beans, you might require access to the ApplicationContext. For example, if your bean needs to look up some other beans. Similarly, if your bean needs access to some application file resource in your bean or even publish some application events, you need access to the ApplicationContext.\n import org.springframework.beans.BeansException; import org.springframework.context.ApplicationContext; import org.springframework.context.ApplicationContextAware; public class ApplicationContextAwareImpl implements ApplicationContextAware { @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { FooBean userBean = (FooBean) applicationContext.getBean(\u0026quot;fooBean\u0026quot;); System.out.println(\u0026quot;User Id: \u0026quot; + fooBean.getFooBeanId() + \u0026quot; Bean Name :\u0026quot; + fooBean.getName()); } }    For more aware interfaces and details, go to the link Interface Aware.\nLifecycle Callbacks Callback Methods  InitializingBean and DisposableBean callback interfaces are not recommended.    Initialization callbacks\n3 options:  Annotation-based\n@PostConstruct annotation or specify a POJO initialization method XML-based\ninit-method attribute to specify the name of the method that has a void no-argument signature Java Config\ninitMethod attribute of @Bean     Destruction callbacks\n3 options:  Annotation-based\n@PreDestroy annotation or specify a generic method that is supported by bean definitions XML-Based\ndestroy-method attribute on the \u0026lt;bean/\u0026gt; Java Config\ndestroyMethod attribute of @Bean     Startup and shutdown callbacks\nThe Lifecycle interface defines the essential methods for any object that has its own lifecycle requirements (e.g. starts and stops some background process)  Callback Examples   XML-based: Custom init() and destroy() methods\nThe default init and destroy methods in bean configuration file can be defined in two ways:\n Bean local definition applicable to a single bean Global definition applicable to all beans defined in beans context  beans.xml\nLocal definition\n \u0026lt;beans\u0026gt; \u0026lt;bean id=\u0026quot;demoBean\u0026quot; class=\u0026quot;com.flycoolman.callbackdemo.DemoBean\u0026quot; init-method=\u0026quot;customInit\u0026quot; destroy-method=\u0026quot;customDestroy\u0026quot;\u0026gt;\u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt;  Global definition\n \u0026lt;beans default-init-method=\u0026quot;customInit\u0026quot; default-destroy-method=\u0026quot;customDestroy\u0026quot;\u0026gt; \u0026lt;bean id=\u0026quot;demoBean\u0026quot; class=\u0026quot;com.flycoolman.callbackdemo.DemoBean\u0026quot;\u0026gt;\u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt;  DemoBean.java\n package com.flycoolman.callbackdemo; public class DemoBean { public void customInit() { System.out.println(\u0026quot;Method customInit() invoked...\u0026quot;); } public void customDestroy() { System.out.println(\u0026quot;Method customDestroy() invoked...\u0026quot;); } }    Annotation-based: @PostConstruct and @PreDestroy\n  @PostConstruct annotated method will be invoked after the bean has been constructed using default constructor and just before it’s instance is returned to requesting object.\n  @PreDestroy annotated method is called just before the bean is about be destroyed inside bean container.\n package com.flycoolman.callbackdemo; import javax.annotation.PostConstruct; import javax.annotation.PreDestroy; public class DemoBean { @PostConstruct public void customInit() { System.out.println(\u0026quot;Method customInit() invoked...\u0026quot;); } @PreDestroy public void customDestroy() { System.out.println(\u0026quot;Method customDestroy() invoked...\u0026quot;); } }      Java Config\n public class Foo { public void init() { // initialization logic } public void cleanup() { // destruction logic } } @Configuration public class AppConfig { @Bean(initMethod = \u0026quot;init\u0026quot;, destroyMethod = \u0026quot;cleanup\u0026quot;) public Foo foo() { return new Foo(); }    Multiple Lifecycle Mechanisms/Execution Orders Multiple lifecycle mechanisms configured for the same bean, with different initialization methods, are called as follows:\n Methods annotated with @PostConstruct afterPropertiesSet() as defined by the InitializingBean callback interface A custom configured init() method  Destroy methods are called in the same order:\n Methods annotated with @PreDestroy destroy() as defined by the DisposableBean callback interface A custom configured destroy() method  What is Callback From Wikipedia\nIn computer programming, a callback, also known as a \u0026ldquo;call-after\u0026rdquo; function, is any executable code that is passed as an argument to other code; that other code is expected to call back (execute) the argument at a given time. This execution may be immediate as in a synchronous callback, or it might happen at a later time as in an asynchronous callback. Programming languages support callbacks in different ways, often implementing them with\n subroutines, lambda expressions, blocks, or function pointers.  For Java\nIn object-oriented programming languages without function-valued arguments, such as in Java before its 1.7 version, callbacks can be simulated by passing an instance of an abstract class or interface, of which the receiver will call one or more methods, while the calling end provides a concrete implementation. Something like below (from Stackoverflow)\npublic class Test { public static void main(String[] args) throws Exception { new Test().doWork(new Callback() { // implementing class @Override public void call() { System.out.println(\u0026quot;callback called\u0026quot;); } }); } public void doWork(Callback callback) { System.out.println(\u0026quot;doing work\u0026quot;); callback.call(); } public interface Callback { void call(); } }  Spring Bean Instantiation   Instantiation with a constructor\n \u0026lt;bean id=\u0026quot;exampleBean\u0026quot;/\u0026gt;    Instantiation with a static factory method\n \u0026lt;bean id=\u0026quot;exampleBean\u0026quot; factory-method=\u0026quot;createInstance\u0026quot;/\u0026gt;    Instantiation using an instance factory method\n \u0026lt;bean id=\u0026quot;myFactoryBean\u0026quot; class=\u0026quot;...\u0026quot;\u0026gt; \u0026lt;bean id=\u0026quot;exampleBean\u0026quot; factory-bean=\u0026quot;myFactoryBean\u0026quot; factory-method=\u0026quot;createInstance\u0026quot;\u0026gt;\u0026lt;/bean\u0026gt;    For more details, go to the link Instantiating beans.\nSpring Bean Configuration/Container Configuration XML-based XML-based configuration metadata shows these beans configured as  elements inside a top-level  element.\n  Bean Configuration with Property\n \u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;beans xmlns=\u0026quot;http://www.springframework.org/schema/beans\u0026quot; xmlns:xsi=\u0026quot;http://www.w3.org/2001/XMLSchema-instance\u0026quot; xsi:schemaLocation=\u0026quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\u0026quot;\u0026gt; \u0026lt;!-- services --\u0026gt; \u0026lt;bean id=\u0026quot;petStore\u0026quot; class=\u0026quot;org.springframework.samples.jpetstore.services.PetStoreServiceImpl\u0026quot;\u0026gt; \u0026lt;property name=\u0026quot;accountDao\u0026quot; ref=\u0026quot;accountDao\u0026quot;/\u0026gt; \u0026lt;property name=\u0026quot;itemDao\u0026quot; ref=\u0026quot;itemDao\u0026quot;/\u0026gt; \u0026lt;!-- additional collaborators and configuration for this bean go here --\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;!-- more bean definitions for services go here --\u0026gt; \u0026lt;/beans\u0026gt;    Multiple Bean Configuration\n \u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;beans xmlns=\u0026quot;http://www.springframework.org/schema/beans\u0026quot; xmlns:xsi=\u0026quot;http://www.w3.org/2001/XMLSchema-instance\u0026quot; xsi:schemaLocation=\u0026quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\u0026quot;\u0026gt; \u0026lt;bean id=\u0026quot;accountDao\u0026quot; class=\u0026quot;org.springframework.samples.jpetstore.dao.jpa.JpaAccountDao\u0026quot;\u0026gt; \u0026lt;!-- additional collaborators and configuration for this bean go here --\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026quot;itemDao\u0026quot; class=\u0026quot;org.springframework.samples.jpetstore.dao.jpa.JpaItemDao\u0026quot;\u0026gt; \u0026lt;!-- additional collaborators and configuration for this bean go here --\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;!-- more bean definitions for data access objects go here --\u0026gt; \u0026lt;/beans\u0026gt;    Multiple XML Files\n \u0026lt;beans\u0026gt; \u0026lt;import resource=\u0026quot;services.xml\u0026quot;/\u0026gt; \u0026lt;import resource=\u0026quot;resources/messageSource.xml\u0026quot;/\u0026gt; \u0026lt;import resource=\u0026quot;/resources/themeSource.xml\u0026quot;/\u0026gt; \u0026lt;bean id=\u0026quot;bean1\u0026quot; class=\u0026quot;...\u0026quot;/\u0026gt; \u0026lt;bean id=\u0026quot;bean2\u0026quot; class=\u0026quot;...\u0026quot;/\u0026gt; \u0026lt;/beans\u0026gt;    Annotation-based Spring 2.5 introduced support for annotation-based configuration metadata.\n Annotation injection is performed before XML injection, thus the latter configuration will override the former for properties wired through both approaches.     Implicit Registeration\n \u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;beans xmlns=\u0026quot;http://www.springframework.org/schema/beans\u0026quot; xmlns:xsi=\u0026quot;http://www.w3.org/2001/XMLSchema-instance\u0026quot; xmlns:context=\u0026quot;http://www.springframework.org/schema/context\u0026quot; xsi:schemaLocation=\u0026quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\u0026quot;\u0026gt; \u0026lt;context:annotation-config/\u0026gt; \u0026lt;/beans\u0026gt;     context:annotation-config/ only looks for annotations on beans in the same application context in which it is defined.\nThis means that, if you put context:annotation-config/ in a WebApplicationContext for a DispatcherServlet, it only checks for @Autowired beans in your controllers, and not your services.     Annotate the Method\n public class SimpleMovieLister { private MovieFinder movieFinder; @Autowired public void setMovieFinder(MovieFinder movieFinder) { this.movieFinder = movieFinder; } // ... }     @Autowired annotation can be applied to setter methods, constructors, methods with arbitrary names and/or multiple arguments, fields, etc.   For more details, go to the link Annotation-based container configuration\nJava-based (JavaConfig)   JavaConfig Overview\nStarting with Spring 3.0, many features provided by the Spring JavaConfig project became part of the core Spring Framework. Thus you can define beans external to your application classes by using Java rather than XML files. To use these new features, see the @Configuration, @Bean, @Import and @DependsOn annotations.\nJava configuration typically uses @Bean annotated methods within a @Configuration class.\nSpring JavaConfig is a product of the Spring community that provides a pure-Java approach to configuring the Spring IoC Container. While JavaConfig aims to be a feature-complete option for configuration, it can be (and often is) used in conjunction with the more well-known XML-based configuration approach.\n  @Bean Annotation\nThe @Bean annotation is used to indicate that a method instantiates, configures and initializes a new object to be managed by the Spring IoC container. For those familiar with Spring’s  XML configuration the @Bean annotation plays the same role as the  element. You can use @Bean annotated methods with any Spring @Component, however, they are most often used with @Configuration beans.\n  @Configuration Annotation\nAnnotating a class with @Configuration indicates that its primary purpose is as a source of bean definitions. Furthermore, @Configuration classes allow inter-bean dependencies to be defined by simply calling other @Bean methods in the same class. The simplest possible @Configuration class would read as follows:\n @Configuration public class AppConfig { @Bean public MyService myService() { return new MyServiceImpl(); } }    For more details, go to the link Java-based container configuration\nLinks  Spring Core Technologies - beans definition\n初识Spring —— Bean的装配（一）\n初识Spring —— Bean的装配（二）\nWhat in the world are Spring beans?\nSpring注入Bean的几种方式\nBeans, BeanFactory和ApplicationContext\nSpring IoC and Bean\nSpring Context and Bean Lifecycle callbacks: practical examples of usage\nwhat do you mean by callbacks?\nSpring Bean Lifecycle: Using Spring Aware Interfaces\nBean life cycle in Java Spring\nCallback (computer programming)\nSpring – Bean Life Cycle\nSpring Bean Lifecycle\nDid you find this page helpful? Consider sharing it 🙌 ","date":1595808000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598572800,"objectID":"53205723b49d19de04f4f98913288f9f","permalink":"/coding/java/spring-beans/","publishdate":"2020-07-27T00:00:00Z","relpermalink":"/coding/java/spring-beans/","section":"coding","summary":"A summary of Spring Beans.","tags":["Java","Spring"],"title":"Spring Beans","type":"coding"},{"authors":["Hongwei Li"],"categories":["Linux","System"],"content":"I needed to set up a development environment for this week\u0026rsquo;s IETF code sprint. As this was a very short term project, I would like to set up the environment in a VM. Instead of downloading Linux image, install it on VirtualBox or KVM, Vagrant is handy to use, particularly for development environment. I was thinking to get some new exeperience. Then CentOS 8 came up, as I have been using CentOS 7, but no CentOS 8 experience. And also I wanted the environment with GUI, so that I dould test my code on the VM directly.\nWhy to upgrade VirtualBox and Vagrant I had VirtualBox and Vagrant on my machine(Ubuntu 18.04). So it was straight forward to boot up CentOS 8 by changing the Vagrantfile.\n # -*- mode: ruby -*-\n# vi: set ft=ruby :\nVagrant.configure(\u0026ldquo;2\u0026rdquo;) do |config|\nconfig.vm.box = \u0026ldquo;centos/8\u0026rdquo;\nconfig.vm.box_check_update = false\nconfig.vm.hostname = \u0026ldquo;centos8\u0026rdquo;\nconfig.vm.provider \u0026ldquo;virtualbox\u0026rdquo; do |vb|\n# Display the VirtualBox GUI when booting the machine\nvb.gui = true\nvb.memory = \u0026ldquo;4096\u0026rdquo;\nvb.cpus = 4\nend\nend\n  Vagrantfile   After\n Vagrant up\n I could vagrant ssh to the VM and there was terminal on VirtualBox console. Everything worked well until the GUI was installed and started. I could not get into the GUI of CentOS.\nI guessed to upgrade the VirtualBox might resolve the issue, because I had VirtualBox 5.2. The latest version of VirtualBox is 6.1. So I started upgrading VirutalBox.\nUpgrade VirtualBox 6.1 To install the latest version of VirtualBox, the usual way is to download the .DEB binary package simply from VirtualBox downloads page\nwget https://download.virtualbox.org/virtualbox/6.1.12/virtualbox-6.1_6.1.12-139181~Ubuntu~bionic_amd64.deb  Install the package by dpkg\ndpkg -i virtualbox-6.1_6.1.12-139181~Ubuntu~bionic_amd64.deb sudo apt install -f  or apt\nsudo apt install ./virtualbox-6.1_6.1.12-139181~Ubuntu~bionic_amd64.deb  Upgrade Vagrant 2.9.9 To support VirtualBox 6.1, I also have to upgrade Vagrant to 2.9.9. Again download the DEB package from Vagrant release website and install with dpkg or apt.\ncurl -O https://releases.hashicorp.com/vagrant/2.2.9/vagrant_2.2.9_x86_64.deb sudo apt install ./vagrant_2.2.9_x86_64.deb  To verify that the installation was successful, run the following command which prints the Vagrant version:\nvagrant --version  Install Virtualbox-dkms When VirtualBox is upgraded, the old virtual machines might fail to open. Some error information like below:\n Failed to open a session for the virtual machine xxxxxxx.\nThe VM session was closed before any attempt to power it on.\nResult Code: NS_ERROR_FAILURE (0x80004005)\nComponent: SessionMachine\nInterface: ISession {7844aa05-b02e-4cdd-a04f-ade4a762e6b7}\n Even running the command \u0026lsquo;sudo /sbin/vboxconfig\u0026rsquo; can\u0026rsquo;t resolve the issue. the dkms package needs to be reinstalled.\ndpkg -l | grep virtualbox-dkms sudo apt-get purge virtualbox-dkms \u0026amp;\u0026amp; sudo apt-get install dkms sudo /sbin/vboxconfig  Install VirtualBox Extension Pack After the Vagrant and VirtualBox had been upgraded, unfortunately I still couldn\u0026rsquo;t make the CentOS 8 GUI working, even though I tried all the options of \u0026lsquo;Graphics Controller\u0026rsquo;. When I tried to connect the VM by RDP, I got an error message of no VirtualBox Extension Pack. Here is the step to install the VirtualBox Extension Pack.\nStep 1 Go to the virtualbox official website and download the package.\nStep 2 Double click the file, such as\n Oracle_VM_VirtualBox_Extension_Pack-6.1.12\n then follow the wizard to install.\nUnfortunately the RDP did not show me the desktop successfully either.\nInstall Vagrant Plugin vagrant-libvirt As I couldn\u0026rsquo;t make the CentOS 8 GUI working on VirtualBox with Vagrant, then I moved to Libvirt with Vagrant. But When I did \u0026lsquo;vagrant up\u0026rsquo;, got the below error.\nIt seemed that the libvirt plugin was not installed, because I installed Vagrant by DEB package, not apt. To install Vagrant Plugin vagrant-libvert, please follow the Vagrant Libvirt Provider on GitHub.\nStep 1 The packages for building needs to be installed.\napt-get build-dep vagrant ruby-libvirt apt-get install qemu libvirt-bin ebtables dnsmasq-base apt-get install libxslt-dev libxml2-dev libvirt-dev zlib1g-dev ruby-dev  Step 2 Install the plugin. vagrant plugin install vagrant-libvirt\nNon-exist Box Error I even tried some pre-built box with GUI, but the box was gone when I used the new version Vagrant.\nTerminal Mode  Bad news - I have not made the CentOS 8 GUI working on VM through Vagrant with either VirtualBox or Libvirt.   The pictures below show the successful terminal.\n The username and password for console are vagrant and vagrant.   Links  How to Install Vagrant on Ubuntu 18.04\nInstall GNOME | How to enable GUI mode | RHEL CentOS 8\nHow to install a GUI on top of CentOS 7\nRun CentOS 8 VM using Vagrant on KVM / VirtualBox / VMWare / Parallels\nDid you find this page helpful? Consider sharing it 🙌 ","date":1595462400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596499200,"objectID":"5c744afba73e60771366b4c5746fa7ff","permalink":"/linux/ubuntu-18.04-upgrade-virtualbox-vagrant/","publishdate":"2020-07-23T00:00:00Z","relpermalink":"/linux/ubuntu-18.04-upgrade-virtualbox-vagrant/","section":"linux","summary":"This guide covers how to install VirtualBox 6.1 and Vagrant 2.9.9.","tags":["Linux","VirtualBox","Vagrant","CentOS"],"title":"Upgrade to VirtualBox 6.1 and Vagrant 2.9.9 on Ubuntu 18.04","type":"linux"},{"authors":["Hongwei Li"],"categories":["Linux","System"],"content":"Symptom After the upgrade from Python 3.6 to Python 3.8:\n Ubuntu Software Center not working Can\u0026rsquo;t start terminal by Ctl+Alt+t Can\u0026rsquo;t start terminal or software Error Message - A problem occured when checking for the updates  Root cause After the Python 3 upgrade, I changed the Update-alternatives configuration. Fortunately I remembered it!\nSolution Start terminal in right click menu and change back to the original configuration of update-alternatives, as below:\nDid you find this page helpful? Consider sharing it 🙌 ","date":1595203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595289600,"objectID":"18250f7549931ecbbd7f97d18d19ea2b","permalink":"/linux/ubuntu-18.04-system-python-3-upgrade/","publishdate":"2020-07-20T00:00:00Z","relpermalink":"/linux/ubuntu-18.04-system-python-3-upgrade/","section":"linux","summary":"Change to update-alternatives broke Ubuntu system.","tags":["Linux","Update-alternatives","Python 3"],"title":"Ubuntu 18.04 System Python 3 Upgrade","type":"linux"},{"authors":["Hongwei Li"],"categories":["Java"],"content":"Java Beans Software Components\nJavaBeans are classes that encapsulate many objects into a single object. They are serializable, have a zero-argument constructor, and allow access to properties using getter and setter methods.\n  a Java classs serializable zero-argument constructor getter and setter    A bean is a Java class with method names that follow the JavaBeans guidelines. A bean builder tool uses introspection to examine the bean class. Based on this inspection, the bean builder tool can figure out the bean\u0026rsquo;s properties, methods, and events.\nAlmost any code can be packaged as a bean.\nThe power of JavaBeans is that you can use software components without having to write them or understand their implementation.\nJava Beans Example import java.io.Serializable; public class Car implements Serializable { //Private Properties private String color; private Boolean isCar; //Zero-argument Constructor public Car(){} //Getter and Setter public void setColor(String color) { this.color = color; } public String getColor() { return color; } public void setCar(Boolean car) { isCar = car; } //'is' for Boolean getter public Boolean isCar() { return isCar; } }  Bean Properties   Read and write property has getter and setter A read-only property has a getter method but no setter A write-only property has a setter method only Boolean property using is instead of get     Indexed Properties\nan array instead of a single value Bound Properties\nPropertyChangeListeners Constrained Properties\nVetoableChangeListeners  Bean Methods Any public method that is not part of a property definition is a bean method.\nBean Events  A bean class can fire off any type of event Method names with a specific pattern Can be used in wiring components together  BeanInfo A BeanInfo is a class that changes how your bean appears in a builder tool.\nBean Persistence Serialization A bean has the property of persistence when its properties, fields, and state information are saved to and retrieved from storage.\nAll beans must persist. To persist, must implement either of below:\n java.io.Serializable java.io.Externalizable  Any class is serializable as long as that class or a parent class implements the java.io.Serializable interface.\nExamples:\n Component String Date Vector Hashtable  Not serializable:\n Image Thread Socket InputStream  Controlling Serialization:\n Automatic serialization Customized serialization Customized file format  Long Term Persistence Long-term persistence is a model that enables beans to be saved in XML format.\nLinks  JavaBeans\nOracle\u0026rsquo;s JavaBeans tutorials\nJavaBeans specification\n初识Spring —— Bean的装配（一）\nDid you find this page helpful? Consider sharing it 🙌 ","date":1594944000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598054400,"objectID":"1c55861fb60fd1b0ed7cd16a20839fe3","permalink":"/coding/java/javabeans/","publishdate":"2020-07-17T00:00:00Z","relpermalink":"/coding/java/javabeans/","section":"coding","summary":"A summary of Java Beans.","tags":["Java"],"title":"Java Beans","type":"coding"},{"authors":["Hongwei Li"],"categories":["Basic","Web Development"],"content":"We have the website up and running in Part-3. If you have not completed that part, please do so before continuing this part.\nPersonalize Academic website This part will include the following sections:\n Choose the right theme Set the site title Set the copyright Set the site icon Select the sections Set Google Analytics Configure the Menu Add dropdown menu Add your content  Choose the right theme Check out the available themes and choose a design you love. Set the configuration in file \u0026lsquo;site_root/config/_default/params.toml\u0026rsquo;. I leave the default as is.\ntheme = \u0026quot;minimal\u0026quot;   Replace the \u0026lsquo;site_root\u0026rsquo; with your site root directory.   Set the site title Set the title in file \u0026lsquo;site_root/config/_default/config.toml\u0026rsquo;. In my case, the \u0026lsquo;flycoolman\u0026rsquo; is used.\ntitle = \u0026quot;flycoolman\u0026quot;  Set the copyright Set the copyright in file \u0026lsquo;site_root/config/_default/config.toml\u0026rsquo;, i.e.\ncopyright = \u0026quot;\u0026amp;copy;{year} flycoolman.com All Rights Reserved\u0026quot;  Set the site icon Save your icon image named icon.png and place the image in your root assets/images/ folder, creating the assets and images folders if they don’t already exist.\nSelect the sections All the sections of the demo home page in the folder \u0026lsquo;site_root/content/home\u0026rsquo;. See the picture below:\nThere is a setting called \u0026lsquo;active\u0026rsquo; in each file/section, just set the value to \u0026lsquo;false\u0026rsquo;, if you don\u0026rsquo;t want it in your home page.\nSet Google Analytics Set the Google Analytics in file \u0026lsquo;site_root/config/_default/params.toml\u0026rsquo; with your Google Analytics Tracking ID.\ngoogle_analytics = \u0026quot;UA-123456789-6\u0026quot;   There are many parameters in the file \u0026lsquo;params.toml\u0026rsquo;, set the ones that you want. They are self-explained   Configure the Menu The Menu on Navigation bar can be configured by change the settings in file \u0026lsquo;site_root/config/_default/menus.toml\u0026rsquo;, i.e.\n Remove current one by deleting or commenting out the corresponding item Rename current one by changing the value of \u0026lsquo;name\u0026rsquo; Add new one by adding additional part of \u0026lsquo;[[main]]\u0026rsquo; Rearrange the order by changing the value of \u0026lsquo;weight\u0026rsquo;  Add dropdown menu To create a dropdown sub-menu, add identifier = \u0026ldquo;something\u0026rdquo; to the parent item and parent = \u0026ldquo;something\u0026rdquo; to the child item. The pictures below show how to add dropdown menu in file \u0026lsquo;site_root/config/_default/menus.toml\u0026rsquo;.\nAdd your content Go to the folder \u0026lsquo;site_root/content/post/\u0026rsquo;, select one post as template, write your own post. Refer to the links below for markdown syntax.\nAll set! Enjoy writing!\nLinks How-To Academic https://sourcethemes.com/academic/docs/get-started/ https://sourcethemes.com/academic/docs/page-builder/\nHow-To Markdown https://guides.github.com/features/mastering-markdown/ https://www.markdownguide.org/basic-syntax/ https://simplpost.com/markdown.html\nAdd Utterances Comment Engine https://mscipio.github.io/post/utterances-comment-engine/\nDid you find this page helpful? Consider sharing it 🙌 ","date":1594857600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594857600,"objectID":"959a089fea3c45eeae7646ae44558f1f","permalink":"/post/build-a-free-website-in-minutes-part-4/","publishdate":"2020-07-16T00:00:00Z","relpermalink":"/post/build-a-free-website-in-minutes-part-4/","section":"post","summary":"Learn how to create a static website by Hugo, GitHub Pages and Academic.","tags":["Hugo","Web Development","Static Website","Github Pages","Academic"],"title":"Build A Free Website In Minutes - Part 4","type":"post"},{"authors":["Hongwei Li"],"categories":["Basic","Web Development"],"content":"In Part-1 and Part-2 we introduced the basic knowledges of the tools to help understand how they work. From this part, we will focus on building our real website.\nPrerequisites   Download and install Git Download and install Hugo Extended Version\nPlease follow the part of \u0026lsquo;Install Hugo Extended Version\u0026rsquo; in Part-2 of this tutorial.  GitHub Repositories GitHub Pages Repository Please follow the guide on GitHub Pages to create the repository correctly.\nAcademic-kickstart Repository Go to the GitHub page of academic-kickstart and fork the repository to your GitHub account. The pictures below show the source of academic-kickstart page and the forked repository in my account.\nAcademic-kickstart source page The forked repository in my account Local Environment Step 1 - git clone \u0026lt;username\u0026gt;.github.io to local git clone https://github.com/your-user-name/your-user-name.github.io.git  Step 2 - git clone academic-kickstart to local git clone https://github.com/your-user-name/academic-kickstart.git  Step 3 - Initialize the theme cd academic-kickstart/ git submodule update --init --recursive  Step 4 - Set base url in config.toml In your config.toml file, set baseurl = \u0026ldquo;https://\u0026lt;USERNAME\u0026gt;.github.io/\u0026rdquo;, where \u0026lt;USERNAME\u0026gt; is your Github username. Stop Hugo if it’s running and delete the public directory if it exists (by typing rm -r public/).\nvi config.toml  The example for my site Step 5 - Add .github.io repository into Academic-kickstart Add .github.io repository as a submodule in a folder named \u0026lsquo;public\u0026rsquo;, replacing with your Github username.\ncd academic-kickstart/ git submodule add -f -b master https://github.com/your-user-name/your-user-name.github.io.git public  Step 6 - Remove initial index file Delete the \u0026lsquo;index.html\u0026rsquo; file in the folder of \u0026lsquo;public\u0026rsquo;, if there is one.\ncd public rm index.html  Step 7 - Build your website cd academic-kickstart/ hugo  Step 8 - Deploy to GitHub Add everything to your local git repository and push it up to your remote repository on GitHub.\ncd academic-kickstart/ cd public git add . git comment -m \u0026quot;Initial commit\u0026quot; git push cd academic-kickstart/ git add . git comment -m \u0026quot;Initial commit\u0026quot; git push  Whilst running the above commands you may be prompted for your GitHub username and password.\nOnce uploading is finished, access https://\u0026lt;USERNAME\u0026gt;.github.io in your browser, substituting with your GitHub username. You will see a page like below:\nBuild Academic Demo Like Site To initialise your site with the demo content, copy the contents of the themes/academic/exampleSite/ folder to your website root folder, overwriting existing files if necessary. The exampleSite folder contains an example config file and content to help you get started.\nStep 1 - Replace the content cd academic-kickstart/ cp -av themes/academic/exampleSite/* .  Step 2 - Build the site cd academic-kickstart/ hugo  Step 3 - Deploy to GitHub cd academic-kickstart/ cd public git add . git comment -m \u0026quot;Initial commit\u0026quot; git push cd academic-kickstart/ git add . git comment -m \u0026quot;Initial commit\u0026quot; git push  Once uploading is finished, refresh the page in your browser, the page shows like below picture.\nUp to now, we have an Academic demo-like website up and running. In next part, we will personalize the website.\nDid you find this page helpful? Consider sharing it 🙌 ","date":1594771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"126430cd432a7a0594468edfd0867ae1","permalink":"/post/build-a-free-website-in-minutes-part-3/","publishdate":"2020-07-15T00:00:00Z","relpermalink":"/post/build-a-free-website-in-minutes-part-3/","section":"post","summary":"Learn how to create a static website by Hugo, GitHub Pages and Academic.","tags":["Hugo","Web Development","Static Website","Github Pages","Academic"],"title":"Build A Free Website In Minutes - Part 3","type":"post"},{"authors":["Hongwei Li"],"categories":["Basic","Web Development"],"content":"To learn more about prerequisites and tools, please check out the Part-1 of this series.\nHugo Theme Step 1 - Create a new site hugo new site hello-theme  Step 2 - Download the ananke theme wget https://github.com/budparr/gohugo-theme-ananke/archive/master.zip  Step 3 - Extract that .zip file to get a “gohugo-theme-ananke-master” directory unzip master.zip  Step 4 - Rename that directory to “ananke”, and move it into the “themes/” directory mv gohugo-theme-ananke-master/ ananke/ mv ananke/ themes/ananke  Step 5 - Add the theme to the site configuration echo 'theme = \u0026quot;ananke\u0026quot;' \u0026gt;\u0026gt; config.toml  Step 6 - Add some content and change the header of the post to say draft: false hugo new posts/hello-hugo-theme-post.md vi content/posts/hello-hugo-theme-post.md  Step 7 - Check the result hugo server  Hugo Uninstall Use the below command to remove the package of Hugo\nsudo apt-get remove --auto-remove hugo   Reading package lists\u0026hellip; Done\nBuilding dependency tree\nReading state information\u0026hellip; Done\nThe following packages will be REMOVED:\nhugo\n0 upgraded, 0 newly installed, 1 to remove and 0 not upgraded.\nAfter this operation, 44.5 MB disk space will be freed.\nDo you want to continue? [Y/n] y\n(Reading database \u0026hellip; 252044 files and directories currently installed.)\nRemoving hugo (0.73.0) \u0026hellip;\ndpkg: warning: while removing hugo, directory \u0026lsquo;/usr/local/bin\u0026rsquo; not empty so not removed\n  As Academic requires Hugo extended version, that\u0026rsquo;s the reason that the standard version is uninstalled.   Hello Academic  Academic is a site builder, it is a hugo site itself.   Install Hugo Extended Version wget https://github.com/gohugoio/hugo/releases/download/v0.73.0/hugo_extended_0.73.0_Linux-64bit.deb sudo dpkg -i hugo_extended_0.73.0_Linux-64bit.deb hugo version  The output below shows the information of Hugo extended version.\n Hugo Static Site Generator v0.73.0-428907CC/extended linux/amd64 BuildDate: 2020-06-23T16:40:09Z\n Install Academic  This linke provides the different ways to install Academic. In this part, the Install with ZIP way is used.\nStep 1 - Download and extract Academic Kickstart wget https://github.com/sourcethemes/academic-kickstart/archive/master.zip unzip master.zip rm master.zip  Step 2 - Download and extract the Academic theme files from the hugo-academic-master folder to the themes/academic/ folder in Academic Kickstart wget https://github.com/gcushen/hugo-academic/archive/master.zip unzip master.zip rm master.zip\nStep 3 - Copy the hugo-academic-master folder to the themes/academic/ folder in Academic Kickstart cp -r hugo-academic-master/* academic-kickstart-master/themes/academic/  Step 4 - Check the result cd academic-kickstart-master/ hugo server  In next part, we will install everything in GitHub.\nDid you find this page helpful? Consider sharing it 🙌 ","date":1594684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594771200,"objectID":"99b2f681b738219aa0f211441b70b44b","permalink":"/post/build-a-free-website-in-minutes-part-2/","publishdate":"2020-07-14T00:00:00Z","relpermalink":"/post/build-a-free-website-in-minutes-part-2/","section":"post","summary":"Learn how to create a static website by Hugo, GitHub Pages and Academic.","tags":["Hugo","Web Development","Static Website","Github Pages","Academic"],"title":"Build A Free Website In Minutes - Part 2","type":"post"},{"authors":["Hongwei Li"],"categories":["Basic","Web Development"],"content":"When I create this website, I find that there are no documents to walk me through each step from \u0026lsquo;Hello World\u0026rsquo; to deployment till customization. This makes me have the idea to create one and help others to understand the whole process. In this tutorial, we will use the tools GitHub Pages, Hugo and Academic.\nThis is the Part-1 of this series.\nPrerequisites Before contiduing this tutorial, you should have the basic knowledge of the following items:\n Git GitHub HTTP HTTPS DNS   If you are familiar with these and just want to build your website quickly, you can jump to Part-3.   Tools GitHub Pages  GitHub Pages is a static site hosting service that takes HTML, CSS, and JavaScript files straight from a repository on GitHub, optionally runs the files through a build process, and publishes a website.\nHugo  Hugo is a fast and modern static site generator written in Go.\nAcademic  Academic is a website builder for Hugo. Build anything with widgets and personalize your site with themes, plugins, and language packs.\nHow to Use the Tools From the brief introduction above, you have known what the tools are. Basically we will use Hugo to generate the website, personalize it with Academic, and deploy it to GitHub Pages. Now let\u0026rsquo;s put hands on it. We will start from \u0026lsquo;Hello World\u0026rsquo;.\nHello World of GitHub Pages Please follow the GitHub Pages Hello World Guide.\nHello World of Hugo Hugo official site has a quick start on macOS. But it may not install the latest version of Hugo, like on Ubuntu 18.04, the Hugo version is quite old if installing by apt. I will introduce the steps on Ubuntu 18.04 here.\nStep 1 - Install mkdir -p temp cd temp/ wget https://github.com/gohugoio/hugo/releases/download/v0.73.0/hugo_0.73.0_Linux-64bit.deb sudo dpkg -i hugo_0.73.0_Linux-64bit.deb hugo version  The output like below\n Hugo Static Site Generator v0.73.0-428907CC linux/amd64 BuildDate: 2020-06-23T16:30:43Z\n Step 2 - New Site sudo rm hugo_0.73.0_Linux-64bit.deb mkdir hugo-sites cd hugo-sites/ hugo new site hello-world  The output like below\n Congratulations! Your new Hugo site is created in /home/hongwei/temp/hugo-sites/hello-world.\nJust a few more steps and you\u0026rsquo;re ready to go:\n Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/ or create your own with the \u0026ldquo;hugo new theme \u0026rdquo; command. Perhaps you want to add some content. You can add single files with \u0026ldquo;hugo new /.\u0026rdquo;. Start the built-in live server via \u0026ldquo;hugo server\u0026rdquo;.  Visit https://gohugo.io/ for quickstart guide and full documentation.\n Let\u0026rsquo;s see the structure of the directory\ncd hello-world tree  The output like below\nLet\u0026rsquo;s check the result\nhugo server  Now go to http://127.0.0.1:1313, you get a blank page.\nStep 3 - Hello World To ask the website to say \u0026lsquo;Hello World\u0026rsquo;, we need to add content and template.\nAdd a file named _index.md in the fold of content\ncd content/ vi _index.md  The file content\n ---\ntitle: Hello, World!\ndescription: |\nMy personal site.\n---\nWelcome to my personal site.\n Add a html template file named index.html in the fold of layouts\ncd .. cd layouts/ vi index.html cd ..  To check the result\nhugo server  Now you get the \u0026lsquo;Hello World\u0026rsquo; page.\nTo customize the website, we will discuss Hugo theme and Academic in Part-2.\nDid you find this page helpful? Consider sharing it 🙌 ","date":1594425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594684800,"objectID":"85a4ed785d3315b4539f9eb99f21fddb","permalink":"/post/build-a-free-website-in-minutes-part-1/","publishdate":"2020-07-11T00:00:00Z","relpermalink":"/post/build-a-free-website-in-minutes-part-1/","section":"post","summary":"Learn how to create a static website by Hugo, GitHub Pages and Academic.","tags":["Hugo","Web Development","Static Website","Github Pages","Academic"],"title":"Build A Free Website In Minutes - Part 1","type":"post"},{"authors":["Hongwei Li"],"categories":["Linux","System","DevOps","SRE"],"content":"Minikube Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day.\nSet up Minikube  minikube start\nMinikube VM Login   On VM console:\nusername: root no password\n  from host terminal:\nminikube ssh\n  username+IP on host:\nusername: docker\npassword: tcuser\ni.e.\nssh docker@192.168.99.103  ssh docker@$(minikube ip)  Exit the login: exit\nSetting a VM driver by default minikube config set vm-driver virtualbox\nUse local images by re-using the Docker daemon  How To Install and Use Docker on Ubuntu 18.04\nUse local images by re-using the Docker daemon\neval $(minikube docker-env)\nCommands for using minikube minikube start\nminikube delete\nminikube status\nminikube start -p test\nminikube delete -p test\nminikube ssh\nminikube ssh -p test\nminikube ip\nminikube dashboard\nminikube addons list\nkubectl get pods -A\n Minikube Cheat Sheet: most helpful commands and features I wish I knew from the start\nMinikube command auto completion sudo apt install bash-completion\nHow to add bash auto completion in Ubuntu Linux\nHow can I enable Tab completion to minikube?\n","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"a97ec2da228f99b0314a295d3bd2307a","permalink":"/devops/minikube/","publishdate":"2020-07-01T00:00:00Z","relpermalink":"/devops/minikube/","section":"devops","summary":"A simple guide introduces how to use Minikube.","tags":["Linux","System","Kubernetes","K8S"],"title":"Minikube","type":"devops"},{"authors":["Hongwei Li"],"categories":["Java"],"content":"Java Notes A free style notes of Java.\nBasic knowledge  Static blocks Non static blocks Static methods Non-Static methods Static blank final variables Non-Static blank final variables Static classes Non-Static inner classes Static and dynamic polymorphism Static binding/Early binding Dynamic binding/Late binding  Top 10 Mistakes Java Developers Make  Top 10 Mistakes Java Developers Make\nDifferentiate JVM JRE JDK JIT  Java Virtual Machine (JVM) is an abstract computing machine. Java Runtime Environment (JRE) is an implementation of the JVM. Java Development Kit (JDK) contains JRE along with various development tools like Java libraries, Java source compilers, Java debuggers, bundling and deployment tools. Just In Time compiler (JIT) is runs after the program has started executing, on the fly. It has access to runtime information and makes optimizations of the code for better performance.   Differentiate JVM JRE JDK JIT\nClass vs Interface  Class: A class is a user-defined blueprint or prototype from which objects are created. It represents the set of properties or methods that are common to all objects of one type. Interface: Like a class, an interface can have methods and variables, but the methods declared in an interface are by default abstract (only method signature, no body). Interfaces specify what a class must do and not how. It is the blueprint of the class.  An interface defines a protocol of behavior and not how we should be implemented. A class that implements an interface adheres to the protocol defined by that interface.\n Interface variables are static because java interfaces cannot be instantiated on their own. The value of the variable must be assigned in a static context in which no instance exists. The final modifier ensures the value assigned to the interface variable is a true constant that cannot be re-assigned. In other words, interfaces can declare only constants, not instance variables.  Pass By Value and Pass By Reference and Pass Reference by Value  Java uses pass by value. There is no pass by reference in Java.   Pass By Value and Pass By Reference and Pass Reference by Value Java Pass By Value and Pass By Reference\n Java always passes parameter variables by value. Object variables in Java always point to the real object in the memory heap. A mutable object’s value can be changed when it is passed to a method. An immutable object’s value cannot be changed, even if it is passed a new value. “Passing by value” refers to passing a copy of the value. “Passing by reference” refers to passing the real reference of the variable in memory.   Does Java pass by reference or pass by value?\nJava (JVM) Memory Types Shared/Common Area\n  Heap Memory\nClass instances and arrays are stored in heap memory. Heap memory is also called as shared memory. As this is the place where multiple threads will share the same data.\nHeap data area is created at VM startup. Claiming the memory back is done automatically by the garbage collector (GC).\n  Non-heap Memory\n Method area Method area is created at JVM startup and shared among all the threads.  per-class structures (runtime constants and static fields) code for methods constructors Run-time Constant Pool      Per-Thread Area\n Program Counter (PC) Register PC keeps a pointer to the current statement that is being executed in its thread. If the current executing method is ‘native’, then the value of program counter register will be undefined. JVM Stacks or Frames Java JVM frames are created when a method is invoked, it performs the dynamic linking. JVM stacks are created and managed for each thread. Native Method Stacks It is used for native methods, and created per thread.  Memory Generations HotSpot VM’s garbage collector uses generational garbage collection. It separates the JVM’s memory into and they are called young generation and old generation.\n Young Generation   Eden space Survivor space  Old Generation   Tenured Generation GC moves live objects from survivor space to tenured generation. PermGen (Permanent Generation) The permanent generation contains meta data of the virtual machine, class and method objects.  (Java JVM Run-time Data Areas)[https://javapapers.com/core-java/java-jvm-run-time-data-areas/#Java_Virtual_Machine_Stacks]\n(Java (JVM) Memory Types)[https://javapapers.com/core-java/java-jvm-memory-types/]\nKey Takeaways\n Local Variables are stored in Frames during runtime. Static Variables are stored in Method Area. Arrays are stored in heap memory.  Java Static Java Static Variables  Java instance variables are given separate memory for storage. If there is a need for a variable to be common to all the objects of a single java class, then the static modifier should be used in the variable declaration. Any java object that belongs to that class can modify its static variables. Also, an instance is not a must to modify the static variable and it can be accessed using the java class directly. Static variables can be accessed by java instance methods also. When the value of a constant is known at compile time it is declared ‘final’ using the ‘static’ keyword.  Java Static Methods  Similar to static variables, java static methods are also common to classes and not tied to a java instance. Good practice in java is that, static methods should be invoked with using the class name though it can be invoked using an object. ClassName.methodName(arguments) or objectName.methodName(arguments) General use for java static methods is to access static fields. Static methods can be accessed by java instance methods. Java static methods cannot access instance variables or instance methods directly. Java static methods cannot use the ‘this’ keyword.  Java Static Classes  For java classes, only an inner class can be declared using the static modifier. For java a static inner class it does not mean that, all their members are static. These are called nested static classes in java.   Java Static\nStatic Block It\u0026rsquo;s a static initializer. It\u0026rsquo;s executed when the class is loaded (or initialized, to be precise, but you usually don\u0026rsquo;t notice the difference).\nIt can be thought of as a \u0026ldquo;class constructor\u0026rdquo;.\nNote that there are also instance initializers, which look the same, except that they don\u0026rsquo;t have the static keyword. Those are run in addition to the code in the constructor when a new instance of the object is created.\nA static block in Java is a block of code that is executed at the time of loading a class for use in a Java application. It starts with a \u0026lsquo;static {\u0026rsquo; and it is used for initializing static Class members in general — and is also known as a \u0026lsquo;Static Initializer\u0026rsquo;. The most powerful use of a static block can be realized while performing operations that are required to be executed only once for a Class in an application lifecycle.\n Static Block in Java\nThe Hidden Synchronized Keyword With a Static Block\nBuilder Patter Builder is a creational design pattern that lets you construct complex objects step by step. The pattern allows you to produce different types and representations of an object using the same construction code.\nBuilder pattern builds a complex object using simple objects and using a step by step approach. This type of design pattern comes under creational pattern as this pattern provides one of the best ways to create an object.\nA Builder class builds the final object step by step. This builder is independent of other objects.\nPros and Cons Pros\n You can construct objects step-by-step, defer construction steps or run steps recursively. You can reuse the same construction code when building various representations of products. Single Responsibility Principle. You can isolate complex construction code from the business logic of the product.  Cons\n The overall complexity of the code increases since the pattern requires creating multiple new classes.  Relations with Other Patterns   Many designs start by using Factory Method (less complicated and more customizable via subclasses) and evolve toward Abstract Factory, Prototype, or Builder (more flexible, but more complicated).\n  Builder focuses on constructing complex objects step by step. Abstract Factory specializes in creating families of related objects. Abstract Factory returns the product immediately, whereas Builder lets you run some additional construction steps before fetching the product.\n  You can use Builder when creating complex Composite trees because you can program its construction steps to work recursively.\n  You can combine Builder with Bridge: the director class plays the role of the abstraction, while different builders act as implementations.\n  Abstract Factories, Builders and Prototypes can all be implemented as Singletons.\n   Builder pattern\nBuilder\nDesign Patterns - Builder Pattern\ntransient keyword in Java The transient keyword in Java is used to indicate that a field should not be part of the serialization (which means saved, like to a file) process.\n Why does Java have transient fields?\nArray vs. ArrayList  Array is a fixed length data structure whereas ArrayList is a variable length Collection class. We cannot change length of array once created in Java but ArrayList can be changed. We cannot store primitives in ArrayList, it can only store objects. But array can contain both primitives and objects in Java.   Array vs ArrayList in Java\nArray vs ArrayList in Java\nArrays.asList() vs new ArrayList() When you call Arrays.asList it does not return a java.util.ArrayList. It returns a java.util.Arrays$ArrayList which is a fixed size list backed by the original source array. In other words, it is a view for the array exposed with Java\u0026rsquo;s collection-based APIs.\n Difference between Arrays.asList(array) and new ArrayList(Arrays.asList(array))\nWhy does Arrays.asList() return its own ArrayList implementation\nAccess Level  Java Access Level for Members: public, protected, private\nSet vs. Set\u0026lt;?\u0026gt;  an unbounded wildcard Set\u0026lt;?\u0026gt; can hold elements of any type, and a raw type Set can also hold elements of any type. wildcard type is safe and the raw type is not. We can not put any element into a Set\u0026lt;?\u0026gt; When you want to use a generic type, but you don\u0026rsquo;t know or care what the actual type the parameter is, you can use \u0026lt;?\u0026gt;[1]. It can only be used as parameters for a method.   Raw type vs. Unbounded wildcard\nArrayList vs. LinkedList vs. Vector Implementation  ArrayList is implemented as a resizable array. As more elements are added to ArrayList, its size is increased dynamically. It\u0026rsquo;s elements can be accessed directly by using the get and set methods, since ArrayList is essentially an array. LinkedList is implemented as a double linked list. Its performance on add and remove is better than Arraylist, but worse on get and set methods. Vector is similar with ArrayList, but it is synchronized.  Some details  ArrayList is a better choice if your program is thread-safe. Vector and ArrayList require more space as more elements are added. Vector each time doubles its array size, while ArrayList grow 50% of its size each time. LinkedList, however, also implements Queue interface which adds more methods than ArrayList and Vector, such as offer(), peek(), poll(), etc.   The default initial capacity of an ArrayList is pretty small. It is a good habit to construct the ArrayList with a higher initial capacity. This can avoid the resizing cost.   How to use  LinkedList should be preferred if there are a large number of add/remove operations LinkedList should be preferred if there are not a lot of random access operations. Vector is almost identical to ArrayList, and the difference is that Vector is synchronized. Because of this, it has an overhead than ArrayList. Normally, most Java programmers use ArrayList instead of Vector because they can synchronize explicitly by themselves.  add() in the table refers to add(E e), and remove() refers to remove(int index)\n ArrayList has O(n) time complexity for arbitrary indices of add/remove, but O(1) for the operation at the end of the list. LinkedList has O(n) time complexity for arbitrary indices of add/remove, but O(1) for operations at end/beginning of the List.  Java Collection Hierarchy The queue (a FIFO list) Implementation of a queue\n A queue (of bounded size) can be efficiently implemented in an array. Look at JavaHypertext entry “queue”. A queue can be efficiently implemented using any linked list that supports deletion in the front and insertion at the end in constant time. The first (last) element of the queue is at the front (end) of the linked list.  The stack (a LIFO list) Implementation of a stack\n A stack (of bounded size) can be efficiently implemented using an array b and an int variable n: The n elements of the stack are in b[0..n-1], with b[0] being the bottom element and b[n-1] being the top element. A stack can be efficiently implemented using a linked list. The first element is the top of the stack and the last element is the bottom. It’s easy to push (prepend) an element and pop (remove) the first element in constant time.  The deque The word deque, usually pronounced deck, is short for double-ended queue. A deque is a list that supports insertion and removal at both ends. Thus, a deque can be used as a queue or as a stack.\nStacks, queues, and deques in the Java Collection framework   Java has interface Deque. It is implemented by classes ArrayDeque (which implements a list in an expandable array) and LinkedList, so these two classes can be used for a queue and for a stack.\n  Both ArrayDeque and LinkedList also implement interface Queue, so you can use this interface to restrict operations to queue operations. For example, create a LinkedList and assign it to a Queue variable.\n Queue\u0026lt;E\u0026gt; q= new LinkedList\u0026lt;\u0026gt;();    Thereafter, use only q for the LinkedList and operations are restricted to queue operations.\n Java also has a class Stack, which implements a stack in an expandable array. However, the Java API would rather you use an ArrayDeque. The problem is that there is no suitable way to restrict the operations of an Array-Deque to stack operations, so we prefer to use class Stack.  Arrays.asList() Variable Arguments Since the asList method in Arrays uses variable arguments, and variable arguments expressions are mapped to arrays, you could either pass an inline array as in:\nList\u0026lt;String\u0026gt; list = Arrays.asList(new String[]{\u0026quot;a\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;});  Or simply pass the variable arguments that will be automatically mapped to an array:\nList\u0026lt;String\u0026gt; list = Arrays.asList(\u0026quot;a\u0026quot;,\u0026quot;b\u0026quot;,\u0026quot;c\u0026quot;);  Varargs (Variable-length Arguments) A method that takes a variable number of arguments is a varargs method.\nA variable-length argument is specified by three periods(…). For Example,\npublic static void varargFunction(dataType \u0026hellip; dataVar) { // method body }\nThe \u0026hellip; syntax tells the Java compiler that the method can be called with zero or more arguments. As a result, dataVar variable is implicitly declared as an array of type dataType[ ]. Thus, inside the method, dataVar variable is accessed using the array syntax.\nIn case of no arguments, the length of nums is 0.\n Vararg Methods can also be overloaded but overloading may lead to ambiguity. Prior to JDK 5, variable length arguments could be handled into two ways : One was using overloading, other was using array argument. There can be only one variable argument in a method. Variable argument (varargs) must be the last argument.  Patterns by type   Creational Creational patterns are ones that create objects, rather than having to instantiate objects directly. This gives the program more flexibility in deciding which objects need to be created for a given case.\n Abstract factory groups object factories that have a common theme. Builder constructs complex objects by separating construction and representation. Factory method creates objects without specifying the exact class to create. Prototype creates objects by cloning an existing object. Singleton restricts object creation for a class to only one instance.    Structural These concern class and object composition. They use inheritance to compose interfaces and define ways to compose objects to obtain new functionality.\n Adapter allows classes with incompatible interfaces to work together by wrapping its own interface around that of an already existing class. Bridge decouples an abstraction from its implementation so that the two can vary independently. Composite composes zero-or-more similar objects so that they can be manipulated as one object. Decorator dynamically adds/overrides behaviour in an existing method of an object. Facade provides a simplified interface to a large body of code. Flyweight reduces the cost of creating and manipulating a large number of similar objects. Proxy provides a placeholder for another object to control access, reduce cost, and reduce complexity.    Behavioral Most of these design patterns are specifically concerned with communication between objects.\n Chain of responsibility delegates commands to a chain of processing objects. Command creates objects which encapsulate actions and parameters. Interpreter implements a specialized language. Iterator accesses the elements of an object sequentially without exposing its underlying representation. Mediator allows loose coupling between classes by being the only class that has detailed knowledge of their methods. Memento provides the ability to restore an object to its previous state (undo). Observer is a publish/subscribe pattern which allows a number of observer objects to see an event. State allows an object to alter its behavior when its internal state changes. Strategy allows one of a family of algorithms to be selected on-the-fly at runtime. Template method defines the skeleton of an algorithm as an abstract class, allowing its subclasses to provide concrete behavior. Visitor separates an algorithm from an object structure by moving the hierarchy of methods into one object.     Design Patterns\nFactory Pattern Java Best Practice Return a List, not a LinkedList Return a List, not an ArrayList Your Java code will be more flexible when you learn to return more-general object references. In most cases other developers only need to see your interface, not your implementation. Put another way, does it matter to anyone else if you used a LinkedList or an ArrayList? If it doesn\u0026rsquo;t matter, then return a List, or perhaps even a Collection.\nIt\u0026rsquo;s best to return the most generic type that\u0026rsquo;s appropriate for your interface.\nIf there\u0026rsquo;s some reason why ArrayList is inherently appropriate for the data you\u0026rsquo;re returning then you should use that. Typically List is fine but you might also consider using Collection if the returned values are inherently unordered.\n Should i return List or ArrayList\nJava Best Practice - return a List, not a LinkedList\nMutable vs. Immutable String vs StringBuilder vs StringBuffer  String vs StringBuilder vs StringBuffer in Java\n String is immutable StringBuilder is mutable StringBuffer is similar to StringBuilder except one difference that StringBuffer is thread safe, i.e., multiple threads can use it without any issue. The thread safety brings a penalty of performance When to use  If a string is going to remain constant throughout the program, then use String class object because a String object is immutable. If a string can change (example: lots of logic and operations in the construction of the string) and will only be accessed from a single thread, using a StringBuilder is good enough. If a string can change, and will be accessed from multiple threads, use a StringBuffer because StringBuffer is synchronous so you have thread-safety.     Java Generics\nGeneric Lists\n Java Generics Tutorial\nJava X-ables Mutable vs. Immutable Closable Serialable Comparable\nJava toString() works differently between Array and ArrayList  Why toString() method works differently between Array and ArrayList object in Java\nThe main difference between an array and an arraylist is that an arraylist is a class that is written in Java and has its own implementation (including the decision to override toString) whereas arrays are part of the language specification itself.\nIn other words the language specification prevents the toString method of an array to be overriden and it therefore uses the default implementation defined in Object which prints the class name and hashcode.\n An array, i.e. int[], String[], does not have toString(), but use the default implementation defined in Object Arrays class has its own toString() implementation, i.e. Arrays.toString(arr)  Comparable vs Comparator in Java  Comparable vs Comparator in Java\nComparator Interface in Java with Examples\nA comparable object is capable of comparing itself with another object. The class itself must implements the java.lang.Comparable interface (override compareTo() method) to compare its instances.\nUnlike Comparable, Comparator is external to the element type we are comparing. It’s a separate class. We create multiple separate classes (that implement Comparator) to compare by different members.\nCollections class has a second sort() method and it takes Comparator. The sort() method invokes the compare() to sort objects.\nHow does Collections.Sort() work? Internally the Sort method does call Compare method of the classes it is sorting. To compare two elements, it asks “Which is greater?” Compare method returns -1, 0 or 1 to say if it is less than, equal, or greater to the other. It uses this result to then determine if they should be swapped for its sort.\n Comparable is meant for objects with natural ordering which means the object itself must know how it is to be ordered. Whereas, Comparator interface sorting is done through a separate class. Logically, Comparable interface compares “this” reference with the object specified and Comparator in Java compares two different class objects provided.\nIf any class implements Comparable interface in Java then collection of that object either List or Array can be sorted automatically by using Collections.sort() or Arrays.sort() method and objects will be sorted based on there natural order defined by CompareTo method. To summarize, if sorting of objects needs to be based on natural order then use Comparable whereas if you sorting needs to be done on attributes of different objects, then use Comparator in Java.  Java.util.Properties The java.util.Properties class is a class which represents a persistent set of properties.The Properties can be saved to a stream or loaded from a stream.Following are the important points about Properties\n Each key and its corresponding value in the property list is a string. A property list can contain another property list as its \u0026lsquo;defaults\u0026rsquo;, this second property list is searched if the property key is not found in the original property list. This class is thread-safe; multiple threads can share a single Properties object without the need for external synchronization. Properties is a subclass of Hashtable.   Java.util.Properties Class\nJava.util.Properties class in Java\nJava 8 Stream forEach Optional Did you find this page helpful? Consider sharing it 🙌 ","date":1577836799,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"6777a5ce133888bd6e0a9f2055df53ff","permalink":"/coding/java/java-notes/","publishdate":"2019-12-31T23:59:59Z","relpermalink":"/coding/java/java-notes/","section":"coding","summary":"A free style notes of Java.","tags":["Java"],"title":"Java Notes","type":"coding"},{"authors":["Hongwei Li"],"categories":["Linux","System","DevOps","SRE"],"content":"K8S NOTES Kubernetes Components  Cluster  A cluster is a set of machines, called nodes, that run containerized applications managed by Kubernetes. A cluster has at least one worker node and at least one master node.   Node  Master node(s)\nThe master node(s) manages the worker nodes and the pods in the cluster. Multiple master nodes are used to provide a cluster with failover and high availability. Worker node(s)\nThe worker node(s) host the pods that are the components of the application.    Master Node Components  kube-apiserver etcd kube-scheduler kube-controller-manager  Node Controller Replication Controller Endpoints Controller Service Account \u0026amp; Token Controllers   cloud-controller-manager\nThe following controllers have cloud provider dependencies:  Node Controller Route Controller Service Controller Volume Controller    Worker Node Components  Kubelet kube-proxy Container Runtime  Addons  DNS Web UI (Dashboard) Container Resource Monitoring Cluster-level Logging  K8S Abbreviation CNI Kubernetes has adopted the Container Network Interface(CNI) specification for managing network resources on a cluster.\nCRI Container Runtime Interface (CRI)\nEach container runtime has it own strengths, and many users have asked for Kubernetes to support more runtimes. CRI consists of a protocol buffers and gRPC API, and libraries, with additional specifications and tools under active development.\nKubelet communicates with the container runtime (or a CRI shim for the runtime) over Unix sockets using the gRPC framework, where kubelet acts as a client and the CRI shim as the server.\n Docker CRI-O Containerd Other CRI runtimes: frakti  CSI The goal of CSI is to establish a standardized mechanism for Container Orchestration Systems (COs) to expose arbitrary storage systems to their containerized workloads. Assuming a CSI storage plugin is already deployed on a Kubernetes cluster, users can use CSI volumes through the familiar Kubernetes storage API objects:\n PersistentVolumeClaims PersistentVolumes StorageClasses  CRD CustomResourceDefinition\nK8S Networking POD Communication   Inner POD\nMulti-containers in one POD\nContainers within a pod share an IP address and port space, and can find each other via localhost. They can also communicate with each other using standard inter-process communications.\n Shared volume IPC, i.e. queue Networking, localhost with different port    Inter PODs\n  Services ClusterIP ClusterIP accesses the services through proxy.\nClusterIP can access services only inside the cluster.\nNodeport NodePort opens a specific port on each node of the cluster and traffic on that node is forwarded directly to the service.\nLoadbalancer All the traffic on the port is forwarded to the service, there\u0026rsquo;s no filtering, no routing.\nIngress Controller Ingress Controller but there are third party solutions like Traefik and Nginx available. Ingress controller also provide L7 load balancing unlike cluster services.\nNetwork policies Isolation policies are configured on a per-namespace basis.\nKubernetes Networking\nK8S Objects To find all the objects in some specific API version:\n kubectl api-resources | cut -c92- kubectl api-resources | cut -c92-150 kubectl api-resources | cut -c92-150 | wc -l kubectl api-resources  Pod A thin wrapper around one or more containers\nDaemonSet Implements a single instance of a pod on a worker node\nDeployment Details how to roll out (or roll back) across versions of your application\nReplicaSet Ensures a defined number of pods are always running\nJob Ensures a pod properly runs to completion\nService Maps a fixed IP address to a logical group of pods\nLabel Key/Value pairs used for association and filtering\nLabels in Kubernetes are intended to be used to specify identifying attributes for objects. They are used by selector queries or with label selectors. Since they are used internally by Kubernetes, the structure of keys and values is constrained, to optimize queries.\nAnnotations annotations are a way to attach non-identifying metadata to objects. This metadata is not used internally by Kubernetes, so they cannot be used to identify within k8s. Instead, they are used by external tools and libraries. Examples of annotations include build/release timestamps, client library information for debugging, or fields managed by a network policy like Calico in this case.\nLabel vs. Annotation You can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects.\nTaints and Tolerations Node affinity, described here, is a property of pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite – they allow a node to repel a set of pods.\nTaints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints. Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints.\nNode isolation/restriction Adding labels to Node objects allows targeting pods to specific nodes or groups of nodes. This can be used to ensure specific pods only run on nodes with certain isolation, security, or regulatory properties.\nNODESELECTOR  Attach a label to the node Add a nodeSelector field to your pod configuration  Affinity and anti-affinity • Node affinity • Inter-pod affinity and anti-affinity  nodeSelector provides a very simple way to constrain pods to nodes with particular labels. The affinity/anti-affinity feature, currently in beta, greatly extends the types of constraints you can express. The key enhancements are:\n• The language is more expressive (not just “AND of exact match”) • You can indicate that the rule is “soft”/“preference” rather than a hard requirement, so if the scheduler can’t satisfy it, the pod will still be scheduled • You can constrain against labels on other pods running on the node (or other topological domain), rather than against labels on the node itself, which allows rules about which pods can and cannot be co-located  The affinity feature consists of two types of affinity, “node affinity” and “inter-pod affinity/anti-affinity”. Node affinity is like the existing nodeSelector (but with the first two benefits listed above), while inter-pod affinity/anti-affinity constrains against pod labels rather than node labels, as described in the third item listed above, in addition to having the first and second properties listed above.\nNode affinity is conceptually similar to nodeSelector – it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node.\nDeploy When you wish to deploy an application in Kubernetes, you usually define three components:\n a Deployment — which is a recipe for creating copies of your application called Pods a Service — an internal load balancer that routes the traffic to Pods an Ingress — a description of how the traffic should flow from outside the cluster to your Service  Ingress What is an ingress? In Kubernetes, an Ingress is an object that allows access to your Kubernetes services from outside the Kubernetes cluster. You configure access by creating a collection of rules that define which inbound connections reach which services.\nIngress, added in Kubernetes v1.1, exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource.\nInternet\u0026mdash;[ Ingress ]\u0026ndash;|\u0026ndash;|\u0026ndash;[ Services ] An Ingress can be configured to give services externally-reachable URLs, load balance traffic, terminate SSL, and offer name based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a loadbalancer, though it may also configure your edge router or additional frontends to help handle the traffic.\nAn Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer.\nWhat is an ingress controller? Kubernetes supports a high level abstraction called Ingress, which allows simple host or URL based HTTP routing. An ingress is a core concept (in beta) of Kubernetes, but is always implemented by a third party proxy. These implementations are known as ingress controllers.\nIn order for the Ingress resource to work, the cluster must have an ingress controller running.\nUnlike other types of controllers which run as part of the kube-controller-manager binary, Ingress controllers are not started automatically with a cluster. Let’s see some options:\n ALB Ingress Controller  Ingress vs. Ingress controller  Ingress should be the rules for the traffic, which indicate the destination of a request will go through in the cluster. Ingress Controller is the implementation for the Ingress. GCE and Nginx are both supported by k8s. They will take care of L4 or L7 proxy. Just like other objects in K8s, ingress is also a type of object, which is mainly referred as set of redirection rules. Where as ingress controller is like other deployment objects(could be deamon set as well) which listen and configure those ingress rules. If I talk in terms of Nginx, Ingress controller is Nginx software itself where as ingress(ingress rules) are nginx configurations.  The Ingress Resource A minimal ingress resource example:\nAs with all other Kubernetes resources, an Ingress needs apiVersion, kind, and metadata fields. Ingress frequently uses annotations to configure some options depending on the Ingress controller, an example of which is the rewrite-target annotation. Different Ingress controller support different annotations. Review the documentation for your choice of Ingress controller to learn which annotations are supported.\nThe Ingress spec has all the information needed to configure a loadbalancer or proxy server. Most importantly, it contains a list of rules matched against all incoming requests. Ingress resource only supports rules for directing HTTP traffic.\nIngress Rules Each http rule contains the following information:\n An optional host. In this example, no host is specified, so the rule applies to all inbound HTTP traffic through the IP address specified. If a host is provided (for example, foo.bar.com), the rules apply to that host. A list of paths (for example, /testpath), each of which has an associated backend defined with a serviceName and servicePort. Both the host and path must match the content of an incoming request before the loadbalancer will direct traffic to the referenced service. A backend is a combination of service and port names as described in the services doc. HTTP (and HTTPS) requests to the Ingress matching the host and path of the rule will be sent to the listed backend. A default backend is often configured in an Ingress controller that will service any requests that do not match a path in the spec.  Default Backend An Ingress with no rules sends all traffic to a single default backend. The default backend is typically a configuration option of the Ingress controller and is not specified in your Ingress resources.\nIf none of the hosts or paths match the HTTP request in the Ingress objects, the traffic is routed to your default backend.\nConfigMap  Ultimate Guide to ConfigMaps in Kubernetes\nSecrets Secret data should be stored and handled in a way that can be easily hidden and possibly encrypted at rest if the environment is configured as such. The Secret data is represented as base64-encoded information, and it is critical to understand that this is not encrypted. As soon as the secret is injected into the pod, the pod itself can see the secret data in plain text.\nSecret data is meant to be small amounts of data, limited by default in Kubernetes to 1 MB in size, for the base64-encoded data, so ensure that the actual data is approximately 750 KB because of the overhead of the encoding.\nThere are three types of secrets in Kubernetes:\n generic\nregular key/value pairs that are created from a file, a directory, or from string literals using the \u0026ndash;from-literal= parameter. docker-registry\nThis is used by the kubelet when passed in a pod template if there is an imagePullsecret to provide the credentials needed to authenticate to a private Docker registry: tls\nThis creates a Transport Layer Security (TLS) secret from a valid public/private key pair. As long as the cert is in a valid PEM format, the key pair will be encoded as a secret and can be passed to the pod to use for SSL/TLS needs.  ConfigMaps vs Secrets  the ConfigMap API is meant more for string data that is not really sensitive data. If your application requires more sensitive data, the Secrets API is more appropriate. ConfigMap data can be injected as either a volume mounted into the pod or as environment variables. Secrets are also mounted into tmpfs only on the nodes that have a pod that requires the secret and are deleted when the pod that needs it is gone. This prevents any secrets from being left behind on the disk of the node.  Roles Kubernetes has two types of roles, role and clusterRole, the difference being that role is specific to a namespace, and clusterRole is a cluster-wide role across all namespaces.\nRoleBindings The RoleBinding allows a mapping of a subject like a user or group to a specific role. Bindings also have two modes: roleBinding, which is specific to a namespace, and clusterRoleBinding, which is across the entire cluster.\nStorage  PersistentVolume – the low level representation of a storage volume Volume Driver – the code used to communicate with the backend storage provider Pod – a running container that will consume a PersistentVolume PersistentVolumeClaim – the binding between a Pod and PersistentVolume StorageClass – allows for dynamic provisioning of PersistentVolumes  Controllers and Kubelets Kubernetes has a number of controllers that run on the masters, monitor the state of the cluster and initiate actions in response to events.\nIt also runs a kubelet process on all of the worker nodes. The kubelet stays in constant contact with the controllers, submitting metrics about current running pods and listening for new instructions.\nKubelet:  Mount and format new PersistentVolumes that are scheduled to this host Start containers with PersistentVolume hostpath mounted inside the container Stop containers and unmount the associated PersistentVolume Constantly send metrics to the controllers about container \u0026amp; PersistentVolume state  Controller:  to match a PersistentVolumeClaim to a PersistentVolume to dynamically provision a new PersistentVolume if a claim cannot be met (if enabled)  in the case of EBS this is done via the AWS api from the masters   to attach the backend storage to a specific node if needed  in the case of EBS this is done via the AWS api from the masters   to instruct the kubelet for a node to mount (and potentially format) the volume  this is done on the actual node   to instruct the kubelet to start a container that uses the volume\nThe kubelet itself performs the low-level mount and mkfs commands when instructed by the controller.  Kubernetes Operator An Operator is an application-specific controller that extends the Kubernetes API to create, configure, and manage instances of complex stateful applications on behalf of a Kubernetes user. It builds upon the basic Kubernetes resource and controller concepts but includes domain or application-specific knowledge to automate common tasks.\nExamples are:\n elasticsearch-operator prometheus-operator cassandra-operator Flux The GitOps Kubernetes Operator   TBD   Troubleshooting  A visual guide on troubleshooting Kubernetes deployments | PDF\nLinks  Kubernetes Networking\nUltimate Guide to ConfigMaps in Kubernetes\nA visual guide on troubleshooting Kubernetes deployments\nA Basic Guide to Kubernetes Storage: PVS, PVCs, Statefulsets and More\nDid you find this page helpful? Consider sharing it 🙌 ","date":1577836799,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"6052b787dd6f1df1336a61f33fedc917","permalink":"/devops/k8s-notes/","publishdate":"2019-12-31T23:59:59Z","relpermalink":"/devops/k8s-notes/","section":"devops","summary":"A free style notes of Kubernetes.","tags":["Linux","System","Kubernetes","K8S"],"title":"Kubernetes Notes","type":"devops"},{"authors":["Hongwei Li"],"categories":["Python"],"content":"Python Notes A free style notes of Python.\nClass or Static Variables in Python In C++ and Java, we can use static keyword to make a variable as class variable. The variables which don’t have preceding static keyword are instance variables.\nThe Python approach is simple, it doesn’t require a static keyword. All variables which are assigned a value in class declaration are class variables. And variables which are assigned values inside methods are instance variables.\nTrue or False Most Values are True:\n Almost any value is evaluated to True if it has some sort of content. Any string is True, except empty strings. Any number is True, except 0. Any list, tuple, set, and dictionary are True, except empty ones.  Some Values are False:\nIn fact, there are not many values that evaluates to False,\n except empty values, such as (), [], {}, \u0026ldquo;\u0026rdquo;, the number 0, and the value None. And of course the value False evaluates to False.  Casting  int() - constructs an integer number from an integer literal, a float literal (by rounding down to the previous whole number), or a string literal (providing the string represents a whole number) float() - constructs a float number from an integer literal, a float literal or a string literal (providing the string represents a float or an integer) tr() - constructs a string from a wide variety of data types, including strings, integer literals and float literals  Dictionary: [] vs. update  How can I add new keys to a dictionary?\n  []\nyou create a new key\\value pair on a dictionary by assigning a value to that key. If the key doesn\u0026rsquo;t exist, it\u0026rsquo;s added and points to that value. If it exists, the current value it points to is overwritten.\nthe d[key]=val syntax as it is shorter and can handle any object as key (as long it is hashable), and only sets one value\n  update\nWhereas the .update(key1=val1, key2=val2) is nicer if you want to set multiple values at the same time, as long as the keys are strings (since kwargs are converted to strings).\n   dict.update can also take another dictionary, but I personally prefer not to explicitly create a new dictionary in order to update another one.   collections  collections — Container datatypes\nThis module implements specialized container datatypes providing alternatives to Python’s general purpose built-in containers, dict, list, set, and tuple.\n Deprecated since version 3.3, will be removed in version 3.10: Moved Collections Abstract Base Classes to the collections.abc module. For backwards compatibility, they continue to be visible in this module through Python 3.9.   collections.abc collections.abc — Abstract Base Classes for Containers\nfor/while else  Why does python use \u0026lsquo;else\u0026rsquo; after for and while loops?\nA common construct is to run a loop until something is found and then to break out of the loop. The problem is that if I break out of the loop or the loop ends I need to determine which case happened. One method is to create a flag or store variable that will let me do a second test to see how the loop was exited.\nFor example assume that I need to search through a list and process each item until a flag item is found and then stop processing. If the flag item is missing then an exception needs to be raised.\nUsing the Python for\u0026hellip;else construct you have\nfor i in mylist: if i == theflag: break process(i) else: raise ValueError(\u0026quot;List argument missing terminal flag.\u0026quot;)  Compare this to a method that does not use this syntactic sugar:\nflagfound = False for i in mylist: if i == theflag: flagfound = True break process(i) if not flagfound: raise ValueError(\u0026quot;List argument missing terminal flag.\u0026quot;)  In the first case the raise is bound tightly to the for loop it works with. In the second the binding is not as strong and errors may be introduced during maintenance.\nstr.split() Return a list of the words in the string, using sep as the delimiter string.\nfor example, \u0026lsquo;1\u0026lt;\u0026gt;2\u0026lt;\u0026gt;3\u0026rsquo;.split(\u0026lsquo;\u0026lt;\u0026gt;\u0026rsquo;) returns [\u0026lsquo;1\u0026rsquo;, \u0026lsquo;2\u0026rsquo;, \u0026lsquo;3\u0026rsquo;]\n The separator can be string, not only single character The return value of split  if not found, the return value is the original full string, like \u0026lsquo;1\u0026lt;\u0026gt;2\u0026lt;\u0026gt;3\u0026rsquo;.split(\u0026lsquo;not\u0026rsquo;) returns [\u0026lsquo;1\u0026lt;\u0026gt;2\u0026lt;\u0026gt;3\u0026rsquo;] if found, the strings are separated by the separator, like \u0026lsquo;1\u0026lt;\u0026gt;2\u0026lt;\u0026gt;3\u0026rsquo;.split(\u0026lsquo;\u0026lt;\u0026gt;\u0026rsquo;, 1) returns [\u0026lsquo;1\u0026rsquo;, \u0026lsquo;2\u0026lt;\u0026gt;3\u0026rsquo;] \u0026lsquo;'.split(\u0026lsquo;\u0026lt;\u0026gt;\u0026rsquo;) returns [''] \u0026lsquo;'.split('') raises exception ' 1 2 3 \u0026lsquo;.split() returns [\u0026lsquo;1\u0026rsquo;, \u0026lsquo;2\u0026rsquo;, \u0026lsquo;3\u0026rsquo;],\n' 1 2 3 \u0026lsquo;.split(None, 1) returns [\u0026lsquo;1\u0026rsquo;, \u0026lsquo;2 3 \u0026lsquo;],\n' 1 2 3 \u0026lsquo;.split(None) returns [\u0026lsquo;1\u0026rsquo;, \u0026lsquo;2\u0026rsquo;, \u0026lsquo;3\u0026rsquo;]     TBD    Just a Placeholder\nDid you find this page helpful? Consider sharing it 🙌 ","date":1577836799,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"624e461c4afac8a7b91f9f55f9440cd6","permalink":"/coding/python/python-notes/","publishdate":"2019-12-31T23:59:59Z","relpermalink":"/coding/python/python-notes/","section":"coding","summary":"A free style notes of Python.","tags":["Python"],"title":"Python Notes","type":"coding"},{"authors":["Hongwei Li"],"categories":["Coding","Python","Java"],"content":"Python vs. Java A VS. notes between Python and Java.\nType casting  Python: int(1.9), float(1), str(1.9) Java:  Narrowing casting: (manually) - converting a larger type to a smaller size type (int)1.9, (float)9.78 double -\u0026gt; float -\u0026gt; long -\u0026gt; int -\u0026gt; char -\u0026gt; short -\u0026gt; byte Widening casting: automatically - converting a smaller type to a larger type size\nbyte -\u0026gt; short -\u0026gt; char -\u0026gt; int -\u0026gt; long -\u0026gt; float -\u0026gt; double    True or False  Python: True/False Java: true/false  Map sorting  Python:\nEasy to use sorted() and lambda to sort dict/map by keys or values Java:\nUse Collections.sort() and Comparator to sort map by keys or values\nJava 8 provides new ways of defining Comparators by using lambda expressions and the comparing() static factory method.  Python max() vs Java Math.max() vs Java Collections.max()   Python max()\nCan campare multiple items, or iterable, even strings\n max(n1, n2, n3, ...)    or\n max(iterable)  or Compare strings\n max(\u0026quot;Mike\u0026quot;, \u0026quot;John\u0026quot;, \u0026quot;Vicky\u0026quot;)    Java max()\nOnly compare two mathematic items, i.e. int, float, double \u0026hellip;\n  Java Collections.max()\n  max(Collection\u0026lt;? extends T\u0026gt; coll)\nReturns the maximum element of the given collection, according to the natural ordering of its elements.\nstatic  T\tmax(Collection\u0026lt;? extends T\u0026gt; coll, Comparator\u0026lt;? super T\u0026gt; comp)\nReturns the maximum element of the given collection, according to the order induced by the specified comparator.\nCompare Integers\n Integer[] num = { 2, 4, 7, 5, 9 }; // using Collections.min() to // find minimum element // using only 1 line. int min = Collections.min(Arrays.asList(num)); // using Collections.max() // to find maximum element // using only 1 line. int max = Collections.max(Arrays.asList(num));  or Compare Strings\n // List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;(Arrays.asList(\u0026quot;Mike Smith\u0026quot;, \u0026quot;John\u0026quot;, \u0026quot;Vicky\u0026quot;)); List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026quot;Mike\u0026quot;, \u0026quot;John\u0026quot;, \u0026quot;Vicky\u0026quot;); String max = Collections.max(list);  or Use comparator\n List\u0026lt;String\u0026gt; list = Arrays.asList(\u0026quot;Mike Smith\u0026quot;, \u0026quot;John\u0026quot;, \u0026quot;Vicky\u0026quot;); String max = Collections.max(list, Comparator.comparing(s -\u0026gt; s.length()));   Python - Java\nThe aboves are same to min().   Infinity   Python\n  math.inf The math.inf constant returns a floating-point positive infinity. For negative infinity, use -math.inf. The inf constant is equivalent to float(\u0026lsquo;inf\u0026rsquo;).\nimport math print(math.inf)\n  sys.maxsize\nAn integer giving the maximum value a variable of type Py_ssize_t can take. It’s usually 231 - 1 on a 32-bit platform and 263 - 1 on a 64-bit platform.\nlike sys.maxint in Python2.\n    Java\n Integer.MAX_VALUE, Integer.MIN_VALUE, Integer.MAX_VALUE+1, Integer.MIN_VALUE-1 Long.MAX_VALUE, Long.MIN_VALUE Float.MAX_VALUE, Float.MIN_VALUE, Float.POSITIVE_INFINITY, Float.NEGATIVE_INFINITY Double.MAX_VALUE, Double.MIN_VALUE, Double.POSITIVE_INFINITY, Double.NEGATIVE_INFINITY, Double.NaN     There is no way to represent infinity as an integer in Python. This matches the behaviour of many other languages. However, due to Python\u0026rsquo;s dynamic typing system, you can use float(\u0026lsquo;inf\u0026rsquo;) in place of an integer, and it will behave as you would expect.    To Be Continued   Map/Dict Composite key   Python\n map = {(1, 0) : 2, (1, 1) : 3, (2, 0) : 4, (2, 1) : 5} map[(1,0)] map.get((1,0))    Java\nNot so easy to do\n  String equals vs == Check if the characters in two strings are identical\n  Python ==\n s1 = \u0026quot;Hello\u0026quot; s2 = \u0026quot;Hello\u0026quot; s3 = \u0026quot;Hello3\u0026quot; s4 = \u0026quot;Hell\u0026quot; + \u0026quot;o\u0026quot; s5 = \u0026quot;Hell\u0026quot; s5 += \u0026quot;o\u0026quot; id(s1) # 140701928086752 id(s2) # 140701928086752 id(s3) # 140701928160032 id(s4) # 140701928086752 id(s5) # 140701884560640 s1 == s2 # True s1 == s3 # False s1 == s4 # True s1 == s5 # True    Java equals() vs ==\n  == to compare memory address or say identity in system equals() to compare the content of strings\n String s1 = \u0026quot;Hello\u0026quot;; String s2 = \u0026quot;Hello\u0026quot;; String s3 = \u0026quot;Hello3\u0026quot;; String s4 = \u0026quot;Hell\u0026quot; + \u0026quot;o\u0026quot;; String s5 = \u0026quot;Hell\u0026quot;; s5 = s5 + \u0026quot;o\u0026quot;; System.out.println(Integer.toHexString(s1.hashCode())); // 42628b2 System.out.println(Integer.toHexString(s2.hashCode())); // 42628b2 System.out.println(Integer.toHexString(s3.hashCode())); // 809eedc1 System.out.println(Integer.toHexString(s4.hashCode())); // 42628b2 System.out.println(Integer.toHexString(s5.hashCode())); // 42628b2 System.out.println(Integer.toHexString(System.identityHashCode(s1))); // 2038ae61 System.out.println(Integer.toHexString(System.identityHashCode(s2))); // 2038ae61 System.out.println(Integer.toHexString(System.identityHashCode(s3))); // 3c0f93f1 System.out.println(Integer.toHexString(System.identityHashCode(s4))); // 2038ae61 System.out.println(Integer.toHexString(System.identityHashCode(s5))); // 31dc339b System.out.println(s1 == s2); // true System.out.println(s1 == s3); // false System.out.println(s1 == s4); // true System.out.println(s1 == s5); // false, not same identity System.out.println(s1.equals(s2)); // true System.out.println(s1.equals(s3)); // false System.out.println(s1.equals(s4)); // true System.out.println(s1.equals(s5)); // true, contents are equal  Did you find this page helpful? Consider sharing it 🙌 ","date":1577836799,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"7e558dbed8d9b88c23c651db9c0ce037","permalink":"/coding/python/python-vs-java/","publishdate":"2019-12-31T23:59:59Z","relpermalink":"/coding/python/python-vs-java/","section":"coding","summary":"A VS. notes between Python and Java.","tags":["Python","Java"],"title":"Python vs. Java","type":"coding"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Hongwei Li","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Hongwei Li","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]